{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9015544-846a-4524-830d-81fdb85a2686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amssa\\AppData\\Local\\Temp\\ipykernel_11236\\438414421.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, self.slices = torch.load(data_path)\n",
      "2025-01-18 20:53:51,601 [INFO] Loaded dataset with 1569586 samples\n",
      "2025-01-18 20:53:51,604 [INFO] Verifying Mutual Information bounds...\n",
      "2025-01-18 20:53:52,498 [INFO] Processed 10000/1569586 samples...\n",
      "2025-01-18 20:53:53,257 [INFO] Processed 20000/1569586 samples...\n",
      "2025-01-18 20:53:54,027 [INFO] Processed 30000/1569586 samples...\n",
      "2025-01-18 20:53:54,786 [INFO] Processed 40000/1569586 samples...\n",
      "2025-01-18 20:53:55,563 [INFO] Processed 50000/1569586 samples...\n",
      "2025-01-18 20:53:56,333 [INFO] Processed 60000/1569586 samples...\n",
      "2025-01-18 20:53:57,195 [INFO] Processed 70000/1569586 samples...\n",
      "2025-01-18 20:53:57,930 [INFO] Processed 80000/1569586 samples...\n",
      "2025-01-18 20:53:58,664 [INFO] Processed 90000/1569586 samples...\n",
      "2025-01-18 20:53:59,511 [INFO] Processed 100000/1569586 samples...\n",
      "2025-01-18 20:54:00,256 [INFO] Processed 110000/1569586 samples...\n",
      "2025-01-18 20:54:00,987 [INFO] Processed 120000/1569586 samples...\n",
      "2025-01-18 20:54:01,717 [INFO] Processed 130000/1569586 samples...\n",
      "2025-01-18 20:54:02,464 [INFO] Processed 140000/1569586 samples...\n",
      "2025-01-18 20:54:03,215 [INFO] Processed 150000/1569586 samples...\n",
      "2025-01-18 20:54:03,963 [INFO] Processed 160000/1569586 samples...\n",
      "2025-01-18 20:54:04,775 [INFO] Processed 170000/1569586 samples...\n",
      "2025-01-18 20:54:05,517 [INFO] Processed 180000/1569586 samples...\n",
      "2025-01-18 20:54:06,257 [INFO] Processed 190000/1569586 samples...\n",
      "2025-01-18 20:54:07,000 [INFO] Processed 200000/1569586 samples...\n",
      "2025-01-18 20:54:07,751 [INFO] Processed 210000/1569586 samples...\n",
      "2025-01-18 20:54:08,517 [INFO] Processed 220000/1569586 samples...\n",
      "2025-01-18 20:54:09,225 [INFO] Processed 230000/1569586 samples...\n",
      "2025-01-18 20:54:09,917 [INFO] Processed 240000/1569586 samples...\n",
      "2025-01-18 20:54:10,664 [INFO] Processed 250000/1569586 samples...\n",
      "2025-01-18 20:54:11,474 [INFO] Processed 260000/1569586 samples...\n",
      "2025-01-18 20:54:12,205 [INFO] Processed 270000/1569586 samples...\n",
      "2025-01-18 20:54:12,922 [INFO] Processed 280000/1569586 samples...\n",
      "2025-01-18 20:54:13,623 [INFO] Processed 290000/1569586 samples...\n",
      "2025-01-18 20:54:14,307 [INFO] Processed 300000/1569586 samples...\n",
      "2025-01-18 20:54:15,001 [INFO] Processed 310000/1569586 samples...\n",
      "2025-01-18 20:54:15,677 [INFO] Processed 320000/1569586 samples...\n",
      "2025-01-18 20:54:16,362 [INFO] Processed 330000/1569586 samples...\n",
      "2025-01-18 20:54:17,034 [INFO] Processed 340000/1569586 samples...\n",
      "2025-01-18 20:54:17,714 [INFO] Processed 350000/1569586 samples...\n",
      "2025-01-18 20:54:18,431 [INFO] Processed 360000/1569586 samples...\n",
      "2025-01-18 20:54:19,125 [INFO] Processed 370000/1569586 samples...\n",
      "2025-01-18 20:54:19,828 [INFO] Processed 380000/1569586 samples...\n",
      "2025-01-18 20:54:20,533 [INFO] Processed 390000/1569586 samples...\n",
      "2025-01-18 20:54:21,252 [INFO] Processed 400000/1569586 samples...\n",
      "2025-01-18 20:54:21,946 [INFO] Processed 410000/1569586 samples...\n",
      "2025-01-18 20:54:22,626 [INFO] Processed 420000/1569586 samples...\n",
      "2025-01-18 20:54:23,313 [INFO] Processed 430000/1569586 samples...\n",
      "2025-01-18 20:54:23,999 [INFO] Processed 440000/1569586 samples...\n",
      "2025-01-18 20:54:24,687 [INFO] Processed 450000/1569586 samples...\n",
      "2025-01-18 20:54:25,365 [INFO] Processed 460000/1569586 samples...\n",
      "2025-01-18 20:54:26,029 [INFO] Processed 470000/1569586 samples...\n",
      "2025-01-18 20:54:26,709 [INFO] Processed 480000/1569586 samples...\n",
      "2025-01-18 20:54:27,525 [INFO] Processed 490000/1569586 samples...\n",
      "2025-01-18 20:54:28,261 [INFO] Processed 500000/1569586 samples...\n",
      "2025-01-18 20:54:29,358 [INFO] Processed 510000/1569586 samples...\n",
      "2025-01-18 20:54:30,044 [INFO] Processed 520000/1569586 samples...\n",
      "2025-01-18 20:54:30,733 [INFO] Processed 530000/1569586 samples...\n",
      "2025-01-18 20:54:31,434 [INFO] Processed 540000/1569586 samples...\n",
      "2025-01-18 20:54:32,089 [INFO] Processed 550000/1569586 samples...\n",
      "2025-01-18 20:54:32,759 [INFO] Processed 560000/1569586 samples...\n",
      "2025-01-18 20:54:33,423 [INFO] Processed 570000/1569586 samples...\n",
      "2025-01-18 20:54:34,108 [INFO] Processed 580000/1569586 samples...\n",
      "2025-01-18 20:54:34,777 [INFO] Processed 590000/1569586 samples...\n",
      "2025-01-18 20:54:35,438 [INFO] Processed 600000/1569586 samples...\n",
      "2025-01-18 20:54:36,093 [INFO] Processed 610000/1569586 samples...\n",
      "2025-01-18 20:54:36,748 [INFO] Processed 620000/1569586 samples...\n",
      "2025-01-18 20:54:37,412 [INFO] Processed 630000/1569586 samples...\n",
      "2025-01-18 20:54:38,084 [INFO] Processed 640000/1569586 samples...\n",
      "2025-01-18 20:54:38,738 [INFO] Processed 650000/1569586 samples...\n",
      "2025-01-18 20:54:39,490 [INFO] Processed 660000/1569586 samples...\n",
      "2025-01-18 20:54:40,234 [INFO] Processed 670000/1569586 samples...\n",
      "2025-01-18 20:54:40,979 [INFO] Processed 680000/1569586 samples...\n",
      "2025-01-18 20:54:41,813 [INFO] Processed 690000/1569586 samples...\n",
      "2025-01-18 20:54:42,597 [INFO] Processed 700000/1569586 samples...\n",
      "2025-01-18 20:54:43,374 [INFO] Processed 710000/1569586 samples...\n",
      "2025-01-18 20:54:44,212 [INFO] Processed 720000/1569586 samples...\n",
      "2025-01-18 20:54:44,975 [INFO] Processed 730000/1569586 samples...\n",
      "2025-01-18 20:54:45,724 [INFO] Processed 740000/1569586 samples...\n",
      "2025-01-18 20:54:46,460 [INFO] Processed 750000/1569586 samples...\n",
      "2025-01-18 20:54:47,215 [INFO] Processed 760000/1569586 samples...\n",
      "2025-01-18 20:54:47,978 [INFO] Processed 770000/1569586 samples...\n",
      "2025-01-18 20:54:48,868 [INFO] Processed 780000/1569586 samples...\n",
      "2025-01-18 20:54:49,614 [INFO] Processed 790000/1569586 samples...\n",
      "2025-01-18 20:54:50,360 [INFO] Processed 800000/1569586 samples...\n",
      "2025-01-18 20:54:51,089 [INFO] Processed 810000/1569586 samples...\n",
      "2025-01-18 20:54:51,835 [INFO] Processed 820000/1569586 samples...\n",
      "2025-01-18 20:54:52,598 [INFO] Processed 830000/1569586 samples...\n",
      "2025-01-18 20:54:53,363 [INFO] Processed 840000/1569586 samples...\n",
      "2025-01-18 20:54:54,116 [INFO] Processed 850000/1569586 samples...\n",
      "2025-01-18 20:54:54,868 [INFO] Processed 860000/1569586 samples...\n",
      "2025-01-18 20:54:55,612 [INFO] Processed 870000/1569586 samples...\n",
      "2025-01-18 20:54:56,363 [INFO] Processed 880000/1569586 samples...\n",
      "2025-01-18 20:54:57,120 [INFO] Processed 890000/1569586 samples...\n",
      "2025-01-18 20:54:57,891 [INFO] Processed 900000/1569586 samples...\n",
      "2025-01-18 20:54:58,657 [INFO] Processed 910000/1569586 samples...\n",
      "2025-01-18 20:54:59,561 [INFO] Processed 920000/1569586 samples...\n",
      "2025-01-18 20:55:00,352 [INFO] Processed 930000/1569586 samples...\n",
      "2025-01-18 20:55:01,103 [INFO] Processed 940000/1569586 samples...\n",
      "2025-01-18 20:55:01,862 [INFO] Processed 950000/1569586 samples...\n",
      "2025-01-18 20:55:02,617 [INFO] Processed 960000/1569586 samples...\n",
      "2025-01-18 20:55:03,377 [INFO] Processed 970000/1569586 samples...\n",
      "2025-01-18 20:55:04,142 [INFO] Processed 980000/1569586 samples...\n",
      "2025-01-18 20:55:04,910 [INFO] Processed 990000/1569586 samples...\n",
      "2025-01-18 20:55:05,648 [INFO] Processed 1000000/1569586 samples...\n",
      "2025-01-18 20:55:06,397 [INFO] Processed 1010000/1569586 samples...\n",
      "2025-01-18 20:55:07,133 [INFO] Processed 1020000/1569586 samples...\n",
      "2025-01-18 20:55:07,837 [INFO] Processed 1030000/1569586 samples...\n",
      "2025-01-18 20:55:08,516 [INFO] Processed 1040000/1569586 samples...\n",
      "2025-01-18 20:55:09,196 [INFO] Processed 1050000/1569586 samples...\n",
      "2025-01-18 20:55:09,877 [INFO] Processed 1060000/1569586 samples...\n",
      "2025-01-18 20:55:10,552 [INFO] Processed 1070000/1569586 samples...\n",
      "2025-01-18 20:55:11,230 [INFO] Processed 1080000/1569586 samples...\n",
      "2025-01-18 20:55:11,921 [INFO] Processed 1090000/1569586 samples...\n",
      "2025-01-18 20:55:12,600 [INFO] Processed 1100000/1569586 samples...\n",
      "2025-01-18 20:55:13,269 [INFO] Processed 1110000/1569586 samples...\n",
      "2025-01-18 20:55:13,947 [INFO] Processed 1120000/1569586 samples...\n",
      "2025-01-18 20:55:14,683 [INFO] Processed 1130000/1569586 samples...\n",
      "2025-01-18 20:55:15,423 [INFO] Processed 1140000/1569586 samples...\n",
      "2025-01-18 20:55:16,160 [INFO] Processed 1150000/1569586 samples...\n",
      "2025-01-18 20:55:16,896 [INFO] Processed 1160000/1569586 samples...\n",
      "2025-01-18 20:55:17,616 [INFO] Processed 1170000/1569586 samples...\n",
      "2025-01-18 20:55:18,350 [INFO] Processed 1180000/1569586 samples...\n",
      "2025-01-18 20:55:19,080 [INFO] Processed 1190000/1569586 samples...\n",
      "2025-01-18 20:55:19,794 [INFO] Processed 1200000/1569586 samples...\n",
      "2025-01-18 20:55:20,540 [INFO] Processed 1210000/1569586 samples...\n",
      "2025-01-18 20:55:21,265 [INFO] Processed 1220000/1569586 samples...\n",
      "2025-01-18 20:55:22,011 [INFO] Processed 1230000/1569586 samples...\n",
      "2025-01-18 20:55:22,893 [INFO] Processed 1240000/1569586 samples...\n",
      "2025-01-18 20:55:23,639 [INFO] Processed 1250000/1569586 samples...\n",
      "2025-01-18 20:55:24,386 [INFO] Processed 1260000/1569586 samples...\n",
      "2025-01-18 20:55:25,116 [INFO] Processed 1270000/1569586 samples...\n",
      "2025-01-18 20:55:25,850 [INFO] Processed 1280000/1569586 samples...\n",
      "2025-01-18 20:55:26,578 [INFO] Processed 1290000/1569586 samples...\n",
      "2025-01-18 20:55:27,303 [INFO] Processed 1300000/1569586 samples...\n",
      "2025-01-18 20:55:28,030 [INFO] Processed 1310000/1569586 samples...\n",
      "2025-01-18 20:55:28,764 [INFO] Processed 1320000/1569586 samples...\n",
      "2025-01-18 20:55:29,514 [INFO] Processed 1330000/1569586 samples...\n",
      "2025-01-18 20:55:30,271 [INFO] Processed 1340000/1569586 samples...\n",
      "2025-01-18 20:55:31,033 [INFO] Processed 1350000/1569586 samples...\n",
      "2025-01-18 20:55:31,782 [INFO] Processed 1360000/1569586 samples...\n",
      "2025-01-18 20:55:32,500 [INFO] Processed 1370000/1569586 samples...\n",
      "2025-01-18 20:55:33,227 [INFO] Processed 1380000/1569586 samples...\n",
      "2025-01-18 20:55:33,918 [INFO] Processed 1390000/1569586 samples...\n",
      "2025-01-18 20:55:34,604 [INFO] Processed 1400000/1569586 samples...\n",
      "2025-01-18 20:55:35,325 [INFO] Processed 1410000/1569586 samples...\n",
      "2025-01-18 20:55:36,015 [INFO] Processed 1420000/1569586 samples...\n",
      "2025-01-18 20:55:36,694 [INFO] Processed 1430000/1569586 samples...\n",
      "2025-01-18 20:55:37,508 [INFO] Processed 1440000/1569586 samples...\n",
      "2025-01-18 20:55:38,184 [INFO] Processed 1450000/1569586 samples...\n",
      "2025-01-18 20:55:38,918 [INFO] Processed 1460000/1569586 samples...\n",
      "2025-01-18 20:55:39,657 [INFO] Processed 1470000/1569586 samples...\n",
      "2025-01-18 20:55:40,406 [INFO] Processed 1480000/1569586 samples...\n",
      "2025-01-18 20:55:41,157 [INFO] Processed 1490000/1569586 samples...\n",
      "2025-01-18 20:55:41,912 [INFO] Processed 1500000/1569586 samples...\n",
      "2025-01-18 20:55:42,633 [INFO] Processed 1510000/1569586 samples...\n",
      "2025-01-18 20:55:43,312 [INFO] Processed 1520000/1569586 samples...\n",
      "2025-01-18 20:55:44,002 [INFO] Processed 1530000/1569586 samples...\n",
      "2025-01-18 20:55:44,677 [INFO] Processed 1540000/1569586 samples...\n",
      "2025-01-18 20:55:45,366 [INFO] Processed 1550000/1569586 samples...\n",
      "2025-01-18 20:55:46,091 [INFO] Processed 1560000/1569586 samples...\n",
      "2025-01-18 20:55:46,820 [INFO] \n",
      "Mutual Information Bound Analysis:\n",
      "2025-01-18 20:55:46,820 [INFO] Total samples analyzed: 1569586\n",
      "2025-01-18 20:55:46,821 [INFO] Number of violations: 261525\n",
      "2025-01-18 20:55:46,821 [INFO] Overall violation rate: 16.6620%\n",
      "2025-01-18 20:55:46,821 [INFO] Maximum violation: 0.682696\n",
      "2025-01-18 20:55:46,831 [INFO] System sizes with violations: [6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0]\n",
      "2025-01-18 20:55:46,832 [INFO] \n",
      "Top 10 worst violations:\n",
      "2025-01-18 20:55:46,907 [INFO]   Size 14.0 (nA=3, nB=11): MI=0.689064, VN=0.006368, Diff=0.682696\n",
      "2025-01-18 20:55:46,908 [INFO]   Size 12.0 (nA=9, nB=3): MI=0.685851, VN=0.004241, Diff=0.681610\n",
      "2025-01-18 20:55:46,909 [INFO]   Size 8.0 (nA=5, nB=3): MI=0.693033, VN=0.017803, Diff=0.675230\n",
      "2025-01-18 20:55:46,910 [INFO]   Size 8.0 (nA=2, nB=6): MI=0.664435, VN=0.007636, Diff=0.656799\n",
      "2025-01-18 20:55:46,910 [INFO]   Size 8.0 (nA=2, nB=6): MI=0.659435, VN=0.007421, Diff=0.652014\n",
      "2025-01-18 20:55:46,910 [INFO]   Size 12.0 (nA=3, nB=9): MI=0.684532, VN=0.040866, Diff=0.643666\n",
      "2025-01-18 20:55:46,911 [INFO]   Size 12.0 (nA=1, nB=11): MI=0.663141, VN=0.022429, Diff=0.640712\n",
      "2025-01-18 20:55:46,912 [INFO]   Size 8.0 (nA=2, nB=6): MI=0.682688, VN=0.042217, Diff=0.640471\n",
      "2025-01-18 20:55:46,913 [INFO]   Size 8.0 (nA=4, nB=4): MI=0.740182, VN=0.103651, Diff=0.636531\n",
      "2025-01-18 20:55:46,914 [INFO]   Size 8.0 (nA=1, nB=7): MI=0.637317, VN=0.003588, Diff=0.633729\n",
      "2025-01-18 20:55:46,923 [INFO] \n",
      "Statistics per system size:\n",
      "2025-01-18 20:55:46,924 [INFO] Size 2.0:\n",
      "2025-01-18 20:55:46,924 [INFO]   Samples: 213794\n",
      "2025-01-18 20:55:46,924 [INFO]   Violation rate: 0.0000%\n",
      "2025-01-18 20:55:46,925 [INFO]   Mean MI/VN ratio: 0.5533\n",
      "2025-01-18 20:55:46,926 [INFO]   Max MI/VN ratio: 0.9977\n",
      "2025-01-18 20:55:46,926 [INFO] Size 4.0:\n",
      "2025-01-18 20:55:46,926 [INFO]   Samples: 214394\n",
      "2025-01-18 20:55:46,927 [INFO]   Violation rate: 0.0000%\n",
      "2025-01-18 20:55:46,927 [INFO]   Mean MI/VN ratio: 0.5691\n",
      "2025-01-18 20:55:46,928 [INFO]   Max MI/VN ratio: 1.0000\n",
      "2025-01-18 20:55:46,928 [INFO] Size 6.0:\n",
      "2025-01-18 20:55:46,929 [INFO]   Samples: 215398\n",
      "2025-01-18 20:55:46,930 [INFO]   Violation rate: 1.7075%\n",
      "2025-01-18 20:55:46,931 [INFO]   Mean MI/VN ratio: 0.5869\n",
      "2025-01-18 20:55:46,931 [INFO]   Max MI/VN ratio: 4.7910\n",
      "2025-01-18 20:55:46,931 [INFO] Size 8.0:\n",
      "2025-01-18 20:55:46,931 [INFO]   Samples: 214912\n",
      "2025-01-18 20:55:46,932 [INFO]   Violation rate: 19.2972%\n",
      "2025-01-18 20:55:46,932 [INFO]   Mean MI/VN ratio: 0.9384\n",
      "2025-01-18 20:55:46,933 [INFO]   Max MI/VN ratio: 271.7670\n",
      "2025-01-18 20:55:46,933 [INFO] Size 10.0:\n",
      "2025-01-18 20:55:46,934 [INFO]   Samples: 214804\n",
      "2025-01-18 20:55:46,934 [INFO]   Violation rate: 40.0821%\n",
      "2025-01-18 20:55:46,935 [INFO]   Mean MI/VN ratio: 1.1545\n",
      "2025-01-18 20:55:46,935 [INFO]   Max MI/VN ratio: 157.4658\n",
      "2025-01-18 20:55:46,936 [INFO] Size 12.0:\n",
      "2025-01-18 20:55:46,936 [INFO]   Samples: 215990\n",
      "2025-01-18 20:55:46,936 [INFO]   Violation rate: 33.7733%\n",
      "2025-01-18 20:55:46,937 [INFO]   Mean MI/VN ratio: 1.0576\n",
      "2025-01-18 20:55:46,937 [INFO]   Max MI/VN ratio: 161.7035\n",
      "2025-01-18 20:55:46,938 [INFO] Size 14.0:\n",
      "2025-01-18 20:55:46,938 [INFO]   Samples: 215294\n",
      "2025-01-18 20:55:46,938 [INFO]   Violation rate: 22.4823%\n",
      "2025-01-18 20:55:46,938 [INFO]   Mean MI/VN ratio: 0.8749\n",
      "2025-01-18 20:55:46,939 [INFO]   Max MI/VN ratio: 172.7026\n",
      "2025-01-18 20:55:46,939 [INFO] Size 16.0:\n",
      "2025-01-18 20:55:46,940 [INFO]   Samples: 32659\n",
      "2025-01-18 20:55:46,940 [INFO]   Violation rate: 15.0893%\n",
      "2025-01-18 20:55:46,941 [INFO]   Mean MI/VN ratio: 0.6816\n",
      "2025-01-18 20:55:46,941 [INFO]   Max MI/VN ratio: 8.1419\n",
      "2025-01-18 20:55:46,942 [INFO] Size 18.0:\n",
      "2025-01-18 20:55:46,942 [INFO]   Samples: 32341\n",
      "2025-01-18 20:55:46,943 [INFO]   Violation rate: 12.3651%\n",
      "2025-01-18 20:55:46,943 [INFO]   Mean MI/VN ratio: 0.5660\n",
      "2025-01-18 20:55:46,943 [INFO]   Max MI/VN ratio: 40.4762\n",
      "2025-01-18 20:55:48,030 [WARNING] Found violations! Details saved to mi_violations.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def setup_logging():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        handlers=[logging.StreamHandler()]\n",
    "    )\n",
    "\n",
    "class SimpleDataset:\n",
    "    \"\"\"Simplified dataset class to load the processed data.pt file\"\"\"\n",
    "    def __init__(self, root='./processed_experimental/processed'):\n",
    "        data_path = os.path.join(root, 'data.pt')\n",
    "        self.data, self.slices = torch.load(data_path)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.slices['x'].size(0) - 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, slice):\n",
    "            return [self[i] for i in range(*idx.indices(len(self)))]\n",
    "        \n",
    "        # Create a class to hold the data similar to Data object\n",
    "        class DataObject:\n",
    "            pass\n",
    "        \n",
    "        data = DataObject()\n",
    "        \n",
    "        for key in self.slices.keys():\n",
    "            item, slices = self.data[key], self.slices[key]\n",
    "            s, e = slices[idx].item(), slices[idx + 1].item()\n",
    "            data.__setattr__(key, item[s:e])\n",
    "        \n",
    "        return data\n",
    "\n",
    "def verify_mutual_information_bound(dataset):\n",
    "    \"\"\"\n",
    "    Verify that mutual information is always lower than or equal to von Neumann entropy\n",
    "    and analyze any violations.\n",
    "    \"\"\"\n",
    "    logging.info(\"Verifying Mutual Information bounds...\")\n",
    "    \n",
    "    violations = []\n",
    "    violation_sizes = []\n",
    "    max_violation = 0\n",
    "    total_samples = len(dataset)\n",
    "    \n",
    "    # Collect statistics per system size\n",
    "    size_stats = {}\n",
    "    \n",
    "    for idx in range(len(dataset)):\n",
    "        data = dataset[idx]\n",
    "        mi = data.mutual_info.item()\n",
    "        vn = data.y.item()  # von Neumann entropy\n",
    "        system_size = data.system_size.item()\n",
    "        \n",
    "        # Check if MI > VN (allowing for small numerical errors)\n",
    "        if mi > vn + 1e-6:  # tolerance of 1e-6\n",
    "            violations.append({\n",
    "                'idx': idx,\n",
    "                'mi': mi,\n",
    "                'vn': vn,\n",
    "                'difference': mi - vn,\n",
    "                'system_size': system_size,\n",
    "                'nA': data.nA.item(),\n",
    "                'nB': data.nB.item()\n",
    "            })\n",
    "            violation_sizes.append(system_size)\n",
    "            max_violation = max(max_violation, mi - vn)\n",
    "            \n",
    "        # Update size statistics\n",
    "        if system_size not in size_stats:\n",
    "            size_stats[system_size] = {\n",
    "                'count': 0,\n",
    "                'violations': 0,\n",
    "                'max_ratio': 0,  # MI/VN ratio\n",
    "                'mean_ratio': 0,\n",
    "                'ratios': []\n",
    "            }\n",
    "        \n",
    "        stats = size_stats[system_size]\n",
    "        stats['count'] += 1\n",
    "        if mi > vn + 1e-6:\n",
    "            stats['violations'] += 1\n",
    "        \n",
    "        # Calculate MI/VN ratio\n",
    "        ratio = mi / (vn + 1e-10)  # avoid division by zero\n",
    "        stats['ratios'].append(ratio)\n",
    "        stats['max_ratio'] = max(stats['max_ratio'], ratio)\n",
    "\n",
    "        # Print progress every 10k samples\n",
    "        if (idx + 1) % 10000 == 0:\n",
    "            logging.info(f\"Processed {idx + 1}/{total_samples} samples...\")\n",
    "    \n",
    "    # Compute final statistics per size\n",
    "    for size, stats in size_stats.items():\n",
    "        stats['violation_rate'] = (stats['violations'] / stats['count']) * 100\n",
    "        stats['mean_ratio'] = np.mean(stats['ratios'])\n",
    "    \n",
    "    # Print summary\n",
    "    logging.info(f\"\\nMutual Information Bound Analysis:\")\n",
    "    logging.info(f\"Total samples analyzed: {total_samples}\")\n",
    "    logging.info(f\"Number of violations: {len(violations)}\")\n",
    "    logging.info(f\"Overall violation rate: {(len(violations)/total_samples)*100:.4f}%\")\n",
    "    if violations:\n",
    "        logging.info(f\"Maximum violation: {max_violation:.6f}\")\n",
    "        logging.info(f\"System sizes with violations: {sorted(set(violation_sizes))}\")\n",
    "        \n",
    "        logging.info(\"\\nTop 10 worst violations:\")\n",
    "        sorted_violations = sorted(violations, key=lambda x: x['difference'], reverse=True)\n",
    "        for v in sorted_violations[:10]:\n",
    "            logging.info(\n",
    "                f\"  Size {v['system_size']:2} (nA={v['nA']:.0f}, nB={v['nB']:.0f}): \"\n",
    "                f\"MI={v['mi']:.6f}, VN={v['vn']:.6f}, Diff={v['difference']:.6f}\"\n",
    "            )\n",
    "    \n",
    "    logging.info(\"\\nStatistics per system size:\")\n",
    "    for size in sorted(size_stats.keys()):\n",
    "        stats = size_stats[size]\n",
    "        logging.info(f\"Size {size:2}:\")\n",
    "        logging.info(f\"  Samples: {stats['count']}\")\n",
    "        logging.info(f\"  Violation rate: {stats['violation_rate']:.4f}%\")\n",
    "        logging.info(f\"  Mean MI/VN ratio: {stats['mean_ratio']:.4f}\")\n",
    "        logging.info(f\"  Max MI/VN ratio: {stats['max_ratio']:.4f}\")\n",
    "    \n",
    "    return violations, size_stats\n",
    "\n",
    "def main():\n",
    "    setup_logging()\n",
    "    \n",
    "    # Load dataset\n",
    "    try:\n",
    "        dataset = SimpleDataset()\n",
    "        logging.info(f\"Loaded dataset with {len(dataset)} samples\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading dataset: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Verify MI bounds\n",
    "    violations, size_stats = verify_mutual_information_bound(dataset)\n",
    "    \n",
    "    # Save violations if any found\n",
    "    if violations:\n",
    "        violation_data = pd.DataFrame(violations)\n",
    "        violation_data.to_csv('mi_violations.csv', index=False)\n",
    "        logging.warning(f\"Found violations! Details saved to mi_violations.csv\")\n",
    "    else:\n",
    "        logging.info(\"No violations found - MI is a proper lower bound for VN entropy\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f633edb7-597a-4344-a123-6bcff730b0c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
