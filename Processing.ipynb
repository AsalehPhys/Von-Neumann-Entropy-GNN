{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d36c4ca-0cb8-4c4b-8bfb-5aff5e2bf07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-24 14:32:05,686 [INFO] First few rows of the dataset:\n",
      "2024-12-24 14:32:05,700 [INFO]    Nx      Delta      Omega  x_spacing  y_spacing      Energy  \\\n",
      "0   7  40.030606   9.988305   7.118764   6.387401 -239.252283   \n",
      "1   7  23.347429  39.473622   4.798695   6.056938 -221.844105   \n",
      "2   7   9.494868  20.161970   4.728944   7.021446 -105.435782   \n",
      "3   7   5.336133  10.603160   4.180909   5.301321  -44.204418   \n",
      "4   7  16.238134  16.933983   6.918425   6.550230 -122.907435   \n",
      "\n",
      "                                      Top_50_Indices  \\\n",
      "0  [6553, 9830, 6425, 9766, 6297, 6537, 9798, 931...   \n",
      "1  [0, 1, 8192, 4096, 2, 4097, 8194, 8193, 4098, ...   \n",
      "2  [0, 8192, 2, 1, 4096, 8194, 4097, 8193, 4098, ...   \n",
      "3  [0, 1, 4096, 8192, 2, 8193, 4098, 4097, 8194, ...   \n",
      "4  [8738, 4369, 8802, 4497, 9762, 8742, 4377, 641...   \n",
      "\n",
      "                                Top_50_Probabilities  Von_Neumann_Entropy  \\\n",
      "0  [0.38220482824683005, 0.38220477448728546, 0.0...             0.666110   \n",
      "1  [0.007107020650037715, 0.006130362632967342, 0...             0.189754   \n",
      "2  [0.011375711808993182, 0.00825373486573358, 0....             0.545898   \n",
      "3  [0.04656726717704942, 0.027560423273533925, 0....             0.489718   \n",
      "4  [0.004426444717012529, 0.004426444717010733, 0...             1.229817   \n",
      "\n",
      "   N_A  Subsystem_Mask  \n",
      "0   13  11101111111111  \n",
      "1    1  00000000001000  \n",
      "2   11  11111011011011  \n",
      "3    2  00000000100001  \n",
      "4    5  01100000110010  \n",
      "2024-12-24 14:32:05,727 [INFO] \n",
      "Dataset Information:\n",
      "2024-12-24 14:32:05,884 [INFO] None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1500000 entries, 0 to 1499999\n",
      "Data columns (total 11 columns):\n",
      " #   Column                Non-Null Count    Dtype  \n",
      "---  ------                --------------    -----  \n",
      " 0   Nx                    1500000 non-null  int64  \n",
      " 1   Delta                 1500000 non-null  float64\n",
      " 2   Omega                 1500000 non-null  float64\n",
      " 3   x_spacing             1500000 non-null  float64\n",
      " 4   y_spacing             1500000 non-null  float64\n",
      " 5   Energy                1500000 non-null  float64\n",
      " 6   Top_50_Indices        1500000 non-null  object \n",
      " 7   Top_50_Probabilities  1500000 non-null  object \n",
      " 8   Von_Neumann_Entropy   1500000 non-null  float64\n",
      " 9   N_A                   1500000 non-null  int64  \n",
      " 10  Subsystem_Mask        1500000 non-null  object \n",
      "dtypes: float64(6), int64(2), object(3)\n",
      "memory usage: 125.9+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    }
   ],
   "source": [
    "# New Features \n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, Data, DataLoader\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import logging\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "CONFIG = {\n",
    "    'data_path': r'C:\\Users\\amssa\\Documents\\Codes\\New\\Von-Neumann-Entropy-GNN\\Random sub sets\\spin_system_properties_gpu1-7.parquet',\n",
    "    'processed_dir': './processed',\n",
    "    'processed_file': './processed/data.pt',\n",
    "    'batch_size': 1024,\n",
    "    'random_seed': 42,\n",
    "    'distance_threshold': 25\n",
    "}\n",
    "\n",
    "def setup_logging():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        handlers=[\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "setup_logging()\n",
    "set_seed(CONFIG['random_seed'])\n",
    "\n",
    "class SpinSystemDataset(InMemoryDataset):\n",
    "    def __init__(self, dataframe, root='.', transform=None, pre_transform=None):\n",
    "        self.df = dataframe\n",
    "        super(SpinSystemDataset, self).__init__(root, transform, pre_transform)\n",
    "        if os.path.exists(self.processed_paths[0]):\n",
    "            self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        else:\n",
    "            self.process()\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        data_list = []\n",
    "        for idx, row in self.df.iterrows():\n",
    "            Nx = row['Nx']\n",
    "            Ny = 2\n",
    "            N = Nx * Ny\n",
    "\n",
    "            x_spacing = row['x_spacing']\n",
    "            y_spacing = row['y_spacing']\n",
    "            positions = np.array([\n",
    "                (col * x_spacing, row_idx * y_spacing)\n",
    "                for row_idx in range(Nx) for col in range(Ny)\n",
    "            ])\n",
    "            positions = torch.tensor(positions, dtype=torch.float)\n",
    "\n",
    "            # Normalize positions\n",
    "            pos_min = positions.min(dim=0).values\n",
    "            pos_max = positions.max(dim=0).values\n",
    "            normalized_positions = (positions - pos_min) / (pos_max - pos_min + 1e-8)\n",
    "\n",
    "            state_indices = row['Top_50_Indices']\n",
    "            state_probs = row['Top_50_Probabilities']\n",
    "\n",
    "            p_rydberg = [0.0] * N\n",
    "            for state, prob in zip(state_indices, state_probs):\n",
    "                state = int(state)\n",
    "                for i in range(N):\n",
    "                    if state & (1 << i):\n",
    "                        p_rydberg[i] += prob\n",
    "            p_rydberg = torch.tensor(p_rydberg, dtype=torch.float).unsqueeze(1)  # [N, 1]\n",
    "\n",
    "            try:\n",
    "                N_A_idx = int(row['N_A'])\n",
    "            except ValueError:\n",
    "                logging.error(f\"Graph {idx}: N_A value '{row['N_A']}' is not an integer.\")\n",
    "                raise\n",
    "            N_A_feature = torch.zeros((N, 1), dtype=torch.float)\n",
    "            if 0 <= N_A_idx < N:\n",
    "                N_A_feature[N_A_idx] = 1.0\n",
    "            else:\n",
    "                logging.warning(f'Graph {idx}: N_A index {N_A_idx} out of range for {N} nodes.')\n",
    "\n",
    "            # Node features: normalized_positions (2), p_rydberg (1), N_A_feature (1)\n",
    "            node_features = torch.cat([normalized_positions, p_rydberg, N_A_feature], dim=1)  # [N, 4]\n",
    "\n",
    "            distance_threshold = CONFIG['distance_threshold']\n",
    "            nbrs = NearestNeighbors(radius=distance_threshold, algorithm='ball_tree').fit(positions.numpy())\n",
    "            indices = nbrs.radius_neighbors(positions.numpy(), return_distance=False)\n",
    "\n",
    "            edge_index = []\n",
    "            for i in range(N):\n",
    "                for j in indices[i]:\n",
    "                    if i < j:\n",
    "                        edge_index.append([i, j])\n",
    "            edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "            # Compute edge attributes (1/r^6)\n",
    "            if edge_index.size(1) > 0:\n",
    "                pos_i = positions[edge_index[0]]\n",
    "                pos_j = positions[edge_index[1]]\n",
    "                distances = torch.norm(pos_i - pos_j, dim=1, keepdim=True)\n",
    "                epsilon = 1e-8\n",
    "                inv_r6 = 1.0 / (distances.pow(6) + epsilon)  # [E, 1]\n",
    "            else:\n",
    "                distances = torch.empty((0, 1), dtype=torch.float)\n",
    "                inv_r6 = torch.empty((0, 1), dtype=torch.float)\n",
    "\n",
    "            # Compute joint probabilities on edges for correlation\n",
    "            edge_tuples = [(edge_index[0, k].item(), edge_index[1, k].item()) for k in range(edge_index.size(1))]\n",
    "            edge_joint_probs = {edge: 0.0 for edge in edge_tuples}\n",
    "\n",
    "            for state, prob in zip(state_indices, state_probs):\n",
    "                state = int(state)\n",
    "                rydberg_particles = [i for i in range(N) if state & (1 << i)]\n",
    "                for i_ in rydberg_particles:\n",
    "                    for j_ in rydberg_particles:\n",
    "                        if i_ < j_ and (i_, j_) in edge_joint_probs:\n",
    "                            edge_joint_probs[(i_, j_)] += prob\n",
    "\n",
    "            edge_correlation = []\n",
    "            for k in range(edge_index.size(1)):\n",
    "                i_ = edge_index[0, k].item()\n",
    "                j_ = edge_index[1, k].item()\n",
    "                joint_prob = edge_joint_probs.get((i_, j_), 0.0)\n",
    "                p_ryd_i = p_rydberg[i_].item()\n",
    "                p_ryd_j = p_rydberg[j_].item()\n",
    "                correlation = joint_prob - p_ryd_i * p_ryd_j\n",
    "                edge_correlation.append([correlation])\n",
    "\n",
    "            edge_correlation = torch.tensor(edge_correlation, dtype=torch.float)  # [E, 1]\n",
    "\n",
    "            if edge_index.size(1) > 0:\n",
    "                edge_attr = torch.cat([inv_r6, edge_correlation], dim=1)  # [E, 2]\n",
    "            else:\n",
    "                edge_attr = torch.empty((0, 2), dtype=torch.float)\n",
    "\n",
    "            # Von Neumann Entropy (target)\n",
    "            original_entropy = row['Von_Neumann_Entropy']\n",
    "            entropy = torch.tensor([np.log(original_entropy + 1e-9)], dtype=torch.float)\n",
    "\n",
    "            # Derived features from experimental data\n",
    "            # Compute P_rydberg statistics\n",
    "            p_ryd_mean = p_rydberg.mean().item()\n",
    "            p_ryd_std = p_rydberg.std().item() if p_rydberg.numel() > 1 else 0.0\n",
    "\n",
    "            # Compute correlation stats\n",
    "            if edge_correlation.numel() > 1:\n",
    "                edge_corr_mean = edge_correlation.mean().item()\n",
    "                edge_corr_std = edge_correlation.std().item()\n",
    "            elif edge_correlation.numel() == 1:\n",
    "                edge_corr_mean = edge_correlation.mean().item()\n",
    "                edge_corr_std = 0.0\n",
    "            else:\n",
    "                edge_corr_mean = 0.0\n",
    "                edge_corr_std = 0.0\n",
    "\n",
    "            # Compute average node degree\n",
    "            if edge_index.size(1) > 0:\n",
    "                degrees = torch.bincount(torch.cat([edge_index[0], edge_index[1]]), minlength=N)\n",
    "                avg_degree = degrees.float().mean().item()\n",
    "            else:\n",
    "                avg_degree = 0.0\n",
    "\n",
    "            # Compute spatial metrics: mean and std of distances between connected nodes\n",
    "            if distances.numel() > 1:\n",
    "                pos_dist_mean = distances.mean().item()\n",
    "                pos_dist_std = distances.std().item()\n",
    "            elif distances.numel() == 1:\n",
    "                pos_dist_mean = distances.mean().item()\n",
    "                pos_dist_std = 0.0\n",
    "            else:\n",
    "                pos_dist_mean = 0.0\n",
    "                pos_dist_std = 0.0\n",
    "\n",
    "            # Nx, Ny are part of experimental setup (system size)\n",
    "            global_features = torch.tensor([\n",
    "                Nx, Ny,\n",
    "                p_ryd_mean, p_ryd_std,\n",
    "                edge_corr_mean, edge_corr_std,\n",
    "                avg_degree,\n",
    "                pos_dist_mean, pos_dist_std\n",
    "            ], dtype=torch.float).unsqueeze(0)  # [1, 9]\n",
    "\n",
    "            data = Data(\n",
    "                x=node_features, \n",
    "                edge_index=edge_index, \n",
    "                edge_attr=edge_attr, \n",
    "                y=entropy\n",
    "            )\n",
    "            data.global_features = global_features\n",
    "\n",
    "            data_list.append(data)\n",
    "\n",
    "        if self.pre_transform:\n",
    "            data_list = [self.pre_transform(d) for d in data_list]\n",
    "\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "def load_data(config):\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch\")\n",
    "    if not os.path.exists(config['data_path']):\n",
    "        logging.error(f\"Data file not found at {config['data_path']}\")\n",
    "        raise FileNotFoundError(f\"Data file not found at {config['data_path']}\")\n",
    "\n",
    "    df = pd.read_parquet(config['data_path'])\n",
    "\n",
    "    logging.info(\"First few rows of the dataset:\")\n",
    "    logging.info(df.head())\n",
    "    logging.info(\"\\nDataset Information:\")\n",
    "    logging.info(df.info())\n",
    "\n",
    "    df_shuffled = df.sample(frac=1, random_state=config['random_seed']).reset_index(drop=True)\n",
    "    dataset = SpinSystemDataset(df_shuffled, root=config['processed_dir'])\n",
    "\n",
    "    logging.info(f'\\nTotal graphs in dataset: {len(dataset)}')\n",
    "    logging.info(f'\\nSample Data Object:')\n",
    "    logging.info(dataset[0])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def split_dataset(dataset, config):\n",
    "    total = len(dataset)\n",
    "    train_end = int(0.8 * total)\n",
    "    val_end = int(0.9 * total)\n",
    "\n",
    "    train_dataset = dataset[:train_end]\n",
    "    val_dataset = dataset[train_end:val_end]\n",
    "    test_dataset = dataset[val_end:]\n",
    "\n",
    "    logging.info(f'\\nTraining graphs: {len(train_dataset)}')\n",
    "    logging.info(f'Validation graphs: {len(val_dataset)}')\n",
    "    logging.info(f'Test graphs: {len(test_dataset)}')\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def create_dataloaders(train_dataset, val_dataset, test_dataset, config):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def main():\n",
    "    dataset = load_data(CONFIG)\n",
    "    train_dataset, val_dataset, test_dataset = split_dataset(dataset, CONFIG)\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(train_dataset, val_dataset, test_dataset, CONFIG)\n",
    "\n",
    "    logging.info(\"Data processing and loading completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad38e3d-58c5-451f-b3af-534d409e5a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
