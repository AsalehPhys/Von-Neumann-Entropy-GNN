{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632ea357-205f-4ccb-b156-5677551bad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, Data, DataLoader\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import logging\n",
    "import random\n",
    "import warnings\n",
    "import joblib  \n",
    "\n",
    "\n",
    "CONFIG = {\n",
    "    'data_path': r'C:\\Users\\amssa\\Documents\\Codes\\1-6\\data1-6.parquet',  # Original data file\n",
    "    'processed_dir': './processed',\n",
    "    'processed_file': './processed/data.pt',\n",
    "    'batch_size': 1024,\n",
    "    'random_seed': 42,\n",
    "    'distance_threshold': 25,  \n",
    "    'scalers_path': 'scalers.pkl',  \n",
    "}\n",
    "\n",
    "\n",
    "def setup_logging():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        handlers=[\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "setup_logging()\n",
    "set_seed(CONFIG['random_seed'])\n",
    "\n",
    "\n",
    "class SpinSystemDataset(InMemoryDataset):\n",
    "    def __init__(self, dataframe, root='.', transform=None, pre_transform=None):\n",
    "        self.df = dataframe\n",
    "        super(SpinSystemDataset, self).__init__(root, transform, pre_transform)\n",
    "        if os.path.exists(self.processed_paths[0]):\n",
    "            self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        else:\n",
    "            self.process()\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "\n",
    "        data_list = []\n",
    "        for idx, row in self.df.iterrows():\n",
    "            # Extract parameters\n",
    "            Nx = row['Nx']\n",
    "            Ny = 2  # As defined in your data generation\n",
    "            N = Nx * Ny  # Total number of spins\n",
    "\n",
    "            # Node Features: Position (x, y), p_rydberg\n",
    "            x_spacing = row['x_spacing']\n",
    "            y_spacing = row['y_spacing']\n",
    "            positions = np.array([\n",
    "                (col * x_spacing, row_idx * y_spacing)\n",
    "                for row_idx in range(Nx) for col in range(Ny)\n",
    "            ])\n",
    "            positions = torch.tensor(positions, dtype=torch.float)\n",
    "\n",
    "            # Normalize positions\n",
    "            pos_min = positions.min(dim=0).values\n",
    "            pos_max = positions.max(dim=0).values\n",
    "            normalized_positions = (positions - pos_min) / (pos_max - pos_min + 1e-8)\n",
    "\n",
    "            # Extracting 'Top_50_Indices' and 'Top_50_Probabilities'\n",
    "            state_indices = row['Top_50_Indices'] \n",
    "            state_probs = row['Top_50_Probabilities']  \n",
    "\n",
    "            num_particles = N  \n",
    "\n",
    "            p_rydberg = [0.0] * num_particles\n",
    "\n",
    "            for state, prob in zip(state_indices, state_probs):\n",
    "                state = int(state)\n",
    "                for i in range(num_particles):\n",
    "                    if state & (1 << i):\n",
    "                        p_rydberg[i] += prob\n",
    "\n",
    "            p_rydberg = torch.tensor(p_rydberg, dtype=torch.float).unsqueeze(1)  # [N, 1]\n",
    "\n",
    "            try:\n",
    "                N_A_idx = int(row['N_A'])  \n",
    "            except ValueError:\n",
    "                logging.error(f\"Graph {idx}: N_A value '{row['N_A']}' is not an integer.\")\n",
    "                raise\n",
    "\n",
    "            N_A_feature = torch.zeros((N, 1), dtype=torch.float)\n",
    "            if 0 <= N_A_idx < N:\n",
    "                N_A_feature[N_A_idx] = 1.0\n",
    "            else:\n",
    "                logging.warning(f'Graph {idx}: N_A index {N_A_idx} out of range for {N} nodes.')\n",
    "\n",
    "            # Concatenate node features: [x, y, p_rydberg, N_A]\n",
    "            node_features = torch.cat([normalized_positions, p_rydberg, N_A_feature], dim=1)  # [N, 4]\n",
    "\n",
    "            # Edge Index and Edge Attributes using Distance Threshold\n",
    "            distance_threshold = CONFIG['distance_threshold']\n",
    "            nbrs = NearestNeighbors(radius=distance_threshold, algorithm='ball_tree').fit(positions.numpy())\n",
    "            indices = nbrs.radius_neighbors(positions.numpy(), return_distance=False)\n",
    "\n",
    "            edge_index = []\n",
    "            for i in range(N):\n",
    "                for j in indices[i]:\n",
    "                    if i < j:\n",
    "                        edge_index.append([i, j])\n",
    "            edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "            logging.debug(f'Graph {idx}: Created {edge_index.size(1)} edges using distance threshold.')\n",
    "\n",
    "            # Compute edge attributes (1/r^6)\n",
    "            if edge_index.size(1) > 0:\n",
    "                pos_i = positions[edge_index[0]]\n",
    "                pos_j = positions[edge_index[1]]\n",
    "                distances = torch.norm(pos_i - pos_j, dim=1, keepdim=True)\n",
    "                epsilon = 1e-8\n",
    "                inv_r6 = 1.0 / (distances.pow(6) + epsilon)  # [E, 1]\n",
    "            else:\n",
    "                inv_r6 = torch.empty((0, 1), dtype=torch.float)\n",
    "\n",
    "            # Compute joint probabilities for edges\n",
    "            edge_tuples = [(edge_index[0, k].item(), edge_index[1, k].item()) for k in range(edge_index.size(1))]\n",
    "            edge_joint_probs = {edge: 0.0 for edge in edge_tuples}\n",
    "\n",
    "            for state, prob in zip(state_indices, state_probs):\n",
    "                state = int(state)\n",
    "                rydberg_particles = [i for i in range(N) if state & (1 << i)]\n",
    "                rydberg_set = set(rydberg_particles)\n",
    "                for i in rydberg_particles:\n",
    "                    for j in rydberg_particles:\n",
    "                        if i < j and (i, j) in edge_joint_probs:\n",
    "                            edge_joint_probs[(i, j)] += prob\n",
    "\n",
    "            # Compute correlation function for edges\n",
    "            edge_correlation = []\n",
    "            for k in range(edge_index.size(1)):\n",
    "                i = edge_index[0, k].item()\n",
    "                j = edge_index[1, k].item()\n",
    "                joint_prob = edge_joint_probs.get((i, j), 0.0)\n",
    "                p_ryd_i = p_rydberg[i].item()\n",
    "                p_ryd_j = p_rydberg[j].item()\n",
    "                correlation = joint_prob - p_ryd_i * p_ryd_j\n",
    "                edge_correlation.append([correlation])\n",
    "\n",
    "            edge_correlation = torch.tensor(edge_correlation, dtype=torch.float)  # [E, 1]\n",
    "\n",
    "            if edge_index.size(1) > 0:\n",
    "                edge_attr = torch.cat([inv_r6, edge_correlation], dim=1)  # [E, 2]\n",
    "            else:\n",
    "                edge_attr = torch.empty((0, 2), dtype=torch.float)\n",
    "\n",
    "            # Target: Von Neumann Entropy\n",
    "            entropy = torch.tensor([row['Von_Neumann_Entropy']], dtype=torch.float)\n",
    "            entropy = torch.log(entropy + 1e-9)  \n",
    "\n",
    "            # Graph-level features: Omega, Delta, Energy\n",
    "            Omega = torch.tensor([[row['Omega']]], dtype=torch.float)       # [1, 1]\n",
    "            Delta = torch.tensor([[row['Delta']]], dtype=torch.float)       # [1, 1]\n",
    "            Energy = torch.tensor([[row['Energy']]], dtype=torch.float)     # [1, 1]\n",
    "\n",
    "            # Create Data object\n",
    "            data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr, y=entropy)\n",
    "            data.Omega = Omega\n",
    "            data.Delta = Delta\n",
    "            data.Energy = Energy\n",
    "\n",
    "            data_list.append(data)\n",
    "\n",
    "        \n",
    "        if self.pre_transform:\n",
    "            data_list = [self.pre_transform(d) for d in data_list]\n",
    "\n",
    "        \n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "\n",
    "def load_data(config):\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch\")\n",
    "\n",
    "    \n",
    "    if not os.path.exists(config['data_path']):\n",
    "        logging.error(f\"Data file not found at {config['data_path']}\")\n",
    "        raise FileNotFoundError(f\"Data file not found at {config['data_path']}\")\n",
    "\n",
    "    df = pd.read_parquet(config['data_path'])\n",
    "\n",
    "    \n",
    "    logging.info(\"First few rows of the dataset:\")\n",
    "    logging.info(df.head())\n",
    "    logging.info(\"\\nDataset Information:\")\n",
    "    logging.info(df.info())\n",
    "\n",
    "    \n",
    "    df_shuffled = df.sample(frac=1, random_state=config['random_seed']).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    dataset = SpinSystemDataset(df_shuffled, root=config['processed_dir'])\n",
    "\n",
    "    logging.info(f'\\nTotal graphs in dataset: {len(dataset)}')\n",
    "    logging.info(f'\\nSample Data Object:')\n",
    "    logging.info(dataset[0])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def split_dataset(dataset, config):\n",
    "    \n",
    "    total = len(dataset)\n",
    "    train_end = int(0.8 * total)\n",
    "    val_end = int(0.9 * total)\n",
    "\n",
    "    \n",
    "    train_dataset = dataset[:train_end]\n",
    "    val_dataset = dataset[train_end:val_end]\n",
    "    test_dataset = dataset[val_end:]\n",
    "\n",
    "    logging.info(f'\\nTraining graphs: {len(train_dataset)}')\n",
    "    logging.info(f'Validation graphs: {len(val_dataset)}')\n",
    "    logging.info(f'Test graphs: {len(test_dataset)}')\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def normalize_features(train_dataset, val_dataset, test_dataset, scalers_path):\n",
    "    # Initialize scalers\n",
    "    scalers = {\n",
    "        'Omega': StandardScaler(),\n",
    "        'Delta': StandardScaler(),\n",
    "        'Energy': StandardScaler()\n",
    "    }\n",
    "\n",
    "    # Fit scalers on training data\n",
    "    features = ['Omega', 'Delta', 'Energy']\n",
    "    for feature in features:\n",
    "        try:\n",
    "            train_values = torch.cat([getattr(data, feature) for data in train_dataset], dim=0).numpy()\n",
    "            scalers[feature].fit(train_values)\n",
    "        except AttributeError as e:\n",
    "            logging.error(f\"AttributeError while accessing feature '{feature}': {e}\")\n",
    "            raise\n",
    "\n",
    "    \n",
    "    joblib.dump(scalers, scalers_path)\n",
    "    logging.info(f\"Scalers saved to '{scalers_path}'.\")\n",
    "\n",
    "    \n",
    "    def normalize(dataset, scalers):\n",
    "        for data in dataset:\n",
    "            for feature in features:\n",
    "                scaled = scalers[feature].transform(getattr(data, feature).numpy())\n",
    "                setattr(data, feature, torch.tensor(scaled, dtype=torch.float))\n",
    "        return dataset\n",
    "\n",
    "    # Apply normalization\n",
    "    train_dataset = normalize(train_dataset, scalers)\n",
    "    val_dataset = normalize(val_dataset, scalers)\n",
    "    test_dataset = normalize(test_dataset, scalers)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def create_dataloaders(train_dataset, val_dataset, test_dataset, config):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load and prepare dataset\n",
    "    dataset = load_data(CONFIG)\n",
    "    train_dataset, val_dataset, test_dataset = split_dataset(dataset, CONFIG)\n",
    "    train_dataset, val_dataset, test_dataset = normalize_features(train_dataset, val_dataset, test_dataset, CONFIG['scalers_path'])\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(train_dataset, val_dataset, test_dataset, CONFIG)\n",
    "\n",
    "    logging.info(\"Data processing and loading completed successfully.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
