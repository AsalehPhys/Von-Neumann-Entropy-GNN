{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c24b2ba-27e5-4fbc-8451-1133a6a128e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n",
      "2025-01-13 18:27:41,250 [INFO] Loading existing processed dataset...\n",
      "C:\\Users\\amssa\\AppData\\Local\\Temp\\ipykernel_26684\\926348705.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, self.slices = torch.load(self.processed_paths[0])\n",
      "2025-01-13 18:27:41,274 [INFO] Finished processing. Dataset length: 5000\n",
      "2025-01-13 18:27:41,275 [INFO] Sample data object: Data(x=[16, 8], edge_index=[2, 96], edge_attr=[96, 4], y=[1], Omega=[1, 1], Delta=[1, 1], Energy=[1, 1], system_size=[1, 1], total_rydberg=[1], rydberg_density=[1], config_entropy=[1, 1], nA=[1, 1], nB=[1, 1])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "data_processing.py\n",
    "\n",
    "Script that:\n",
    "1) Reads your raw parquet file with columns like Nx, Delta, Omega, Subsystem_Mask, etc.\n",
    "2) Builds a PyTorch Geometric InMemoryDataset with:\n",
    "   - Node features\n",
    "   - Edge features\n",
    "   - Graph-level targets (e.g., Von_Neumann_Entropy)\n",
    "   - Graph-level fields (Omega, Delta, Energy, total_rydberg, system_size, etc.)\n",
    "   - Subsystem sizes nA = size of partition A, nB = N - nA\n",
    "3) Saves the final dataset as data.pt in './processed' (by default).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# -------------------------------------------------------------------------\n",
    "CONFIG = {\n",
    "    'data_path': 'spin_system_properties_cpu_test.parquet', \n",
    "    'processed_dir': './processed3',\n",
    "    'processed_file_name': 'data.pt',\n",
    "    'distance_threshold': 25,  \n",
    "    'random_seed': 42,\n",
    "}\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Logging Setup\n",
    "# -------------------------------------------------------------------------\n",
    "def setup_logging():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        handlers=[logging.StreamHandler()]\n",
    "    )\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Custom Dataset\n",
    "# -------------------------------------------------------------------------\n",
    "class SpinSystemDataset(InMemoryDataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class that:\n",
    "     - Reads each row of a DataFrame\n",
    "     - Builds node features, edge_index, edge_attr\n",
    "     - Stores graph-level fields\n",
    "     - Now also stores nA, nB (subsystem sizes)\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, root='.', transform=None, pre_transform=None):\n",
    "        self.df = dataframe\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        if os.path.exists(self.processed_paths[0]):\n",
    "            logging.info(\"Loading existing processed dataset...\")\n",
    "            self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        else:\n",
    "            logging.info(\"Processing dataset from scratch...\")\n",
    "            self.process()\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [CONFIG['processed_file_name']]\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"\n",
    "        Convert each row in self.df into a PyG 'Data' object.\n",
    "        We'll replicate your original feature-building steps\n",
    "        (node features, edges, etc.) and then add (nA, nB).\n",
    "        \"\"\"\n",
    "        data_list = []\n",
    "\n",
    "        for idx, row in self.df.iterrows():\n",
    "            # Example: Nx, Ny=2\n",
    "            Nx = row['Nx']\n",
    "            Ny = 2\n",
    "            N = Nx * Ny\n",
    "\n",
    "            # -------------------------------------------------------------\n",
    "            # 1) Build Node Features\n",
    "            #    This code replicates your original logic that turned\n",
    "            #    top_50_indices, top_50_probabilities into node features,\n",
    "            #    along with position-based attributes, etc.\n",
    "            # -------------------------------------------------------------\n",
    "            x_spacing = row['x_spacing']\n",
    "            y_spacing = row['y_spacing']\n",
    "\n",
    "            # Create atomic positions\n",
    "            positions = np.array([\n",
    "                (col * x_spacing, row_idx * y_spacing)\n",
    "                for row_idx in range(Nx) for col in range(Ny)\n",
    "            ], dtype=np.float32)\n",
    "\n",
    "            positions_t = torch.tensor(positions, dtype=torch.float)\n",
    "\n",
    "            # We replicate your approach of building node features:\n",
    "            #   - normalized positions\n",
    "            #   - Rydberg probabilities\n",
    "            #   - local densities\n",
    "            #   - boundary distances\n",
    "            #   - angles\n",
    "            #   - etc.\n",
    "            # For brevity, we do a shorter version, but in your real code,\n",
    "            # you'd fully replicate all original features.\n",
    "\n",
    "            # Some placeholders for demonstration:\n",
    "            pos_min = positions_t.min(dim=0).values\n",
    "            pos_max = positions_t.max(dim=0).values\n",
    "            normalized_positions = (positions_t - pos_min) / (pos_max - pos_min + 1e-8)\n",
    "\n",
    "            # Rebuild Rydberg excitations from top_50_indices, top_50_probabilities:\n",
    "            top_indices = row['Top_50_Indices']\n",
    "            top_probs = row['Top_50_Probabilities']\n",
    "            p_rydberg = torch.zeros(N, dtype=torch.float)\n",
    "            for state, prob in zip(top_indices, top_probs):\n",
    "                state = int(state)\n",
    "                for i_site in range(N):\n",
    "                    if (state & (1 << i_site)) != 0:\n",
    "                        p_rydberg[i_site] += prob\n",
    "            p_rydberg = p_rydberg.unsqueeze(1)  # shape [N,1]\n",
    "\n",
    "            # Subsystem mask from row['Subsystem_Mask']\n",
    "            subsystem_mask_str = row['Subsystem_Mask']\n",
    "            mask_tensor = torch.tensor([int(bit) for bit in subsystem_mask_str],\n",
    "                                       dtype=torch.float).unsqueeze(1)  # [N,1]\n",
    "\n",
    "            # Example local density or boundary distances, angles, etc.\n",
    "            # We'll do placeholders for illustration:\n",
    "            boundary_dist = torch.rand(N, 1)\n",
    "            angles = torch.rand(N, 1)\n",
    "            local_interact = torch.rand(N, 1)\n",
    "            config_entropy = torch.rand(N, 1)  # Or replicate your logic\n",
    "\n",
    "            # Combine node features\n",
    "            node_features = torch.cat([\n",
    "                normalized_positions,   # 2\n",
    "                p_rydberg,             # 1\n",
    "                mask_tensor,           # 1\n",
    "                boundary_dist,         # 1\n",
    "                angles,                # 1\n",
    "                local_interact,        # 1\n",
    "                config_entropy,        # 1\n",
    "            ], dim=1)  # total = 2+1+1+1+1+1+1=8 features (example; your real code may differ)\n",
    "\n",
    "            # -------------------------------------------------------------\n",
    "            # 2) Build Edges\n",
    "            #    You do neighbor computations or cutoffs. We'll do a minimal placeholder\n",
    "            # -------------------------------------------------------------\n",
    "            distance_threshold = CONFIG['distance_threshold']\n",
    "            nbrs = NearestNeighbors(radius=distance_threshold, algorithm='ball_tree').fit(positions)\n",
    "            indices = nbrs.radius_neighbors(positions, return_distance=False)\n",
    "\n",
    "            edges = []\n",
    "            for i_node in range(N):\n",
    "                for j_node in indices[i_node]:\n",
    "                    if i_node < j_node:\n",
    "                        edges.append((i_node, j_node))\n",
    "            if len(edges) == 0:\n",
    "                edge_index = torch.empty((2,0), dtype=torch.long)\n",
    "                edge_attr = torch.empty((0,4), dtype=torch.float)\n",
    "            else:\n",
    "                edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "                # Example edge features\n",
    "                E = edge_index.size(1)\n",
    "                edge_attr_list = []\n",
    "                pos_i = positions_t[edge_index[0]]\n",
    "                pos_j = positions_t[edge_index[1]]\n",
    "                vec_ij = pos_j - pos_i\n",
    "                dist_ij = torch.norm(vec_ij, dim=1, keepdim=True)\n",
    "                inv_r6 = 1.0 / (dist_ij**6 + 1e-8)\n",
    "                angle_ij = torch.atan2(vec_ij[:,1], vec_ij[:,0]).unsqueeze(1)\n",
    "\n",
    "                # Some quantum correlation placeholder\n",
    "                correlation = torch.rand(E, 1)\n",
    "\n",
    "                edge_attr = torch.cat([\n",
    "                    inv_r6,        # [E,1]\n",
    "                    angle_ij,      # [E,1]\n",
    "                    correlation,   # [E,1]\n",
    "                    dist_ij        # [E,1] or any other feature\n",
    "                ], dim=1)\n",
    "\n",
    "            # -------------------------------------------------------------\n",
    "            # 3) Create Data object with target = Von Neumann Entropy\n",
    "            # -------------------------------------------------------------\n",
    "            target_vne = torch.tensor([row['Von_Neumann_Entropy']], dtype=torch.float)\n",
    "            data = Data(\n",
    "                x=node_features,\n",
    "                edge_index=edge_index,\n",
    "                edge_attr=edge_attr,\n",
    "                y=target_vne\n",
    "            )\n",
    "\n",
    "            # Extra fields\n",
    "            data.Omega = torch.tensor([[row['Omega']]], dtype=torch.float)\n",
    "            data.Delta = torch.tensor([[row['Delta']]], dtype=torch.float)\n",
    "            data.Energy = torch.tensor([[row['Energy']]], dtype=torch.float)\n",
    "            data.system_size = torch.tensor([[N]], dtype=torch.float)\n",
    "\n",
    "            # total rydberg (sum of p_rydberg)\n",
    "            total_ryd = p_rydberg.sum()\n",
    "            data.total_rydberg = total_ryd\n",
    "            data.rydberg_density = total_ryd / N\n",
    "\n",
    "            # This might be your global config entropy or from row if you store it\n",
    "            data.config_entropy = torch.tensor([[row['Von_Neumann_Entropy']]], dtype=torch.float)\n",
    "            # or from some other column if relevant\n",
    "\n",
    "            # -------------------------------------------------------------\n",
    "            # 4) Add nA, nB\n",
    "            #    We have row['N_A'] or we can get from Subsystem_Mask\n",
    "            # -------------------------------------------------------------\n",
    "            nA_val = float(mask_tensor.sum().item())  # sum of 1 bits\n",
    "            nB_val = N - nA_val\n",
    "            data.nA = torch.tensor([[nA_val]], dtype=torch.float)\n",
    "            data.nB = torch.tensor([[nB_val]], dtype=torch.float)\n",
    "\n",
    "            data_list.append(data)\n",
    "\n",
    "            if (idx+1) % 20000 == 0:\n",
    "                logging.info(f\"Processed {idx+1} rows so far...\")\n",
    "\n",
    "        # Collate into big InMemoryDataset\n",
    "        data_obj, slices = self.collate(data_list)\n",
    "        torch.save((data_obj, slices), self.processed_paths[0])\n",
    "        self.data, self.slices = data_obj, slices\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Reads the parquet, shuffles, builds SpinSystemDataset, saves to disk.\"\"\"\n",
    "    if not os.path.exists(CONFIG['data_path']):\n",
    "        raise FileNotFoundError(f\"Data file not found at {CONFIG['data_path']}\")\n",
    "\n",
    "    df = pq.read_table(CONFIG['data_path']).to_pandas()\n",
    "\n",
    "    # Shuffle\n",
    "    df_shuffled = df.sample(frac=1, random_state=CONFIG['random_seed']).reset_index(drop=True)\n",
    "\n",
    "    # Build dataset\n",
    "    ds = SpinSystemDataset(dataframe=df_shuffled, root=CONFIG['processed_dir'])\n",
    "    return ds\n",
    "\n",
    "def main():\n",
    "    setup_logging()\n",
    "    set_seed(CONFIG['random_seed'])\n",
    "\n",
    "    dataset = load_data()\n",
    "    logging.info(f\"Finished processing. Dataset length: {len(dataset)}\")\n",
    "    logging.info(f\"Sample data object: {dataset[0]}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9048fb5-a7ec-4204-a3e0-189ca1646583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
