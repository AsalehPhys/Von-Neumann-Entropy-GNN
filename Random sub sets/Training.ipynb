{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc52b2-11c2-4438-88ed-8da4cbc327a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amssa\\AppData\\Local\\Temp\\ipykernel_3312\\2836796082.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, self.slices = torch.load(self.processed_paths[0])\n",
      "C:\\Users\\amssa\\anaconda3\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n",
      "  [Batch 000] loss=3.366929, time=7.108s\n",
      "  [Batch 001] loss=1.187062, time=0.483s\n",
      "  [Batch 002] loss=1.148598, time=0.459s\n",
      "  [Batch 003] loss=0.986141, time=0.442s\n",
      "  [Batch 004] loss=0.870196, time=0.335s\n",
      "  [Batch 005] loss=0.977821, time=0.415s\n",
      "  [Batch 006] loss=0.890420, time=0.465s\n",
      "  [Batch 007] loss=1.017218, time=0.433s\n",
      "  [Batch 008] loss=0.839870, time=0.431s\n",
      "  [Batch 009] loss=0.861150, time=0.431s\n",
      "  [Batch 010] loss=0.771906, time=0.303s\n",
      "  [Batch 011] loss=0.908237, time=0.376s\n",
      "  [Batch 012] loss=0.773724, time=0.427s\n",
      "  [Batch 013] loss=0.765590, time=0.473s\n",
      "  [Batch 014] loss=0.597785, time=0.478s\n",
      "  [Batch 015] loss=0.772159, time=0.358s\n",
      "  [Batch 016] loss=0.671009, time=0.262s\n",
      "  [Batch 017] loss=0.772884, time=0.337s\n",
      "  [Batch 018] loss=0.654636, time=0.538s\n",
      "  [Batch 019] loss=0.814314, time=0.392s\n",
      "  [Batch 020] loss=0.571000, time=0.449s\n",
      "  [Batch 021] loss=0.543551, time=0.317s\n",
      "  [Batch 022] loss=0.848314, time=0.293s\n",
      "  [Batch 023] loss=0.838554, time=0.492s\n",
      "  [Batch 024] loss=0.558337, time=0.462s\n",
      "  [Batch 025] loss=0.693709, time=0.326s\n",
      "  [Batch 026] loss=0.785631, time=0.350s\n",
      "  [Batch 027] loss=0.534425, time=0.454s\n",
      "  [Batch 028] loss=0.638865, time=0.595s\n",
      "  [Batch 029] loss=0.742074, time=0.371s\n",
      "  [Batch 030] loss=0.636710, time=0.533s\n",
      "  [Batch 031] loss=0.906146, time=0.280s\n",
      "  [Batch 032] loss=0.558955, time=0.270s\n",
      "  [Batch 033] loss=0.551800, time=0.308s\n",
      "  [Batch 034] loss=0.545760, time=0.310s\n",
      "  [Batch 035] loss=0.711744, time=0.236s\n",
      "  [Batch 036] loss=0.665838, time=0.268s\n",
      "  [Batch 037] loss=0.547372, time=0.228s\n",
      "  [Batch 038] loss=0.530266, time=0.271s\n",
      "  [Batch 039] loss=0.562571, time=0.252s\n",
      "  [Batch 040] loss=0.510381, time=0.287s\n",
      "  [Batch 041] loss=0.672714, time=0.236s\n",
      "  [Batch 042] loss=0.460277, time=0.279s\n",
      "  [Batch 043] loss=0.556410, time=0.315s\n",
      "  [Batch 044] loss=0.673413, time=0.267s\n",
      "  [Batch 045] loss=0.496985, time=0.295s\n",
      "  [Batch 046] loss=0.477288, time=0.294s\n",
      "  [Batch 047] loss=0.654820, time=0.272s\n",
      "  [Batch 048] loss=0.693106, time=0.283s\n",
      "  [Batch 049] loss=0.529504, time=0.257s\n",
      "  [Batch 050] loss=0.817727, time=0.251s\n",
      "  [Batch 051] loss=0.566065, time=0.267s\n",
      "  [Batch 052] loss=0.670567, time=0.272s\n",
      "  [Batch 053] loss=0.660887, time=0.258s\n",
      "  [Batch 054] loss=0.558103, time=0.274s\n",
      "  [Batch 055] loss=0.516868, time=0.253s\n",
      "  [Batch 056] loss=0.592790, time=0.263s\n",
      "  [Batch 057] loss=0.665221, time=0.231s\n",
      "  [Batch 058] loss=0.612451, time=0.252s\n",
      "  [Batch 059] loss=0.629221, time=0.293s\n",
      "  [Batch 060] loss=0.635597, time=0.267s\n",
      "  [Batch 061] loss=0.485756, time=0.221s\n",
      "  [Batch 062] loss=0.578150, time=0.246s\n",
      "  [Batch 063] loss=0.603658, time=0.273s\n",
      "  [Batch 064] loss=0.473428, time=0.262s\n",
      "  [Batch 065] loss=0.469459, time=0.235s\n",
      "  [Batch 066] loss=0.555929, time=0.242s\n",
      "  [Batch 067] loss=0.665305, time=0.232s\n",
      "  [Batch 068] loss=0.486733, time=0.234s\n",
      "  [Batch 069] loss=0.430446, time=0.245s\n",
      "  [Batch 070] loss=0.602406, time=0.243s\n",
      "  [Batch 071] loss=0.488142, time=0.282s\n",
      "  [Batch 072] loss=0.536687, time=0.239s\n",
      "  [Batch 073] loss=0.513574, time=0.277s\n",
      "  [Batch 074] loss=0.652867, time=0.270s\n",
      "  [Batch 075] loss=0.527404, time=0.242s\n",
      "  [Batch 076] loss=0.585813, time=0.230s\n",
      "  [Batch 077] loss=0.604663, time=0.238s\n",
      "  [Batch 078] loss=0.486279, time=0.239s\n",
      "  [Batch 079] loss=0.447831, time=0.266s\n",
      "  [Batch 080] loss=0.496847, time=0.214s\n",
      "  [Batch 081] loss=0.528900, time=0.211s\n",
      "  [Batch 082] loss=0.594734, time=0.236s\n",
      "  [Batch 083] loss=0.636878, time=0.233s\n",
      "  [Batch 084] loss=0.641873, time=0.242s\n",
      "  [Batch 085] loss=0.677651, time=0.226s\n",
      "  [Batch 086] loss=0.531578, time=0.207s\n",
      "  [Batch 087] loss=0.358501, time=0.248s\n",
      "  [Batch 088] loss=0.651792, time=0.215s\n",
      "  [Batch 089] loss=0.485432, time=0.235s\n",
      "  [Batch 090] loss=0.629341, time=0.215s\n",
      "  [Batch 091] loss=0.589544, time=0.236s\n",
      "  [Batch 092] loss=0.443720, time=0.226s\n",
      "  [Batch 093] loss=0.687069, time=0.216s\n",
      "  [Batch 094] loss=0.609256, time=0.216s\n",
      "  [Batch 095] loss=0.622556, time=0.213s\n",
      "  [Batch 096] loss=0.583648, time=0.211s\n",
      "  [Batch 097] loss=0.625640, time=0.251s\n",
      "  [Batch 098] loss=0.725817, time=0.203s\n",
      "  [Batch 099] loss=0.622585, time=0.260s\n",
      "  [Batch 100] loss=0.556170, time=0.221s\n",
      "  [Batch 101] loss=0.687198, time=0.204s\n",
      "  [Batch 102] loss=0.531548, time=0.232s\n",
      "  [Batch 103] loss=0.459855, time=0.199s\n",
      "  [Batch 104] loss=0.499973, time=0.262s\n",
      "  [Batch 105] loss=0.585019, time=0.218s\n",
      "  [Batch 106] loss=0.403572, time=0.203s\n",
      "  [Batch 107] loss=0.518587, time=0.222s\n",
      "  [Batch 108] loss=0.555641, time=0.228s\n",
      "  [Batch 109] loss=0.480360, time=0.216s\n",
      "  [Batch 110] loss=0.714566, time=0.234s\n",
      "  [Batch 111] loss=0.566551, time=0.243s\n",
      "  [Batch 112] loss=0.725792, time=0.202s\n",
      "  [Batch 113] loss=0.651179, time=0.226s\n",
      "  [Batch 114] loss=0.646058, time=0.216s\n",
      "  [Batch 115] loss=0.430725, time=0.240s\n",
      "  [Batch 116] loss=0.563254, time=0.211s\n",
      "  [Batch 117] loss=0.518257, time=0.226s\n",
      "  [Batch 118] loss=0.549227, time=0.222s\n",
      "  [Batch 119] loss=0.732461, time=0.213s\n",
      "  [Batch 120] loss=0.657516, time=0.236s\n",
      "  [Batch 121] loss=0.513722, time=0.220s\n",
      "  [Batch 122] loss=0.687058, time=0.269s\n",
      "  [Batch 123] loss=0.608918, time=0.219s\n",
      "  [Batch 124] loss=0.538946, time=0.232s\n",
      "  [Batch 125] loss=0.483533, time=0.200s\n",
      "  [Batch 126] loss=0.760245, time=0.184s\n",
      "  [Batch 127] loss=0.517110, time=0.221s\n",
      "  [Batch 128] loss=0.624364, time=0.211s\n",
      "  [Batch 129] loss=0.544209, time=0.213s\n",
      "  [Batch 130] loss=0.458288, time=0.214s\n",
      "  [Batch 131] loss=0.468759, time=0.201s\n",
      "  [Batch 132] loss=0.621773, time=0.189s\n",
      "  [Batch 133] loss=0.607777, time=0.221s\n",
      "  [Batch 134] loss=0.596222, time=0.210s\n",
      "  [Batch 135] loss=0.563975, time=0.219s\n",
      "  [Batch 136] loss=0.624641, time=0.196s\n",
      "  [Batch 137] loss=0.476107, time=0.232s\n",
      "  [Batch 138] loss=0.507347, time=0.204s\n",
      "  [Batch 139] loss=0.545596, time=0.247s\n",
      "  [Batch 140] loss=0.535538, time=0.214s\n",
      "  [Batch 141] loss=0.658957, time=0.211s\n",
      "  [Batch 142] loss=0.423349, time=0.208s\n",
      "  [Batch 143] loss=0.513064, time=0.218s\n",
      "  [Batch 144] loss=0.560800, time=0.217s\n",
      "  [Batch 145] loss=0.477422, time=0.219s\n",
      "  [Batch 146] loss=0.604932, time=0.193s\n",
      "  [Batch 147] loss=0.430684, time=0.218s\n",
      "  [Batch 148] loss=0.517576, time=0.192s\n",
      "  [Batch 149] loss=0.487048, time=0.208s\n",
      "  [Batch 150] loss=0.553843, time=0.257s\n",
      "  [Batch 151] loss=0.496244, time=0.199s\n",
      "  [Batch 152] loss=0.474802, time=0.225s\n",
      "  [Batch 153] loss=0.433434, time=0.214s\n",
      "  [Batch 154] loss=0.634886, time=0.192s\n",
      "  [Batch 155] loss=0.574562, time=0.198s\n",
      "  [Batch 156] loss=0.408079, time=0.199s\n",
      "  [Batch 157] loss=0.492447, time=0.215s\n",
      "  [Batch 158] loss=0.520752, time=0.238s\n",
      "  [Batch 159] loss=0.464760, time=0.187s\n",
      "  [Batch 160] loss=0.437399, time=0.242s\n",
      "  [Batch 161] loss=0.601820, time=0.198s\n",
      "  [Batch 162] loss=0.380588, time=0.215s\n",
      "  [Batch 163] loss=0.465863, time=0.206s\n",
      "  [Batch 164] loss=0.508087, time=0.222s\n",
      "  [Batch 165] loss=0.452933, time=0.251s\n",
      "  [Batch 166] loss=0.602935, time=0.196s\n",
      "  [Batch 167] loss=0.380322, time=0.224s\n",
      "  [Batch 168] loss=0.591962, time=0.239s\n",
      "  [Batch 169] loss=0.654518, time=0.223s\n",
      "  [Batch 170] loss=0.448787, time=0.191s\n",
      "  [Batch 171] loss=0.483024, time=0.185s\n",
      "  [Batch 172] loss=0.377842, time=0.202s\n",
      "  [Batch 173] loss=0.397117, time=0.215s\n",
      "  [Batch 174] loss=0.536511, time=0.213s\n",
      "  [Batch 175] loss=0.600521, time=0.196s\n",
      "  [Batch 176] loss=0.472665, time=0.183s\n",
      "  [Batch 177] loss=0.433082, time=0.210s\n",
      "  [Batch 178] loss=0.605817, time=0.209s\n",
      "  [Batch 179] loss=0.567909, time=0.215s\n",
      "  [Batch 180] loss=0.483612, time=0.207s\n",
      "  [Batch 181] loss=0.411589, time=0.195s\n",
      "  [Batch 182] loss=0.514577, time=0.191s\n",
      "  [Batch 183] loss=0.582126, time=0.245s\n",
      "  [Batch 184] loss=0.612383, time=0.202s\n",
      "  [Batch 185] loss=0.562859, time=0.194s\n",
      "  [Batch 186] loss=0.522297, time=0.215s\n",
      "  [Batch 187] loss=0.536356, time=0.200s\n",
      "  [Batch 188] loss=0.482689, time=0.202s\n",
      "  [Batch 189] loss=0.545144, time=0.206s\n",
      "  [Batch 190] loss=0.507821, time=0.208s\n",
      "  [Batch 191] loss=0.455926, time=0.241s\n",
      "  [Batch 192] loss=0.480273, time=0.202s\n",
      "  [Batch 193] loss=0.548466, time=0.226s\n",
      "  [Batch 194] loss=0.604527, time=0.211s\n",
      "  [Batch 195] loss=0.454269, time=0.200s\n",
      "  [Batch 196] loss=0.393807, time=0.193s\n",
      "  [Batch 197] loss=0.443085, time=0.227s\n",
      "  [Batch 198] loss=0.708127, time=0.207s\n",
      "  [Batch 199] loss=0.425771, time=0.209s\n",
      "  [Batch 200] loss=0.627926, time=0.211s\n",
      "  [Batch 201] loss=0.570652, time=0.224s\n",
      "  [Batch 202] loss=0.449143, time=0.210s\n",
      "  [Batch 203] loss=0.448161, time=0.199s\n",
      "  [Batch 204] loss=0.515629, time=0.214s\n",
      "  [Batch 205] loss=0.480065, time=0.183s\n",
      "  [Batch 206] loss=0.477483, time=0.219s\n",
      "  [Batch 207] loss=0.431648, time=0.190s\n",
      "  [Batch 208] loss=0.434610, time=0.205s\n",
      "  [Batch 209] loss=0.437340, time=0.270s\n",
      "  [Batch 210] loss=0.388989, time=0.231s\n",
      "  [Batch 211] loss=0.410266, time=0.210s\n",
      "  [Batch 212] loss=0.376441, time=0.204s\n",
      "  [Batch 213] loss=0.435671, time=0.223s\n",
      "  [Batch 214] loss=0.416978, time=0.225s\n",
      "  [Batch 215] loss=0.618279, time=0.227s\n",
      "  [Batch 216] loss=0.564900, time=0.220s\n",
      "  [Batch 217] loss=0.461041, time=0.220s\n",
      "  [Batch 218] loss=0.631790, time=0.226s\n",
      "  [Batch 219] loss=0.383655, time=0.219s\n",
      "  [Batch 220] loss=0.383805, time=0.202s\n",
      "  [Batch 221] loss=0.478633, time=0.205s\n",
      "  [Batch 222] loss=0.476571, time=0.220s\n",
      "  [Batch 223] loss=0.402470, time=0.197s\n",
      "  [Batch 224] loss=0.471295, time=0.211s\n",
      "  [Batch 225] loss=0.522044, time=0.185s\n",
      "  [Batch 226] loss=0.501521, time=0.242s\n",
      "  [Batch 227] loss=0.374768, time=0.197s\n",
      "  [Batch 228] loss=0.599893, time=0.219s\n",
      "  [Batch 229] loss=0.496293, time=0.208s\n",
      "  [Batch 230] loss=0.526339, time=0.228s\n",
      "  [Batch 231] loss=0.476489, time=0.209s\n",
      "  [Batch 232] loss=0.430043, time=0.206s\n",
      "  [Batch 233] loss=0.419114, time=0.209s\n",
      "  [Batch 234] loss=0.488076, time=0.218s\n",
      "  [Batch 235] loss=0.392746, time=0.199s\n",
      "  [Batch 236] loss=0.408918, time=0.230s\n",
      "  [Batch 237] loss=0.469422, time=0.230s\n",
      "  [Batch 238] loss=0.435247, time=0.203s\n",
      "  [Batch 239] loss=0.456761, time=0.221s\n",
      "  [Batch 240] loss=0.511896, time=0.191s\n",
      "  [Batch 241] loss=0.455152, time=0.233s\n",
      "  [Batch 242] loss=0.525286, time=0.203s\n",
      "  [Batch 243] loss=0.502813, time=0.210s\n",
      "  [Batch 244] loss=0.404262, time=0.215s\n",
      "  [Batch 245] loss=0.551019, time=0.204s\n",
      "  [Batch 246] loss=0.490815, time=0.212s\n",
      "  [Batch 247] loss=0.361905, time=0.217s\n",
      "  [Batch 248] loss=0.601012, time=0.223s\n",
      "  [Batch 249] loss=0.681271, time=0.231s\n",
      "  [Batch 250] loss=0.664447, time=0.195s\n",
      "  [Batch 251] loss=0.100480, time=0.128s\n",
      "\n",
      "[Validation] Loss: 1.033475\n",
      "  Size= 2 : MSE=0.002140 MAE=0.037697 MAPE=125.66%\n",
      "  Size= 4 : MSE=0.021138 MAE=0.108476 MAPE=27.64%\n",
      "  Size= 6 : MSE=0.059936 MAE=0.175865 MAPE=1963.89%\n",
      "  Size= 8 : MSE=0.076423 MAE=0.206170 MAPE=45.29%\n",
      "  Size=10 : MSE=0.079893 MAE=0.211409 MAPE=1562.87%\n",
      "  Size=12 : MSE=0.108724 MAE=0.244961 MAPE=54.87%\n",
      "[Info] New best model saved (val_loss=1.033475)\n",
      "\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amssa\\anaconda3\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Batch 000] loss=0.609838, time=0.663s\n",
      "  [Batch 001] loss=0.647852, time=0.519s\n",
      "  [Batch 002] loss=0.543496, time=0.468s\n",
      "  [Batch 003] loss=0.388132, time=0.480s\n",
      "  [Batch 004] loss=0.558100, time=0.485s\n",
      "  [Batch 005] loss=0.380284, time=0.279s\n",
      "  [Batch 006] loss=0.536658, time=0.326s\n",
      "  [Batch 007] loss=0.535147, time=0.454s\n",
      "  [Batch 008] loss=0.563443, time=0.429s\n",
      "  [Batch 009] loss=0.530141, time=0.297s\n",
      "  [Batch 010] loss=0.621670, time=0.355s\n",
      "  [Batch 011] loss=0.390298, time=0.369s\n",
      "  [Batch 012] loss=0.451335, time=0.280s\n",
      "  [Batch 013] loss=0.431002, time=0.242s\n",
      "  [Batch 014] loss=0.349761, time=0.277s\n",
      "  [Batch 015] loss=0.466944, time=0.359s\n",
      "  [Batch 016] loss=0.411623, time=0.309s\n",
      "  [Batch 017] loss=0.415435, time=0.286s\n",
      "  [Batch 018] loss=0.525186, time=0.361s\n",
      "  [Batch 019] loss=0.395995, time=0.480s\n",
      "  [Batch 020] loss=0.438428, time=0.387s\n",
      "  [Batch 021] loss=0.624413, time=0.260s\n",
      "  [Batch 022] loss=0.486005, time=0.509s\n",
      "  [Batch 023] loss=0.614464, time=0.391s\n",
      "  [Batch 024] loss=0.373281, time=0.425s\n",
      "  [Batch 025] loss=0.473748, time=0.394s\n",
      "  [Batch 026] loss=0.559810, time=0.384s\n",
      "  [Batch 027] loss=0.446558, time=0.261s\n",
      "  [Batch 028] loss=0.380481, time=0.466s\n",
      "  [Batch 029] loss=0.438097, time=0.383s\n",
      "  [Batch 030] loss=0.524818, time=0.471s\n",
      "  [Batch 031] loss=0.408096, time=0.360s\n",
      "  [Batch 032] loss=0.532061, time=0.225s\n",
      "  [Batch 033] loss=0.349375, time=0.224s\n",
      "  [Batch 034] loss=0.574181, time=0.249s\n",
      "  [Batch 035] loss=0.490656, time=0.474s\n",
      "  [Batch 036] loss=0.454894, time=0.247s\n",
      "  [Batch 037] loss=0.437582, time=0.348s\n",
      "  [Batch 038] loss=0.402258, time=0.325s\n",
      "  [Batch 039] loss=0.414412, time=0.440s\n",
      "  [Batch 040] loss=0.432218, time=0.432s\n",
      "  [Batch 041] loss=0.441119, time=0.460s\n",
      "  [Batch 042] loss=0.399697, time=0.333s\n",
      "  [Batch 043] loss=0.354014, time=0.450s\n",
      "  [Batch 044] loss=0.444082, time=0.377s\n",
      "  [Batch 045] loss=0.582530, time=0.204s\n",
      "  [Batch 046] loss=0.458064, time=0.418s\n",
      "  [Batch 047] loss=0.395281, time=0.356s\n",
      "  [Batch 048] loss=0.558483, time=0.419s\n",
      "  [Batch 049] loss=0.400248, time=0.457s\n",
      "  [Batch 050] loss=0.427995, time=0.301s\n",
      "  [Batch 051] loss=0.410508, time=0.472s\n",
      "  [Batch 052] loss=0.421807, time=0.452s\n",
      "  [Batch 053] loss=0.450477, time=0.469s\n",
      "  [Batch 054] loss=0.383613, time=0.340s\n",
      "  [Batch 055] loss=0.417666, time=0.403s\n",
      "  [Batch 056] loss=0.419846, time=0.311s\n",
      "  [Batch 057] loss=0.461640, time=0.182s\n",
      "  [Batch 058] loss=0.358753, time=0.387s\n",
      "  [Batch 059] loss=0.486498, time=0.204s\n",
      "  [Batch 060] loss=0.388178, time=0.278s\n",
      "  [Batch 061] loss=0.485474, time=0.402s\n",
      "  [Batch 062] loss=0.415686, time=0.422s\n",
      "  [Batch 063] loss=0.591763, time=0.224s\n",
      "  [Batch 064] loss=0.476505, time=0.259s\n",
      "  [Batch 065] loss=0.360382, time=0.442s\n",
      "  [Batch 066] loss=0.367229, time=0.212s\n",
      "  [Batch 067] loss=0.518803, time=0.376s\n",
      "  [Batch 068] loss=0.399382, time=0.236s\n",
      "  [Batch 069] loss=0.329989, time=0.355s\n",
      "  [Batch 070] loss=0.459243, time=0.454s\n",
      "  [Batch 071] loss=0.508932, time=0.266s\n",
      "  [Batch 072] loss=0.473043, time=0.371s\n",
      "  [Batch 073] loss=0.491278, time=0.394s\n",
      "  [Batch 074] loss=0.364416, time=0.457s\n",
      "  [Batch 075] loss=0.339185, time=0.437s\n",
      "  [Batch 076] loss=0.316936, time=0.471s\n",
      "  [Batch 077] loss=0.676934, time=0.256s\n",
      "  [Batch 078] loss=0.425469, time=0.407s\n",
      "  [Batch 079] loss=0.380612, time=0.340s\n",
      "  [Batch 080] loss=0.404473, time=0.214s\n",
      "  [Batch 081] loss=0.381036, time=0.327s\n",
      "  [Batch 082] loss=0.603102, time=0.436s\n",
      "  [Batch 083] loss=0.334969, time=0.203s\n",
      "  [Batch 084] loss=0.463010, time=0.317s\n",
      "  [Batch 085] loss=0.387451, time=0.373s\n",
      "  [Batch 086] loss=0.529080, time=0.370s\n",
      "  [Batch 087] loss=0.395302, time=0.426s\n",
      "  [Batch 088] loss=0.435987, time=0.422s\n",
      "  [Batch 089] loss=0.298936, time=0.385s\n",
      "  [Batch 090] loss=0.473411, time=0.379s\n",
      "  [Batch 091] loss=0.526200, time=0.367s\n",
      "  [Batch 092] loss=0.449466, time=0.275s\n",
      "  [Batch 093] loss=0.394290, time=0.338s\n",
      "  [Batch 094] loss=0.521563, time=0.395s\n",
      "  [Batch 095] loss=0.551735, time=0.279s\n",
      "  [Batch 096] loss=0.569337, time=0.395s\n",
      "  [Batch 097] loss=0.373754, time=0.378s\n",
      "  [Batch 098] loss=0.416188, time=0.228s\n",
      "  [Batch 099] loss=0.410485, time=0.440s\n",
      "  [Batch 100] loss=0.459156, time=0.275s\n",
      "  [Batch 101] loss=0.383955, time=0.210s\n",
      "  [Batch 102] loss=0.306364, time=0.279s\n",
      "  [Batch 103] loss=0.484085, time=0.248s\n",
      "  [Batch 104] loss=0.405321, time=0.216s\n",
      "  [Batch 105] loss=0.433455, time=0.217s\n",
      "  [Batch 106] loss=0.494861, time=0.273s\n",
      "  [Batch 107] loss=0.376721, time=0.292s\n",
      "  [Batch 108] loss=0.415753, time=0.301s\n",
      "  [Batch 109] loss=0.334934, time=0.444s\n",
      "  [Batch 110] loss=0.403472, time=0.424s\n",
      "  [Batch 111] loss=0.495657, time=0.333s\n",
      "  [Batch 112] loss=0.362926, time=0.264s\n",
      "  [Batch 113] loss=0.413866, time=0.458s\n",
      "  [Batch 114] loss=0.374902, time=0.399s\n",
      "  [Batch 115] loss=0.396731, time=0.315s\n",
      "  [Batch 116] loss=0.517718, time=0.439s\n",
      "  [Batch 117] loss=0.450897, time=0.314s\n",
      "  [Batch 118] loss=0.366991, time=0.205s\n",
      "  [Batch 119] loss=0.546621, time=0.216s\n",
      "  [Batch 120] loss=0.337432, time=0.273s\n",
      "  [Batch 121] loss=0.385265, time=0.223s\n",
      "  [Batch 122] loss=0.460766, time=0.304s\n",
      "  [Batch 123] loss=0.310190, time=0.274s\n",
      "  [Batch 124] loss=0.410941, time=0.247s\n",
      "  [Batch 125] loss=0.557525, time=0.326s\n",
      "  [Batch 126] loss=0.318678, time=0.275s\n",
      "  [Batch 127] loss=0.388914, time=0.435s\n",
      "  [Batch 128] loss=0.326749, time=0.211s\n",
      "  [Batch 129] loss=0.409040, time=0.458s\n",
      "  [Batch 130] loss=0.380687, time=0.327s\n",
      "  [Batch 131] loss=0.372536, time=0.370s\n",
      "  [Batch 132] loss=0.379471, time=0.426s\n",
      "  [Batch 133] loss=0.407374, time=0.359s\n",
      "  [Batch 134] loss=0.368749, time=0.447s\n",
      "  [Batch 135] loss=0.415958, time=0.302s\n",
      "  [Batch 136] loss=0.387955, time=0.421s\n",
      "  [Batch 137] loss=0.342886, time=0.444s\n",
      "  [Batch 138] loss=0.333213, time=0.441s\n",
      "  [Batch 139] loss=0.502426, time=0.211s\n",
      "  [Batch 140] loss=0.469545, time=0.327s\n",
      "  [Batch 141] loss=0.443098, time=0.288s\n",
      "  [Batch 142] loss=0.445529, time=0.301s\n",
      "  [Batch 143] loss=0.469988, time=0.213s\n",
      "  [Batch 144] loss=0.397645, time=0.271s\n",
      "  [Batch 145] loss=0.629743, time=0.300s\n",
      "  [Batch 146] loss=0.400893, time=0.360s\n",
      "  [Batch 147] loss=0.480027, time=0.372s\n",
      "  [Batch 148] loss=0.505704, time=0.294s\n",
      "  [Batch 149] loss=0.425538, time=0.276s\n",
      "  [Batch 150] loss=0.388456, time=0.254s\n",
      "  [Batch 151] loss=0.419098, time=0.215s\n",
      "  [Batch 152] loss=0.456209, time=0.263s\n",
      "  [Batch 153] loss=0.504970, time=0.451s\n",
      "  [Batch 154] loss=0.555558, time=0.332s\n",
      "  [Batch 155] loss=0.377675, time=0.416s\n",
      "  [Batch 156] loss=0.335826, time=0.340s\n",
      "  [Batch 157] loss=0.416119, time=0.414s\n",
      "  [Batch 158] loss=0.489769, time=0.448s\n",
      "  [Batch 159] loss=0.386113, time=0.200s\n",
      "  [Batch 160] loss=0.474345, time=0.372s\n",
      "  [Batch 161] loss=0.398680, time=0.394s\n",
      "  [Batch 162] loss=0.657369, time=0.279s\n",
      "  [Batch 163] loss=0.346638, time=0.364s\n",
      "  [Batch 164] loss=0.525887, time=0.428s\n",
      "  [Batch 165] loss=0.492314, time=0.411s\n",
      "  [Batch 166] loss=0.434135, time=0.237s\n",
      "  [Batch 167] loss=0.329959, time=0.326s\n",
      "  [Batch 168] loss=0.376941, time=0.253s\n",
      "  [Batch 169] loss=0.496315, time=0.299s\n",
      "  [Batch 170] loss=0.393247, time=0.450s\n",
      "  [Batch 171] loss=0.339269, time=0.432s\n",
      "  [Batch 172] loss=0.441076, time=0.321s\n",
      "  [Batch 173] loss=0.439965, time=0.279s\n",
      "  [Batch 174] loss=0.353479, time=0.315s\n",
      "  [Batch 175] loss=0.391647, time=0.423s\n",
      "  [Batch 176] loss=0.397056, time=0.437s\n",
      "  [Batch 177] loss=0.349619, time=0.317s\n",
      "  [Batch 178] loss=0.360288, time=0.223s\n",
      "  [Batch 179] loss=0.428614, time=0.455s\n",
      "  [Batch 180] loss=0.351212, time=0.379s\n",
      "  [Batch 181] loss=0.355682, time=0.318s\n",
      "  [Batch 182] loss=0.366600, time=0.465s\n",
      "  [Batch 183] loss=0.554691, time=0.444s\n",
      "  [Batch 184] loss=0.473622, time=0.228s\n",
      "  [Batch 185] loss=0.410159, time=0.469s\n",
      "  [Batch 186] loss=0.487740, time=0.327s\n",
      "  [Batch 187] loss=0.399419, time=0.470s\n",
      "  [Batch 188] loss=0.490357, time=0.412s\n",
      "  [Batch 189] loss=0.471807, time=0.359s\n",
      "  [Batch 190] loss=0.428085, time=0.285s\n",
      "  [Batch 191] loss=0.524479, time=0.396s\n",
      "  [Batch 192] loss=0.580940, time=0.299s\n",
      "  [Batch 193] loss=0.400710, time=0.376s\n",
      "  [Batch 194] loss=0.326570, time=0.200s\n",
      "  [Batch 195] loss=0.408135, time=0.382s\n",
      "  [Batch 196] loss=0.380531, time=0.418s\n",
      "  [Batch 197] loss=0.425846, time=0.240s\n",
      "  [Batch 198] loss=0.337148, time=0.374s\n",
      "  [Batch 199] loss=0.437323, time=0.337s\n",
      "  [Batch 200] loss=0.449542, time=0.219s\n",
      "  [Batch 201] loss=0.382362, time=0.306s\n",
      "  [Batch 202] loss=0.463675, time=0.333s\n",
      "  [Batch 203] loss=0.449630, time=0.214s\n",
      "  [Batch 204] loss=0.550582, time=0.261s\n",
      "  [Batch 205] loss=0.438772, time=0.290s\n",
      "  [Batch 206] loss=0.555710, time=0.340s\n",
      "  [Batch 207] loss=0.420369, time=0.321s\n",
      "  [Batch 208] loss=0.490266, time=0.381s\n",
      "  [Batch 209] loss=0.397288, time=0.276s\n",
      "  [Batch 210] loss=0.414714, time=0.355s\n",
      "  [Batch 211] loss=0.350115, time=0.318s\n",
      "  [Batch 212] loss=0.498223, time=0.396s\n",
      "  [Batch 213] loss=0.531414, time=0.228s\n",
      "  [Batch 214] loss=0.417905, time=0.388s\n",
      "  [Batch 215] loss=0.502965, time=0.231s\n",
      "  [Batch 216] loss=0.364367, time=0.433s\n",
      "  [Batch 217] loss=0.475045, time=0.302s\n",
      "  [Batch 218] loss=0.423745, time=0.444s\n",
      "  [Batch 219] loss=0.381668, time=0.428s\n",
      "  [Batch 220] loss=0.394708, time=0.276s\n",
      "  [Batch 221] loss=0.467795, time=0.257s\n",
      "  [Batch 222] loss=0.306278, time=0.192s\n",
      "  [Batch 223] loss=0.521712, time=0.337s\n",
      "  [Batch 224] loss=0.370616, time=0.321s\n",
      "  [Batch 225] loss=0.337042, time=0.221s\n",
      "  [Batch 226] loss=0.325112, time=0.448s\n",
      "  [Batch 227] loss=0.343801, time=0.370s\n",
      "  [Batch 228] loss=0.327849, time=0.231s\n",
      "  [Batch 229] loss=0.333621, time=0.435s\n",
      "  [Batch 230] loss=0.395131, time=0.376s\n",
      "  [Batch 231] loss=0.358290, time=0.305s\n",
      "  [Batch 232] loss=0.407280, time=0.443s\n",
      "  [Batch 233] loss=0.372386, time=0.278s\n",
      "  [Batch 234] loss=0.335465, time=0.399s\n",
      "  [Batch 235] loss=0.404067, time=0.317s\n",
      "  [Batch 236] loss=0.517527, time=0.202s\n",
      "  [Batch 237] loss=0.427442, time=0.240s\n",
      "  [Batch 238] loss=0.347087, time=0.306s\n",
      "  [Batch 239] loss=0.455668, time=0.378s\n",
      "  [Batch 240] loss=0.447570, time=0.268s\n",
      "  [Batch 241] loss=0.443217, time=0.387s\n",
      "  [Batch 242] loss=0.307989, time=0.243s\n",
      "  [Batch 243] loss=0.604390, time=0.406s\n",
      "  [Batch 244] loss=0.340127, time=0.235s\n",
      "  [Batch 245] loss=0.422644, time=0.264s\n",
      "  [Batch 246] loss=0.345353, time=0.314s\n",
      "  [Batch 247] loss=0.383499, time=0.413s\n",
      "  [Batch 248] loss=0.415658, time=0.344s\n",
      "  [Batch 249] loss=0.390907, time=0.275s\n",
      "  [Batch 250] loss=0.315111, time=0.370s\n",
      "  [Batch 251] loss=0.287157, time=0.072s\n",
      "\n",
      "[Validation] Loss: 0.656057\n",
      "  Size= 2 : MSE=0.000809 MAE=0.021889 MAPE=67.85%\n",
      "  Size= 4 : MSE=0.006330 MAE=0.051694 MAPE=17.08%\n",
      "  Size= 6 : MSE=0.023236 MAE=0.102599 MAPE=1770.18%\n",
      "  Size= 8 : MSE=0.026438 MAE=0.120415 MAPE=35.52%\n",
      "  Size=10 : MSE=0.036968 MAE=0.141406 MAPE=1681.90%\n",
      "  Size=12 : MSE=0.045078 MAE=0.156505 MAPE=42.97%\n",
      "[Info] New best model saved (val_loss=0.656057)\n",
      "\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amssa\\anaconda3\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Batch 000] loss=0.267145, time=0.677s\n",
      "  [Batch 001] loss=0.376926, time=0.388s\n",
      "  [Batch 002] loss=0.320740, time=0.372s\n",
      "  [Batch 003] loss=0.306412, time=0.310s\n",
      "  [Batch 004] loss=0.478899, time=0.276s\n",
      "  [Batch 005] loss=0.348388, time=0.398s\n",
      "  [Batch 006] loss=0.345529, time=0.320s\n",
      "  [Batch 007] loss=0.370452, time=0.476s\n",
      "  [Batch 008] loss=0.406927, time=0.392s\n",
      "  [Batch 009] loss=0.299567, time=0.267s\n",
      "  [Batch 010] loss=0.348388, time=0.289s\n",
      "  [Batch 011] loss=0.421946, time=0.301s\n",
      "  [Batch 012] loss=0.396721, time=0.426s\n",
      "  [Batch 013] loss=0.308888, time=0.252s\n",
      "  [Batch 014] loss=0.388705, time=0.381s\n",
      "  [Batch 015] loss=0.399593, time=0.330s\n",
      "  [Batch 016] loss=0.372262, time=0.368s\n",
      "  [Batch 017] loss=0.357016, time=0.385s\n",
      "  [Batch 018] loss=0.334178, time=0.388s\n",
      "  [Batch 019] loss=0.284477, time=0.328s\n",
      "  [Batch 020] loss=0.383374, time=0.358s\n",
      "  [Batch 021] loss=0.321554, time=0.461s\n",
      "  [Batch 022] loss=0.322876, time=0.434s\n",
      "  [Batch 023] loss=0.286461, time=0.445s\n",
      "  [Batch 024] loss=0.397196, time=0.369s\n",
      "  [Batch 025] loss=0.310825, time=0.378s\n",
      "  [Batch 026] loss=0.391644, time=0.417s\n",
      "  [Batch 027] loss=0.433664, time=0.253s\n",
      "  [Batch 028] loss=0.417931, time=0.321s\n",
      "  [Batch 029] loss=0.448837, time=0.314s\n",
      "  [Batch 030] loss=0.390082, time=0.302s\n",
      "  [Batch 031] loss=0.427501, time=0.459s\n",
      "  [Batch 032] loss=0.309545, time=0.417s\n",
      "  [Batch 033] loss=0.489156, time=0.259s\n",
      "  [Batch 034] loss=0.378633, time=0.235s\n",
      "  [Batch 035] loss=0.414456, time=0.236s\n",
      "  [Batch 036] loss=0.345097, time=0.421s\n",
      "  [Batch 037] loss=0.333771, time=0.298s\n",
      "  [Batch 038] loss=0.387014, time=0.200s\n",
      "  [Batch 039] loss=0.382277, time=0.449s\n",
      "  [Batch 040] loss=0.632751, time=0.380s\n",
      "  [Batch 041] loss=0.365884, time=0.305s\n",
      "  [Batch 042] loss=0.328941, time=0.222s\n",
      "  [Batch 043] loss=0.370985, time=0.304s\n",
      "  [Batch 044] loss=0.331235, time=0.460s\n",
      "  [Batch 045] loss=0.416599, time=0.414s\n",
      "  [Batch 046] loss=0.346698, time=0.265s\n",
      "  [Batch 047] loss=0.478448, time=0.236s\n",
      "  [Batch 048] loss=0.361381, time=0.546s\n",
      "  [Batch 049] loss=0.378884, time=0.323s\n",
      "  [Batch 050] loss=0.372900, time=0.443s\n",
      "  [Batch 051] loss=0.443524, time=0.393s\n",
      "  [Batch 052] loss=0.335461, time=0.436s\n",
      "  [Batch 053] loss=0.360265, time=0.352s\n",
      "  [Batch 054] loss=0.374594, time=0.270s\n",
      "  [Batch 055] loss=0.297753, time=0.455s\n",
      "  [Batch 056] loss=0.332851, time=0.276s\n",
      "  [Batch 057] loss=0.322057, time=0.262s\n",
      "  [Batch 058] loss=0.324806, time=0.225s\n",
      "  [Batch 059] loss=0.307616, time=0.269s\n",
      "  [Batch 060] loss=0.388991, time=0.259s\n",
      "  [Batch 061] loss=0.336496, time=0.329s\n",
      "  [Batch 062] loss=0.355528, time=0.395s\n",
      "  [Batch 063] loss=0.316025, time=0.267s\n",
      "  [Batch 064] loss=0.293311, time=0.461s\n",
      "  [Batch 065] loss=0.253955, time=0.286s\n",
      "  [Batch 066] loss=0.489795, time=0.471s\n",
      "  [Batch 067] loss=0.577231, time=0.367s\n",
      "  [Batch 068] loss=0.285826, time=0.307s\n",
      "  [Batch 069] loss=0.296496, time=0.305s\n",
      "  [Batch 070] loss=0.338059, time=0.405s\n",
      "  [Batch 071] loss=0.315865, time=0.441s\n",
      "  [Batch 072] loss=0.299716, time=0.296s\n",
      "  [Batch 073] loss=0.259340, time=0.451s\n",
      "  [Batch 074] loss=0.258743, time=0.385s\n",
      "  [Batch 075] loss=0.273931, time=0.214s\n",
      "  [Batch 076] loss=0.289904, time=0.371s\n",
      "  [Batch 077] loss=0.373879, time=0.286s\n",
      "  [Batch 078] loss=0.312836, time=0.438s\n",
      "  [Batch 079] loss=0.257565, time=0.454s\n",
      "  [Batch 080] loss=0.315297, time=0.459s\n",
      "  [Batch 081] loss=0.305960, time=0.306s\n",
      "  [Batch 082] loss=0.309526, time=0.361s\n",
      "  [Batch 083] loss=0.404059, time=0.466s\n",
      "  [Batch 084] loss=0.302890, time=0.224s\n",
      "  [Batch 085] loss=0.272454, time=0.378s\n",
      "  [Batch 086] loss=0.296349, time=0.250s\n",
      "  [Batch 087] loss=0.321829, time=0.222s\n",
      "  [Batch 088] loss=0.271215, time=0.242s\n",
      "  [Batch 089] loss=0.309898, time=0.324s\n",
      "  [Batch 090] loss=0.316266, time=0.420s\n",
      "  [Batch 091] loss=0.326696, time=0.374s\n",
      "  [Batch 092] loss=0.297432, time=0.422s\n",
      "  [Batch 093] loss=0.336405, time=0.222s\n",
      "  [Batch 094] loss=0.278773, time=0.282s\n",
      "  [Batch 095] loss=0.252841, time=0.427s\n",
      "  [Batch 096] loss=0.224455, time=0.374s\n",
      "  [Batch 097] loss=0.241632, time=0.375s\n",
      "  [Batch 098] loss=0.228483, time=0.339s\n",
      "  [Batch 099] loss=0.301009, time=0.348s\n",
      "  [Batch 100] loss=0.305284, time=0.249s\n",
      "  [Batch 101] loss=0.309792, time=0.239s\n",
      "  [Batch 102] loss=0.367811, time=0.212s\n",
      "  [Batch 103] loss=0.384679, time=0.323s\n",
      "  [Batch 104] loss=0.386263, time=0.200s\n",
      "  [Batch 105] loss=0.389175, time=0.303s\n",
      "  [Batch 106] loss=0.373385, time=0.320s\n",
      "  [Batch 107] loss=0.370716, time=0.394s\n",
      "  [Batch 108] loss=0.235320, time=0.386s\n",
      "  [Batch 109] loss=0.244710, time=0.317s\n",
      "  [Batch 110] loss=0.254519, time=0.454s\n",
      "  [Batch 111] loss=0.239506, time=0.418s\n",
      "  [Batch 112] loss=0.323720, time=0.372s\n",
      "  [Batch 113] loss=0.432971, time=0.374s\n",
      "  [Batch 114] loss=0.358203, time=0.428s\n",
      "  [Batch 115] loss=0.263424, time=0.410s\n",
      "  [Batch 116] loss=0.243986, time=0.337s\n",
      "  [Batch 117] loss=0.280513, time=0.447s\n",
      "  [Batch 118] loss=0.406782, time=0.423s\n",
      "  [Batch 119] loss=0.229707, time=0.233s\n",
      "  [Batch 120] loss=0.306349, time=0.453s\n",
      "  [Batch 121] loss=0.323086, time=0.430s\n",
      "  [Batch 122] loss=0.234953, time=0.265s\n",
      "  [Batch 123] loss=0.231995, time=0.320s\n",
      "  [Batch 124] loss=0.250950, time=0.417s\n",
      "  [Batch 125] loss=0.223570, time=0.451s\n",
      "  [Batch 126] loss=0.245404, time=0.455s\n",
      "  [Batch 127] loss=0.326538, time=0.333s\n",
      "  [Batch 128] loss=0.258094, time=0.218s\n",
      "  [Batch 129] loss=0.280475, time=0.339s\n",
      "  [Batch 130] loss=0.205482, time=0.456s\n",
      "  [Batch 131] loss=0.265897, time=0.423s\n",
      "  [Batch 132] loss=0.272217, time=0.441s\n",
      "  [Batch 133] loss=0.303623, time=0.241s\n",
      "  [Batch 134] loss=0.261900, time=0.249s\n",
      "  [Batch 135] loss=0.420627, time=0.438s\n",
      "  [Batch 136] loss=0.230261, time=0.248s\n",
      "  [Batch 137] loss=0.182035, time=0.195s\n",
      "  [Batch 138] loss=0.269129, time=0.346s\n",
      "  [Batch 139] loss=0.227812, time=0.443s\n",
      "  [Batch 140] loss=0.263454, time=0.445s\n",
      "  [Batch 141] loss=0.207164, time=0.240s\n",
      "  [Batch 142] loss=0.313756, time=0.307s\n",
      "  [Batch 143] loss=0.170901, time=0.312s\n",
      "  [Batch 144] loss=0.220627, time=0.424s\n",
      "  [Batch 145] loss=0.212164, time=0.431s\n",
      "  [Batch 146] loss=0.221091, time=0.202s\n",
      "  [Batch 147] loss=0.269147, time=0.329s\n",
      "  [Batch 148] loss=0.207952, time=0.295s\n",
      "  [Batch 149] loss=0.219943, time=0.454s\n",
      "  [Batch 150] loss=0.253476, time=0.275s\n",
      "  [Batch 151] loss=0.282096, time=0.315s\n",
      "  [Batch 152] loss=0.244042, time=0.310s\n",
      "  [Batch 153] loss=0.266954, time=0.340s\n",
      "  [Batch 154] loss=0.196308, time=0.201s\n",
      "  [Batch 155] loss=0.377219, time=0.370s\n",
      "  [Batch 156] loss=0.287203, time=0.397s\n",
      "  [Batch 157] loss=0.230755, time=0.411s\n",
      "  [Batch 158] loss=0.266353, time=0.384s\n",
      "  [Batch 159] loss=0.217348, time=0.355s\n",
      "  [Batch 160] loss=0.259423, time=0.350s\n",
      "  [Batch 161] loss=0.218007, time=0.405s\n",
      "  [Batch 162] loss=0.221668, time=0.368s\n",
      "  [Batch 163] loss=0.270385, time=0.399s\n",
      "  [Batch 164] loss=0.213253, time=0.301s\n",
      "  [Batch 165] loss=0.234819, time=0.398s\n",
      "  [Batch 166] loss=0.253182, time=0.410s\n",
      "  [Batch 167] loss=0.243983, time=0.362s\n",
      "  [Batch 168] loss=0.193846, time=0.240s\n",
      "  [Batch 169] loss=0.217531, time=0.274s\n",
      "  [Batch 170] loss=0.218513, time=0.382s\n",
      "  [Batch 171] loss=0.312185, time=0.229s\n",
      "  [Batch 172] loss=0.179083, time=0.463s\n",
      "  [Batch 173] loss=0.190594, time=0.407s\n",
      "  [Batch 174] loss=0.248806, time=0.168s\n",
      "  [Batch 175] loss=0.294394, time=0.185s\n",
      "  [Batch 176] loss=0.225111, time=0.252s\n",
      "  [Batch 177] loss=0.211324, time=0.410s\n",
      "  [Batch 178] loss=0.182577, time=0.336s\n",
      "  [Batch 179] loss=0.192271, time=0.183s\n",
      "  [Batch 180] loss=0.243004, time=0.203s\n",
      "  [Batch 181] loss=0.220116, time=0.241s\n",
      "  [Batch 182] loss=0.226821, time=0.245s\n",
      "  [Batch 183] loss=0.182161, time=0.231s\n",
      "  [Batch 184] loss=0.174297, time=0.286s\n",
      "  [Batch 185] loss=0.316411, time=0.333s\n",
      "  [Batch 186] loss=0.150442, time=0.234s\n",
      "  [Batch 187] loss=0.255690, time=0.235s\n",
      "  [Batch 188] loss=0.490306, time=0.416s\n",
      "  [Batch 189] loss=0.255952, time=0.186s\n",
      "  [Batch 190] loss=0.393662, time=0.194s\n",
      "  [Batch 191] loss=0.201169, time=0.256s\n",
      "  [Batch 192] loss=0.284174, time=0.384s\n",
      "  [Batch 193] loss=0.196513, time=0.362s\n",
      "  [Batch 194] loss=0.242387, time=0.319s\n",
      "  [Batch 195] loss=0.284498, time=0.277s\n",
      "  [Batch 196] loss=0.261184, time=0.265s\n",
      "  [Batch 197] loss=0.272806, time=0.369s\n",
      "  [Batch 198] loss=0.224760, time=0.216s\n",
      "  [Batch 199] loss=0.198158, time=0.206s\n",
      "  [Batch 200] loss=0.202544, time=0.334s\n",
      "  [Batch 201] loss=0.195752, time=0.390s\n",
      "  [Batch 202] loss=0.239784, time=0.372s\n",
      "  [Batch 203] loss=0.181531, time=0.334s\n",
      "  [Batch 204] loss=0.197273, time=0.429s\n",
      "  [Batch 205] loss=0.225541, time=0.196s\n",
      "  [Batch 206] loss=0.267988, time=0.266s\n",
      "  [Batch 207] loss=0.241811, time=0.221s\n",
      "  [Batch 208] loss=0.202392, time=0.234s\n",
      "  [Batch 209] loss=0.270144, time=0.425s\n",
      "  [Batch 210] loss=0.208300, time=0.205s\n",
      "  [Batch 211] loss=0.181960, time=0.324s\n",
      "  [Batch 212] loss=0.224131, time=0.214s\n",
      "  [Batch 213] loss=0.244243, time=0.232s\n",
      "  [Batch 214] loss=0.258005, time=0.317s\n",
      "  [Batch 215] loss=0.203102, time=0.407s\n",
      "  [Batch 216] loss=0.255895, time=0.370s\n",
      "  [Batch 217] loss=0.316836, time=0.417s\n",
      "  [Batch 218] loss=0.283725, time=0.272s\n",
      "  [Batch 219] loss=0.294932, time=0.426s\n",
      "  [Batch 220] loss=0.229510, time=0.410s\n",
      "  [Batch 221] loss=0.256853, time=0.380s\n",
      "  [Batch 222] loss=0.210707, time=0.338s\n",
      "  [Batch 223] loss=0.199723, time=0.451s\n",
      "  [Batch 224] loss=0.216775, time=0.317s\n",
      "  [Batch 225] loss=0.270682, time=0.242s\n",
      "  [Batch 226] loss=0.301996, time=0.244s\n",
      "  [Batch 227] loss=0.271412, time=0.282s\n",
      "  [Batch 228] loss=0.261073, time=0.300s\n",
      "  [Batch 229] loss=0.243400, time=0.447s\n",
      "  [Batch 230] loss=0.242090, time=0.290s\n",
      "  [Batch 231] loss=0.188850, time=0.299s\n",
      "  [Batch 232] loss=0.168598, time=0.244s\n",
      "  [Batch 233] loss=0.153847, time=0.206s\n",
      "  [Batch 234] loss=0.263248, time=0.379s\n",
      "  [Batch 235] loss=0.201211, time=0.421s\n",
      "  [Batch 236] loss=0.264781, time=0.234s\n",
      "  [Batch 237] loss=0.197202, time=0.274s\n",
      "  [Batch 238] loss=0.151810, time=0.403s\n",
      "  [Batch 239] loss=0.207747, time=0.386s\n",
      "  [Batch 240] loss=0.268069, time=0.307s\n",
      "  [Batch 241] loss=0.230771, time=0.332s\n",
      "  [Batch 242] loss=0.200101, time=0.370s\n",
      "  [Batch 243] loss=0.204025, time=0.410s\n",
      "  [Batch 244] loss=0.425908, time=0.421s\n",
      "  [Batch 245] loss=0.209511, time=0.414s\n",
      "  [Batch 246] loss=0.217123, time=0.278s\n",
      "  [Batch 247] loss=0.190060, time=0.336s\n",
      "  [Batch 248] loss=0.218494, time=0.188s\n",
      "  [Batch 249] loss=0.210459, time=0.337s\n",
      "  [Batch 250] loss=0.273911, time=0.415s\n",
      "  [Batch 251] loss=0.037459, time=0.045s\n",
      "\n",
      "[Validation] Loss: 0.622039\n",
      "  Size= 2 : MSE=0.002543 MAE=0.040072 MAPE=41.97%\n",
      "  Size= 4 : MSE=0.007633 MAE=0.059846 MAPE=17.94%\n",
      "  Size= 6 : MSE=0.013018 MAE=0.080204 MAPE=64.60%\n",
      "  Size= 8 : MSE=0.027777 MAE=0.126083 MAPE=34.39%\n",
      "  Size=10 : MSE=0.043557 MAE=0.156241 MAPE=1940.04%\n",
      "  Size=12 : MSE=0.059824 MAE=0.183965 MAPE=67.17%\n",
      "[Info] New best model saved (val_loss=0.622039)\n",
      "\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amssa\\anaconda3\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Batch 000] loss=0.205947, time=0.685s\n",
      "  [Batch 001] loss=0.276370, time=0.414s\n",
      "  [Batch 002] loss=0.221187, time=0.412s\n",
      "  [Batch 003] loss=0.215707, time=0.477s\n",
      "  [Batch 004] loss=0.333962, time=0.244s\n",
      "  [Batch 005] loss=0.304534, time=0.329s\n",
      "  [Batch 006] loss=0.224144, time=0.489s\n",
      "  [Batch 007] loss=0.230042, time=0.265s\n",
      "  [Batch 008] loss=0.185692, time=0.367s\n",
      "  [Batch 009] loss=0.168142, time=0.384s\n",
      "  [Batch 010] loss=0.222292, time=0.462s\n",
      "  [Batch 011] loss=0.258654, time=0.232s\n",
      "  [Batch 012] loss=0.285875, time=0.447s\n",
      "  [Batch 013] loss=0.149791, time=0.329s\n",
      "  [Batch 014] loss=0.222346, time=0.441s\n",
      "  [Batch 015] loss=0.152922, time=0.459s\n",
      "  [Batch 016] loss=0.245393, time=0.297s\n",
      "  [Batch 017] loss=0.170472, time=0.236s\n",
      "  [Batch 018] loss=0.165756, time=0.339s\n",
      "  [Batch 019] loss=0.206734, time=0.417s\n",
      "  [Batch 020] loss=0.162794, time=0.201s\n",
      "  [Batch 021] loss=0.220008, time=0.237s\n",
      "  [Batch 022] loss=0.170315, time=0.318s\n",
      "  [Batch 023] loss=0.196068, time=0.279s\n",
      "  [Batch 024] loss=0.257378, time=0.259s\n",
      "  [Batch 025] loss=0.204077, time=0.275s\n",
      "  [Batch 026] loss=0.203405, time=0.361s\n",
      "  [Batch 027] loss=0.171499, time=0.307s\n",
      "  [Batch 028] loss=0.212360, time=0.242s\n",
      "  [Batch 029] loss=0.169709, time=0.212s\n",
      "  [Batch 030] loss=0.263962, time=0.399s\n",
      "  [Batch 031] loss=0.164958, time=0.413s\n",
      "  [Batch 032] loss=0.306156, time=0.465s\n",
      "  [Batch 033] loss=0.192819, time=0.437s\n",
      "  [Batch 034] loss=0.165884, time=0.195s\n",
      "  [Batch 035] loss=0.187054, time=0.297s\n",
      "  [Batch 036] loss=0.238222, time=0.201s\n",
      "  [Batch 037] loss=0.184936, time=0.427s\n",
      "  [Batch 038] loss=0.185164, time=0.443s\n",
      "  [Batch 039] loss=0.179282, time=0.431s\n",
      "  [Batch 040] loss=0.280605, time=0.395s\n",
      "  [Batch 041] loss=0.211439, time=0.385s\n",
      "  [Batch 042] loss=0.215773, time=0.315s\n",
      "  [Batch 043] loss=0.243630, time=0.411s\n",
      "  [Batch 044] loss=0.224819, time=0.257s\n",
      "  [Batch 045] loss=0.210595, time=0.204s\n",
      "  [Batch 046] loss=0.321260, time=0.417s\n",
      "  [Batch 047] loss=0.235511, time=0.215s\n",
      "  [Batch 048] loss=0.155487, time=0.416s\n",
      "  [Batch 049] loss=0.216330, time=0.436s\n",
      "  [Batch 050] loss=0.199450, time=0.266s\n",
      "  [Batch 051] loss=0.162919, time=0.398s\n",
      "  [Batch 052] loss=0.189122, time=0.217s\n",
      "  [Batch 053] loss=0.209453, time=0.282s\n",
      "  [Batch 054] loss=0.191966, time=0.277s\n",
      "  [Batch 055] loss=0.205951, time=0.319s\n",
      "  [Batch 056] loss=0.228598, time=0.381s\n",
      "  [Batch 057] loss=0.224491, time=0.297s\n",
      "  [Batch 058] loss=0.158960, time=0.375s\n",
      "  [Batch 059] loss=0.254673, time=0.357s\n",
      "  [Batch 060] loss=0.208413, time=0.368s\n",
      "  [Batch 061] loss=0.201460, time=0.418s\n",
      "  [Batch 062] loss=0.193959, time=0.417s\n",
      "  [Batch 063] loss=0.173793, time=0.352s\n",
      "  [Batch 064] loss=0.252624, time=0.222s\n",
      "  [Batch 065] loss=0.199175, time=0.163s\n",
      "  [Batch 066] loss=0.224204, time=0.380s\n",
      "  [Batch 067] loss=0.196853, time=0.407s\n",
      "  [Batch 068] loss=0.180524, time=0.361s\n",
      "  [Batch 069] loss=0.254988, time=0.331s\n",
      "  [Batch 070] loss=0.234544, time=0.307s\n",
      "  [Batch 071] loss=0.179976, time=0.431s\n",
      "  [Batch 072] loss=0.156596, time=0.433s\n",
      "  [Batch 073] loss=0.197596, time=0.412s\n",
      "  [Batch 074] loss=0.226537, time=0.396s\n",
      "  [Batch 075] loss=0.162582, time=0.340s\n",
      "  [Batch 076] loss=0.333429, time=0.262s\n",
      "  [Batch 077] loss=0.201457, time=0.199s\n",
      "  [Batch 078] loss=0.226489, time=0.275s\n",
      "  [Batch 079] loss=0.222810, time=0.303s\n",
      "  [Batch 080] loss=0.169862, time=0.299s\n",
      "  [Batch 081] loss=0.195547, time=0.377s\n",
      "  [Batch 082] loss=0.233301, time=0.188s\n",
      "  [Batch 083] loss=0.226228, time=0.222s\n",
      "  [Batch 084] loss=0.188499, time=0.273s\n",
      "  [Batch 085] loss=0.190791, time=0.318s\n",
      "  [Batch 086] loss=0.256551, time=0.354s\n",
      "  [Batch 087] loss=0.320552, time=0.344s\n",
      "  [Batch 088] loss=0.202124, time=0.160s\n",
      "  [Batch 089] loss=0.258459, time=0.185s\n",
      "  [Batch 090] loss=0.197976, time=0.172s\n",
      "  [Batch 091] loss=0.195335, time=0.189s\n",
      "  [Batch 092] loss=0.198551, time=0.413s\n",
      "  [Batch 093] loss=0.190352, time=0.399s\n",
      "  [Batch 094] loss=0.172146, time=0.297s\n",
      "  [Batch 095] loss=0.259828, time=0.438s\n",
      "  [Batch 096] loss=0.207392, time=0.328s\n",
      "  [Batch 097] loss=0.219063, time=0.282s\n",
      "  [Batch 098] loss=0.241839, time=0.405s\n",
      "  [Batch 099] loss=0.173980, time=0.190s\n",
      "  [Batch 100] loss=0.159776, time=0.207s\n",
      "  [Batch 101] loss=0.205131, time=0.305s\n",
      "  [Batch 102] loss=0.196630, time=0.287s\n",
      "  [Batch 103] loss=0.195293, time=0.248s\n",
      "  [Batch 104] loss=0.204153, time=0.239s\n",
      "  [Batch 105] loss=0.201795, time=0.322s\n",
      "  [Batch 106] loss=0.217391, time=0.397s\n",
      "  [Batch 107] loss=0.212586, time=0.420s\n",
      "  [Batch 108] loss=0.187187, time=0.329s\n",
      "  [Batch 109] loss=0.181401, time=0.246s\n",
      "  [Batch 110] loss=0.185781, time=0.439s\n",
      "  [Batch 111] loss=0.176020, time=0.334s\n",
      "  [Batch 112] loss=0.215413, time=0.402s\n",
      "  [Batch 113] loss=0.292030, time=0.382s\n",
      "  [Batch 114] loss=0.228602, time=0.200s\n",
      "  [Batch 115] loss=0.221942, time=0.204s\n",
      "  [Batch 116] loss=0.209294, time=0.314s\n",
      "  [Batch 117] loss=0.201850, time=0.247s\n",
      "  [Batch 118] loss=0.189046, time=0.229s\n",
      "  [Batch 119] loss=0.196988, time=0.385s\n",
      "  [Batch 120] loss=0.218362, time=0.413s\n",
      "  [Batch 121] loss=0.238882, time=0.339s\n",
      "  [Batch 122] loss=0.158779, time=0.338s\n",
      "  [Batch 123] loss=0.177517, time=0.176s\n",
      "  [Batch 124] loss=0.172235, time=0.180s\n",
      "  [Batch 125] loss=0.181263, time=0.252s\n",
      "  [Batch 126] loss=0.191342, time=0.369s\n",
      "  [Batch 127] loss=0.231022, time=0.360s\n",
      "  [Batch 128] loss=0.193993, time=0.409s\n",
      "  [Batch 129] loss=0.159458, time=0.193s\n",
      "  [Batch 130] loss=0.132305, time=0.176s\n",
      "  [Batch 131] loss=0.154054, time=0.284s\n",
      "  [Batch 132] loss=0.173376, time=0.198s\n",
      "  [Batch 133] loss=0.214117, time=0.245s\n",
      "  [Batch 134] loss=0.149864, time=0.471s\n",
      "  [Batch 135] loss=0.238253, time=0.294s\n",
      "  [Batch 136] loss=0.190257, time=0.261s\n",
      "  [Batch 137] loss=0.241924, time=0.220s\n",
      "  [Batch 138] loss=0.185367, time=0.211s\n",
      "  [Batch 139] loss=0.164275, time=0.409s\n",
      "  [Batch 140] loss=0.146581, time=0.346s\n",
      "  [Batch 141] loss=0.223343, time=0.289s\n",
      "  [Batch 142] loss=0.220193, time=0.263s\n",
      "  [Batch 143] loss=0.201735, time=0.217s\n",
      "  [Batch 144] loss=0.199348, time=0.289s\n",
      "  [Batch 145] loss=0.173892, time=0.390s\n",
      "  [Batch 146] loss=0.206003, time=0.386s\n",
      "  [Batch 147] loss=0.248887, time=0.271s\n",
      "  [Batch 148] loss=0.211778, time=0.294s\n",
      "  [Batch 149] loss=0.191726, time=0.417s\n",
      "  [Batch 150] loss=0.165564, time=0.248s\n",
      "  [Batch 151] loss=0.158126, time=0.221s\n",
      "  [Batch 152] loss=0.170677, time=0.318s\n",
      "  [Batch 153] loss=0.159513, time=0.187s\n",
      "  [Batch 154] loss=0.245530, time=0.407s\n",
      "  [Batch 155] loss=0.233906, time=0.207s\n",
      "  [Batch 156] loss=0.231556, time=0.215s\n",
      "  [Batch 157] loss=0.187858, time=0.202s\n",
      "  [Batch 158] loss=0.191560, time=0.341s\n",
      "  [Batch 159] loss=0.262730, time=0.419s\n",
      "  [Batch 160] loss=0.206300, time=0.409s\n",
      "  [Batch 161] loss=0.212465, time=0.414s\n",
      "  [Batch 162] loss=0.188025, time=0.391s\n",
      "  [Batch 163] loss=0.158953, time=0.400s\n",
      "  [Batch 164] loss=0.142759, time=0.244s\n",
      "  [Batch 165] loss=0.175632, time=0.401s\n",
      "  [Batch 166] loss=0.172680, time=0.301s\n",
      "  [Batch 167] loss=0.175229, time=0.233s\n",
      "  [Batch 168] loss=0.163569, time=0.319s\n",
      "  [Batch 169] loss=0.197773, time=0.258s\n",
      "  [Batch 170] loss=0.147168, time=0.434s\n",
      "  [Batch 171] loss=0.194327, time=0.263s\n",
      "  [Batch 172] loss=0.238479, time=0.460s\n",
      "  [Batch 173] loss=0.164025, time=0.414s\n",
      "  [Batch 174] loss=0.149458, time=0.420s\n",
      "  [Batch 175] loss=0.148489, time=0.361s\n",
      "  [Batch 176] loss=0.157688, time=0.267s\n",
      "  [Batch 177] loss=0.168108, time=0.172s\n",
      "  [Batch 178] loss=0.176440, time=0.342s\n",
      "  [Batch 179] loss=0.160183, time=0.349s\n",
      "  [Batch 180] loss=0.235474, time=0.242s\n",
      "  [Batch 181] loss=0.171517, time=0.266s\n",
      "  [Batch 182] loss=0.267149, time=0.375s\n",
      "  [Batch 183] loss=0.164743, time=0.387s\n",
      "  [Batch 184] loss=0.155674, time=0.250s\n",
      "  [Batch 185] loss=0.161105, time=0.300s\n",
      "  [Batch 186] loss=0.170491, time=0.281s\n",
      "  [Batch 187] loss=0.157645, time=0.347s\n",
      "  [Batch 188] loss=0.232255, time=0.292s\n",
      "  [Batch 189] loss=0.225412, time=0.307s\n",
      "  [Batch 190] loss=0.173925, time=0.355s\n",
      "  [Batch 191] loss=0.132986, time=0.209s\n",
      "  [Batch 192] loss=0.191569, time=0.313s\n",
      "  [Batch 193] loss=0.177785, time=0.200s\n",
      "  [Batch 194] loss=0.189244, time=0.219s\n",
      "  [Batch 195] loss=0.183655, time=0.413s\n",
      "  [Batch 196] loss=0.129543, time=0.436s\n",
      "  [Batch 197] loss=0.288374, time=0.307s\n",
      "  [Batch 198] loss=0.121232, time=0.368s\n",
      "  [Batch 199] loss=0.165963, time=0.424s\n",
      "  [Batch 200] loss=0.275042, time=0.404s\n",
      "  [Batch 201] loss=0.206300, time=0.233s\n",
      "  [Batch 202] loss=0.177688, time=0.403s\n",
      "  [Batch 203] loss=0.192680, time=0.214s\n",
      "  [Batch 204] loss=0.142035, time=0.328s\n",
      "  [Batch 205] loss=0.165200, time=0.427s\n",
      "  [Batch 206] loss=0.147665, time=0.338s\n",
      "  [Batch 207] loss=0.167093, time=0.378s\n",
      "  [Batch 208] loss=0.163331, time=0.393s\n",
      "  [Batch 209] loss=0.229072, time=0.418s\n",
      "  [Batch 210] loss=0.168025, time=0.315s\n",
      "  [Batch 211] loss=0.131008, time=0.420s\n",
      "  [Batch 212] loss=0.194543, time=0.231s\n",
      "  [Batch 213] loss=0.208155, time=0.348s\n",
      "  [Batch 214] loss=0.245255, time=0.324s\n",
      "  [Batch 215] loss=0.194346, time=0.450s\n",
      "  [Batch 216] loss=0.195375, time=0.193s\n",
      "  [Batch 217] loss=0.165467, time=0.431s\n",
      "  [Batch 218] loss=0.173839, time=0.411s\n",
      "  [Batch 219] loss=0.179142, time=0.263s\n",
      "  [Batch 220] loss=0.165754, time=0.252s\n",
      "  [Batch 221] loss=0.233690, time=0.181s\n",
      "  [Batch 222] loss=0.198355, time=0.439s\n",
      "  [Batch 223] loss=0.215621, time=0.201s\n",
      "  [Batch 224] loss=0.166990, time=0.440s\n",
      "  [Batch 225] loss=0.184140, time=0.440s\n",
      "  [Batch 226] loss=0.203549, time=0.382s\n",
      "  [Batch 227] loss=0.202031, time=0.436s\n",
      "  [Batch 228] loss=0.109638, time=0.446s\n",
      "  [Batch 229] loss=0.147364, time=0.425s\n",
      "  [Batch 230] loss=0.147231, time=0.302s\n",
      "  [Batch 231] loss=0.133686, time=0.252s\n",
      "  [Batch 232] loss=0.169307, time=0.233s\n",
      "  [Batch 233] loss=0.147630, time=0.204s\n",
      "  [Batch 234] loss=0.282152, time=0.440s\n",
      "  [Batch 235] loss=0.139566, time=0.238s\n",
      "  [Batch 236] loss=0.193238, time=0.215s\n",
      "  [Batch 237] loss=0.166092, time=0.266s\n",
      "  [Batch 238] loss=0.175167, time=0.189s\n",
      "  [Batch 239] loss=0.211653, time=0.302s\n",
      "  [Batch 240] loss=0.175893, time=0.323s\n",
      "  [Batch 241] loss=0.154161, time=0.281s\n",
      "  [Batch 242] loss=0.190341, time=0.428s\n",
      "  [Batch 243] loss=0.178238, time=0.423s\n",
      "  [Batch 244] loss=0.162312, time=0.257s\n",
      "  [Batch 245] loss=0.141031, time=0.395s\n",
      "  [Batch 246] loss=0.188762, time=0.465s\n",
      "  [Batch 247] loss=0.123798, time=0.442s\n",
      "  [Batch 248] loss=0.136769, time=0.223s\n",
      "  [Batch 249] loss=0.162125, time=0.226s\n",
      "  [Batch 250] loss=0.295209, time=0.402s\n",
      "  [Batch 251] loss=0.312401, time=0.043s\n",
      "\n",
      "[Validation] Loss: 0.482772\n",
      "  Size= 2 : MSE=0.000551 MAE=0.018779 MAPE=28.67%\n",
      "  Size= 4 : MSE=0.005414 MAE=0.048854 MAPE=16.43%\n",
      "  Size= 6 : MSE=0.009657 MAE=0.069946 MAPE=37.74%\n",
      "  Size= 8 : MSE=0.023706 MAE=0.116499 MAPE=31.59%\n",
      "  Size=10 : MSE=0.036888 MAE=0.142727 MAPE=1028.72%\n",
      "  Size=12 : MSE=0.041520 MAE=0.150590 MAPE=36.63%\n",
      "[Info] New best model saved (val_loss=0.482772)\n",
      "\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amssa\\anaconda3\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Batch 000] loss=0.190119, time=0.765s\n",
      "  [Batch 001] loss=0.185925, time=0.615s\n",
      "  [Batch 002] loss=0.140049, time=0.263s\n",
      "  [Batch 003] loss=0.255653, time=0.321s\n",
      "  [Batch 004] loss=0.213246, time=0.366s\n",
      "  [Batch 005] loss=0.208204, time=0.343s\n",
      "  [Batch 006] loss=0.167255, time=0.361s\n",
      "  [Batch 007] loss=0.207878, time=0.450s\n",
      "  [Batch 008] loss=0.189681, time=0.421s\n",
      "  [Batch 009] loss=0.278139, time=0.534s\n",
      "  [Batch 010] loss=0.168731, time=0.249s\n",
      "  [Batch 011] loss=0.163367, time=0.459s\n",
      "  [Batch 012] loss=0.164868, time=0.472s\n",
      "  [Batch 013] loss=0.199090, time=0.438s\n",
      "  [Batch 014] loss=0.208514, time=0.487s\n",
      "  [Batch 015] loss=0.169479, time=0.357s\n",
      "  [Batch 016] loss=0.144530, time=0.377s\n",
      "  [Batch 017] loss=0.241863, time=0.253s\n",
      "  [Batch 018] loss=0.190202, time=0.430s\n",
      "  [Batch 019] loss=0.166105, time=0.260s\n",
      "  [Batch 020] loss=0.168403, time=0.394s\n",
      "  [Batch 021] loss=0.143363, time=0.362s\n",
      "  [Batch 022] loss=0.170210, time=0.321s\n",
      "  [Batch 023] loss=0.161148, time=0.348s\n",
      "  [Batch 024] loss=0.145710, time=0.458s\n",
      "  [Batch 025] loss=0.299323, time=0.424s\n",
      "  [Batch 026] loss=0.148843, time=0.420s\n",
      "  [Batch 027] loss=0.153832, time=0.387s\n",
      "  [Batch 028] loss=0.219212, time=0.465s\n",
      "  [Batch 029] loss=0.166421, time=0.343s\n",
      "  [Batch 030] loss=0.190421, time=0.251s\n",
      "  [Batch 031] loss=0.162708, time=0.323s\n",
      "  [Batch 032] loss=0.173739, time=0.295s\n",
      "  [Batch 033] loss=0.144687, time=0.446s\n",
      "  [Batch 034] loss=0.204101, time=0.218s\n",
      "  [Batch 035] loss=0.179242, time=0.377s\n",
      "  [Batch 036] loss=0.159440, time=0.217s\n",
      "  [Batch 037] loss=0.158696, time=0.374s\n",
      "  [Batch 038] loss=0.169490, time=0.475s\n",
      "  [Batch 039] loss=0.167691, time=0.203s\n",
      "  [Batch 040] loss=0.192439, time=0.324s\n",
      "  [Batch 041] loss=0.165172, time=0.436s\n",
      "  [Batch 042] loss=0.164123, time=0.309s\n",
      "  [Batch 043] loss=0.236501, time=0.380s\n",
      "  [Batch 044] loss=0.149645, time=0.214s\n",
      "  [Batch 045] loss=0.176345, time=0.266s\n",
      "  [Batch 046] loss=0.207242, time=0.384s\n",
      "  [Batch 047] loss=0.144408, time=0.420s\n",
      "  [Batch 048] loss=0.182310, time=0.326s\n",
      "  [Batch 049] loss=0.238511, time=0.363s\n",
      "  [Batch 050] loss=0.156237, time=0.396s\n",
      "  [Batch 051] loss=0.156014, time=0.449s\n",
      "  [Batch 052] loss=0.160047, time=0.304s\n",
      "  [Batch 053] loss=0.152229, time=0.207s\n",
      "  [Batch 054] loss=0.244734, time=0.298s\n",
      "  [Batch 055] loss=0.211936, time=0.336s\n",
      "  [Batch 056] loss=0.219383, time=0.412s\n",
      "  [Batch 057] loss=0.158815, time=0.369s\n",
      "  [Batch 058] loss=0.228045, time=0.250s\n",
      "  [Batch 059] loss=0.225267, time=0.429s\n",
      "  [Batch 060] loss=0.133399, time=0.345s\n",
      "  [Batch 061] loss=0.172048, time=0.375s\n",
      "  [Batch 062] loss=0.153010, time=0.430s\n",
      "  [Batch 063] loss=0.151908, time=0.395s\n",
      "  [Batch 064] loss=0.145729, time=0.228s\n",
      "  [Batch 065] loss=0.226497, time=0.196s\n",
      "  [Batch 066] loss=0.160761, time=0.271s\n",
      "  [Batch 067] loss=0.130213, time=0.305s\n",
      "  [Batch 068] loss=0.172180, time=0.428s\n",
      "  [Batch 069] loss=0.159166, time=0.435s\n",
      "  [Batch 070] loss=0.241370, time=0.264s\n",
      "  [Batch 071] loss=0.144260, time=0.353s\n",
      "  [Batch 072] loss=0.162046, time=0.432s\n",
      "  [Batch 073] loss=0.150057, time=0.218s\n",
      "  [Batch 074] loss=0.152865, time=0.385s\n",
      "  [Batch 075] loss=0.196269, time=0.214s\n",
      "  [Batch 076] loss=0.198895, time=0.397s\n",
      "  [Batch 077] loss=0.223012, time=0.379s\n",
      "  [Batch 078] loss=0.163462, time=0.332s\n",
      "  [Batch 079] loss=0.163726, time=0.435s\n",
      "  [Batch 080] loss=0.190624, time=0.442s\n",
      "  [Batch 081] loss=0.161299, time=0.321s\n",
      "  [Batch 082] loss=0.136058, time=0.278s\n",
      "  [Batch 083] loss=0.198152, time=0.298s\n",
      "  [Batch 084] loss=0.278588, time=0.230s\n",
      "  [Batch 085] loss=0.126575, time=0.263s\n",
      "  [Batch 086] loss=0.185822, time=0.277s\n",
      "  [Batch 087] loss=0.197089, time=0.265s\n",
      "  [Batch 088] loss=0.119621, time=0.198s\n",
      "  [Batch 089] loss=0.169158, time=0.219s\n",
      "  [Batch 090] loss=0.138165, time=0.413s\n",
      "  [Batch 091] loss=0.259962, time=0.300s\n",
      "  [Batch 092] loss=0.145548, time=0.259s\n",
      "  [Batch 093] loss=0.207151, time=0.373s\n",
      "  [Batch 094] loss=0.214040, time=0.434s\n",
      "  [Batch 095] loss=0.179012, time=0.213s\n",
      "  [Batch 096] loss=0.179603, time=0.266s\n",
      "  [Batch 097] loss=0.164744, time=0.391s\n",
      "  [Batch 098] loss=0.212711, time=0.425s\n",
      "  [Batch 099] loss=0.211400, time=0.382s\n",
      "  [Batch 100] loss=0.213156, time=0.324s\n",
      "  [Batch 101] loss=0.187451, time=0.328s\n",
      "  [Batch 102] loss=0.180144, time=0.432s\n",
      "  [Batch 103] loss=0.206488, time=0.646s\n",
      "  [Batch 104] loss=0.172623, time=0.404s\n",
      "  [Batch 105] loss=0.235333, time=0.336s\n",
      "  [Batch 106] loss=0.186379, time=0.231s\n",
      "  [Batch 107] loss=0.190273, time=0.409s\n",
      "  [Batch 108] loss=0.221820, time=0.435s\n",
      "  [Batch 109] loss=0.208597, time=0.222s\n",
      "  [Batch 110] loss=0.138014, time=0.246s\n",
      "  [Batch 111] loss=0.155901, time=0.329s\n",
      "  [Batch 112] loss=0.175314, time=0.394s\n",
      "  [Batch 113] loss=0.154661, time=0.254s\n",
      "  [Batch 114] loss=0.195220, time=0.209s\n",
      "  [Batch 115] loss=0.162197, time=0.423s\n",
      "  [Batch 116] loss=0.155207, time=0.215s\n",
      "  [Batch 117] loss=0.217165, time=0.342s\n",
      "  [Batch 118] loss=0.158807, time=0.231s\n",
      "  [Batch 119] loss=0.179386, time=0.400s\n",
      "  [Batch 120] loss=0.187557, time=0.349s\n",
      "  [Batch 121] loss=0.163651, time=0.283s\n",
      "  [Batch 122] loss=0.152362, time=0.455s\n",
      "  [Batch 123] loss=0.157476, time=0.464s\n",
      "  [Batch 124] loss=0.178683, time=0.334s\n",
      "  [Batch 125] loss=0.141562, time=0.398s\n",
      "  [Batch 126] loss=0.133110, time=0.286s\n",
      "  [Batch 127] loss=0.147426, time=0.367s\n",
      "  [Batch 128] loss=0.157795, time=0.261s\n",
      "  [Batch 129] loss=0.157255, time=0.225s\n",
      "  [Batch 130] loss=0.141090, time=0.408s\n",
      "  [Batch 131] loss=0.127142, time=0.285s\n",
      "  [Batch 132] loss=0.185030, time=0.253s\n",
      "  [Batch 133] loss=0.152565, time=0.297s\n",
      "  [Batch 134] loss=0.179586, time=0.251s\n",
      "  [Batch 135] loss=0.176724, time=0.249s\n",
      "  [Batch 136] loss=0.277914, time=0.270s\n",
      "  [Batch 137] loss=0.152722, time=0.355s\n",
      "  [Batch 138] loss=0.151213, time=0.499s\n",
      "  [Batch 139] loss=0.192769, time=0.427s\n",
      "  [Batch 140] loss=0.189799, time=0.390s\n",
      "  [Batch 141] loss=0.156519, time=0.403s\n",
      "  [Batch 142] loss=0.144298, time=0.408s\n",
      "  [Batch 143] loss=0.174600, time=0.189s\n",
      "  [Batch 144] loss=0.211176, time=0.383s\n",
      "  [Batch 145] loss=0.225115, time=0.371s\n",
      "  [Batch 146] loss=0.174586, time=0.176s\n",
      "  [Batch 147] loss=0.240048, time=0.202s\n",
      "  [Batch 148] loss=0.161392, time=0.246s\n",
      "  [Batch 149] loss=0.157214, time=0.236s\n",
      "  [Batch 150] loss=0.165485, time=0.402s\n",
      "  [Batch 151] loss=0.152488, time=0.419s\n",
      "  [Batch 152] loss=0.144963, time=0.338s\n",
      "  [Batch 153] loss=0.149712, time=0.375s\n",
      "  [Batch 154] loss=0.191025, time=0.342s\n",
      "  [Batch 155] loss=0.186964, time=0.240s\n",
      "  [Batch 156] loss=0.156453, time=0.391s\n",
      "  [Batch 157] loss=0.214202, time=0.271s\n",
      "  [Batch 158] loss=0.141127, time=0.393s\n",
      "  [Batch 159] loss=0.174616, time=0.191s\n",
      "  [Batch 160] loss=0.140521, time=0.352s\n",
      "  [Batch 161] loss=0.156019, time=0.262s\n",
      "  [Batch 162] loss=0.172073, time=0.424s\n",
      "  [Batch 163] loss=0.153221, time=0.430s\n",
      "  [Batch 164] loss=0.179908, time=0.376s\n",
      "  [Batch 165] loss=0.205602, time=0.419s\n",
      "  [Batch 166] loss=0.179328, time=0.245s\n",
      "  [Batch 167] loss=0.144524, time=0.193s\n",
      "  [Batch 168] loss=0.132213, time=0.395s\n",
      "  [Batch 169] loss=0.146191, time=0.300s\n",
      "  [Batch 170] loss=0.159575, time=0.370s\n",
      "  [Batch 171] loss=0.133813, time=0.266s\n",
      "  [Batch 172] loss=0.165494, time=0.348s\n",
      "  [Batch 173] loss=0.189598, time=0.256s\n",
      "  [Batch 174] loss=0.142726, time=0.254s\n",
      "  [Batch 175] loss=0.161554, time=0.395s\n",
      "  [Batch 176] loss=0.150604, time=0.393s\n",
      "  [Batch 177] loss=0.163992, time=0.393s\n",
      "  [Batch 178] loss=0.164443, time=0.356s\n",
      "  [Batch 179] loss=0.173434, time=0.393s\n",
      "  [Batch 180] loss=0.161112, time=0.393s\n",
      "  [Batch 181] loss=0.142067, time=0.180s\n",
      "  [Batch 182] loss=0.133404, time=0.428s\n",
      "  [Batch 183] loss=0.136182, time=0.280s\n",
      "  [Batch 184] loss=0.142760, time=0.412s\n",
      "  [Batch 185] loss=0.154336, time=0.200s\n",
      "  [Batch 186] loss=0.166913, time=0.289s\n",
      "  [Batch 187] loss=0.156681, time=0.173s\n",
      "  [Batch 188] loss=0.133989, time=0.232s\n",
      "  [Batch 189] loss=0.227354, time=0.379s\n",
      "  [Batch 190] loss=0.220441, time=0.391s\n",
      "  [Batch 191] loss=0.139576, time=0.266s\n",
      "  [Batch 192] loss=0.144986, time=0.413s\n",
      "  [Batch 193] loss=0.152861, time=0.186s\n",
      "  [Batch 194] loss=0.132427, time=0.268s\n",
      "  [Batch 195] loss=0.178272, time=0.256s\n",
      "  [Batch 196] loss=0.218089, time=0.342s\n",
      "  [Batch 197] loss=0.194583, time=0.373s\n",
      "  [Batch 198] loss=0.147788, time=0.387s\n",
      "  [Batch 199] loss=0.170354, time=0.388s\n",
      "  [Batch 200] loss=0.212418, time=0.277s\n",
      "  [Batch 201] loss=0.173754, time=0.423s\n",
      "  [Batch 202] loss=0.192951, time=0.325s\n",
      "  [Batch 203] loss=0.152454, time=0.326s\n",
      "  [Batch 204] loss=0.111763, time=0.261s\n",
      "  [Batch 205] loss=0.164401, time=0.221s\n",
      "  [Batch 206] loss=0.155982, time=0.282s\n",
      "  [Batch 207] loss=0.121874, time=0.376s\n",
      "  [Batch 208] loss=0.166157, time=0.248s\n",
      "  [Batch 209] loss=0.128293, time=0.450s\n",
      "  [Batch 210] loss=0.148325, time=0.188s\n",
      "  [Batch 211] loss=0.114457, time=0.331s\n",
      "  [Batch 212] loss=0.143096, time=0.185s\n",
      "  [Batch 213] loss=0.194732, time=0.215s\n",
      "  [Batch 214] loss=0.206818, time=0.213s\n",
      "  [Batch 215] loss=0.168715, time=0.203s\n",
      "  [Batch 216] loss=0.152699, time=0.415s\n",
      "  [Batch 217] loss=0.231382, time=0.272s\n",
      "  [Batch 218] loss=0.174681, time=0.304s\n",
      "  [Batch 219] loss=0.191513, time=0.183s\n",
      "  [Batch 220] loss=0.171503, time=0.302s\n",
      "  [Batch 221] loss=0.195314, time=0.331s\n",
      "  [Batch 222] loss=0.186266, time=0.293s\n",
      "  [Batch 223] loss=0.187111, time=0.194s\n",
      "  [Batch 224] loss=0.181979, time=0.399s\n",
      "  [Batch 225] loss=0.162740, time=0.195s\n",
      "  [Batch 226] loss=0.167401, time=0.262s\n",
      "  [Batch 227] loss=0.195068, time=0.407s\n",
      "  [Batch 228] loss=0.239280, time=0.275s\n",
      "  [Batch 229] loss=0.214813, time=0.277s\n",
      "  [Batch 230] loss=0.170349, time=0.227s\n",
      "  [Batch 231] loss=0.151462, time=0.191s\n",
      "  [Batch 232] loss=0.171005, time=0.203s\n",
      "  [Batch 233] loss=0.185334, time=0.218s\n",
      "  [Batch 234] loss=0.164191, time=0.246s\n",
      "  [Batch 235] loss=0.133413, time=0.213s\n",
      "  [Batch 236] loss=0.161129, time=0.290s\n",
      "  [Batch 237] loss=0.170496, time=0.392s\n",
      "  [Batch 238] loss=0.164842, time=0.386s\n",
      "  [Batch 239] loss=0.155894, time=0.304s\n",
      "  [Batch 240] loss=0.145858, time=0.337s\n",
      "  [Batch 241] loss=0.143305, time=0.432s\n",
      "  [Batch 242] loss=0.153563, time=0.292s\n",
      "  [Batch 243] loss=0.148674, time=0.358s\n",
      "  [Batch 244] loss=0.204396, time=0.274s\n",
      "  [Batch 245] loss=0.159155, time=0.298s\n",
      "  [Batch 246] loss=0.170122, time=0.425s\n",
      "  [Batch 247] loss=0.159574, time=0.181s\n",
      "  [Batch 248] loss=0.151336, time=0.432s\n",
      "  [Batch 249] loss=0.217975, time=0.290s\n",
      "  [Batch 250] loss=0.146831, time=0.200s\n",
      "  [Batch 251] loss=0.074551, time=0.062s\n",
      "\n",
      "[Validation] Loss: 0.343340\n",
      "  Size= 2 : MSE=0.000429 MAE=0.016177 MAPE=15.95%\n",
      "  Size= 4 : MSE=0.004698 MAE=0.044472 MAPE=14.59%\n",
      "  Size= 6 : MSE=0.007492 MAE=0.058491 MAPE=26.15%\n",
      "  Size= 8 : MSE=0.015954 MAE=0.088662 MAPE=24.18%\n",
      "  Size=10 : MSE=0.027858 MAE=0.117142 MAPE=342.72%\n",
      "  Size=12 : MSE=0.035069 MAE=0.130634 MAPE=29.18%\n",
      "[Info] New best model saved (val_loss=0.343340)\n",
      "\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amssa\\anaconda3\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Batch 000] loss=0.147665, time=0.709s\n",
      "  [Batch 001] loss=0.151410, time=0.276s\n",
      "  [Batch 002] loss=0.150261, time=0.258s\n",
      "  [Batch 003] loss=0.175071, time=0.504s\n",
      "  [Batch 004] loss=0.137552, time=0.357s\n",
      "  [Batch 005] loss=0.172080, time=0.473s\n",
      "  [Batch 006] loss=0.172553, time=0.473s\n",
      "  [Batch 007] loss=0.144244, time=0.444s\n",
      "  [Batch 008] loss=0.191412, time=0.420s\n",
      "  [Batch 009] loss=0.136857, time=0.323s\n",
      "  [Batch 010] loss=0.192510, time=0.337s\n",
      "  [Batch 011] loss=0.137436, time=0.485s\n",
      "  [Batch 012] loss=0.139768, time=0.365s\n",
      "  [Batch 013] loss=0.154926, time=0.363s\n",
      "  [Batch 014] loss=0.137817, time=0.444s\n",
      "  [Batch 015] loss=0.141568, time=0.274s\n",
      "  [Batch 016] loss=0.169491, time=0.404s\n",
      "  [Batch 017] loss=0.174049, time=0.319s\n",
      "  [Batch 018] loss=0.139160, time=0.289s\n",
      "  [Batch 019] loss=0.122464, time=0.227s\n",
      "  [Batch 020] loss=0.204966, time=0.364s\n",
      "  [Batch 021] loss=0.143853, time=0.313s\n",
      "  [Batch 022] loss=0.209343, time=0.427s\n",
      "  [Batch 023] loss=0.168545, time=0.385s\n",
      "  [Batch 024] loss=0.165314, time=0.257s\n",
      "  [Batch 025] loss=0.168892, time=0.263s\n",
      "  [Batch 026] loss=0.143006, time=0.453s\n",
      "  [Batch 027] loss=0.233324, time=0.457s\n",
      "  [Batch 028] loss=0.159748, time=0.364s\n",
      "  [Batch 029] loss=0.125986, time=0.446s\n",
      "  [Batch 030] loss=0.167615, time=0.414s\n",
      "  [Batch 031] loss=0.149744, time=0.349s\n",
      "  [Batch 032] loss=0.159895, time=0.346s\n",
      "  [Batch 033] loss=0.126125, time=0.325s\n",
      "  [Batch 034] loss=0.192649, time=0.249s\n",
      "  [Batch 035] loss=0.166419, time=0.246s\n",
      "  [Batch 036] loss=0.267384, time=0.206s\n",
      "  [Batch 037] loss=0.206474, time=0.456s\n",
      "  [Batch 038] loss=0.175943, time=0.389s\n",
      "  [Batch 039] loss=0.202382, time=0.391s\n",
      "  [Batch 040] loss=0.221049, time=0.340s\n",
      "  [Batch 041] loss=0.135495, time=0.200s\n",
      "  [Batch 042] loss=0.243856, time=0.418s\n",
      "  [Batch 043] loss=0.135973, time=0.380s\n",
      "  [Batch 044] loss=0.277301, time=0.256s\n",
      "  [Batch 045] loss=0.141034, time=0.370s\n",
      "  [Batch 046] loss=0.144137, time=0.404s\n",
      "  [Batch 047] loss=0.122661, time=0.208s\n",
      "  [Batch 048] loss=0.143558, time=0.235s\n",
      "  [Batch 049] loss=0.137062, time=0.364s\n",
      "  [Batch 050] loss=0.198875, time=0.435s\n",
      "  [Batch 051] loss=0.152204, time=0.286s\n",
      "  [Batch 052] loss=0.217304, time=0.307s\n",
      "  [Batch 053] loss=0.213886, time=0.423s\n",
      "  [Batch 054] loss=0.190556, time=0.401s\n",
      "  [Batch 055] loss=0.129192, time=0.189s\n",
      "  [Batch 056] loss=0.150618, time=0.235s\n",
      "  [Batch 057] loss=0.176265, time=0.185s\n",
      "  [Batch 058] loss=0.262567, time=0.417s\n",
      "  [Batch 059] loss=0.294337, time=0.428s\n",
      "  [Batch 060] loss=0.168370, time=0.339s\n",
      "  [Batch 061] loss=0.224450, time=0.298s\n",
      "  [Batch 062] loss=0.165412, time=0.310s\n",
      "  [Batch 063] loss=0.213066, time=0.347s\n",
      "  [Batch 064] loss=0.209319, time=0.191s\n",
      "  [Batch 065] loss=0.160794, time=0.226s\n",
      "  [Batch 066] loss=0.154028, time=0.199s\n",
      "  [Batch 067] loss=0.124599, time=0.215s\n",
      "  [Batch 068] loss=0.160713, time=0.215s\n",
      "  [Batch 069] loss=0.146321, time=0.431s\n",
      "  [Batch 070] loss=0.182972, time=0.436s\n",
      "  [Batch 071] loss=0.145644, time=0.416s\n",
      "  [Batch 072] loss=0.158107, time=0.389s\n",
      "  [Batch 073] loss=0.185288, time=0.302s\n",
      "  [Batch 074] loss=0.204777, time=0.408s\n",
      "  [Batch 075] loss=0.184282, time=0.410s\n",
      "  [Batch 076] loss=0.176760, time=0.372s\n",
      "  [Batch 077] loss=0.152170, time=0.394s\n",
      "  [Batch 078] loss=0.155997, time=0.229s\n",
      "  [Batch 079] loss=0.127817, time=0.197s\n",
      "  [Batch 080] loss=0.139680, time=0.217s\n",
      "  [Batch 081] loss=0.157093, time=0.340s\n",
      "  [Batch 082] loss=0.170682, time=0.299s\n",
      "  [Batch 083] loss=0.131621, time=0.302s\n",
      "  [Batch 084] loss=0.151040, time=0.447s\n",
      "  [Batch 085] loss=0.133408, time=0.252s\n",
      "  [Batch 086] loss=0.177587, time=0.848s\n",
      "  [Batch 087] loss=0.124127, time=0.557s\n",
      "  [Batch 088] loss=0.129724, time=0.415s\n",
      "  [Batch 089] loss=0.140587, time=0.484s\n",
      "  [Batch 090] loss=0.157724, time=0.613s\n",
      "  [Batch 091] loss=0.127448, time=0.611s\n",
      "  [Batch 092] loss=0.157947, time=0.614s\n",
      "  [Batch 093] loss=0.150440, time=0.556s\n",
      "  [Batch 094] loss=0.139864, time=0.301s\n",
      "  [Batch 095] loss=0.151179, time=0.505s\n",
      "  [Batch 096] loss=0.161964, time=0.435s\n",
      "  [Batch 097] loss=0.127936, time=0.247s\n",
      "  [Batch 098] loss=0.247077, time=0.298s\n",
      "  [Batch 099] loss=0.205589, time=0.457s\n",
      "  [Batch 100] loss=0.166885, time=0.498s\n",
      "  [Batch 101] loss=0.121739, time=0.249s\n",
      "  [Batch 102] loss=0.144506, time=0.434s\n",
      "  [Batch 103] loss=0.156429, time=0.372s\n",
      "  [Batch 104] loss=0.115244, time=0.400s\n",
      "  [Batch 105] loss=0.141469, time=0.367s\n",
      "  [Batch 106] loss=0.206387, time=0.278s\n",
      "  [Batch 107] loss=0.145913, time=0.311s\n",
      "  [Batch 108] loss=0.155399, time=0.320s\n",
      "  [Batch 109] loss=0.188480, time=0.310s\n",
      "  [Batch 110] loss=0.177726, time=0.404s\n",
      "  [Batch 111] loss=0.141231, time=0.434s\n",
      "  [Batch 112] loss=0.150492, time=0.247s\n",
      "  [Batch 113] loss=0.147150, time=0.448s\n",
      "  [Batch 114] loss=0.157309, time=0.392s\n",
      "  [Batch 115] loss=0.158759, time=0.477s\n",
      "  [Batch 116] loss=0.149353, time=0.384s\n",
      "  [Batch 117] loss=0.205341, time=0.256s\n",
      "  [Batch 118] loss=0.160027, time=0.341s\n",
      "  [Batch 119] loss=0.155189, time=0.259s\n",
      "  [Batch 120] loss=0.185070, time=0.350s\n",
      "  [Batch 121] loss=0.129834, time=0.451s\n",
      "  [Batch 122] loss=0.176538, time=0.440s\n",
      "  [Batch 123] loss=0.182060, time=0.330s\n",
      "  [Batch 124] loss=0.106418, time=0.380s\n",
      "  [Batch 125] loss=0.198717, time=0.360s\n",
      "  [Batch 126] loss=0.131369, time=0.443s\n",
      "  [Batch 127] loss=0.128203, time=0.453s\n",
      "  [Batch 128] loss=0.173126, time=0.322s\n",
      "  [Batch 129] loss=0.141551, time=0.313s\n",
      "  [Batch 130] loss=0.167562, time=0.228s\n",
      "  [Batch 131] loss=0.120740, time=0.442s\n",
      "  [Batch 132] loss=0.124220, time=0.350s\n",
      "  [Batch 133] loss=0.118486, time=0.316s\n",
      "  [Batch 134] loss=0.144699, time=0.359s\n",
      "  [Batch 135] loss=0.123316, time=0.200s\n",
      "  [Batch 136] loss=0.130954, time=0.443s\n",
      "  [Batch 137] loss=0.115863, time=0.377s\n",
      "  [Batch 138] loss=0.160152, time=0.424s\n",
      "  [Batch 139] loss=0.174186, time=0.439s\n",
      "  [Batch 140] loss=0.223966, time=0.455s\n",
      "  [Batch 141] loss=0.125583, time=0.297s\n",
      "  [Batch 142] loss=0.144067, time=0.418s\n",
      "  [Batch 143] loss=0.151692, time=0.269s\n",
      "  [Batch 144] loss=0.133844, time=0.191s\n",
      "  [Batch 145] loss=0.171971, time=0.319s\n",
      "  [Batch 146] loss=0.168918, time=0.382s\n",
      "  [Batch 147] loss=0.147527, time=0.452s\n",
      "  [Batch 148] loss=0.183580, time=0.241s\n",
      "  [Batch 149] loss=0.160484, time=0.363s\n",
      "  [Batch 150] loss=0.127341, time=0.308s\n",
      "  [Batch 151] loss=0.155880, time=0.315s\n",
      "  [Batch 152] loss=0.168114, time=0.299s\n",
      "  [Batch 153] loss=0.116132, time=0.329s\n",
      "  [Batch 154] loss=0.177003, time=0.256s\n",
      "  [Batch 155] loss=0.153836, time=0.269s\n",
      "  [Batch 156] loss=0.134260, time=0.440s\n",
      "  [Batch 157] loss=0.154865, time=0.392s\n",
      "  [Batch 158] loss=0.134204, time=0.392s\n",
      "  [Batch 159] loss=0.133101, time=0.433s\n",
      "  [Batch 160] loss=0.184253, time=0.310s\n",
      "  [Batch 161] loss=0.138100, time=0.266s\n",
      "  [Batch 162] loss=0.144015, time=0.448s\n",
      "  [Batch 163] loss=0.126365, time=0.442s\n",
      "  [Batch 164] loss=0.144529, time=0.235s\n",
      "  [Batch 165] loss=0.174297, time=0.389s\n",
      "  [Batch 166] loss=0.211109, time=0.259s\n",
      "  [Batch 167] loss=0.176446, time=0.410s\n",
      "  [Batch 168] loss=0.128905, time=0.204s\n",
      "  [Batch 169] loss=0.231618, time=0.202s\n",
      "  [Batch 170] loss=0.182289, time=0.196s\n",
      "  [Batch 171] loss=0.166862, time=0.247s\n",
      "  [Batch 172] loss=0.211564, time=0.293s\n",
      "  [Batch 173] loss=0.178616, time=0.324s\n",
      "  [Batch 174] loss=0.188010, time=0.303s\n",
      "  [Batch 175] loss=0.199967, time=0.337s\n",
      "  [Batch 176] loss=0.194984, time=0.435s\n",
      "  [Batch 177] loss=0.142535, time=0.371s\n",
      "  [Batch 178] loss=0.132130, time=0.209s\n",
      "  [Batch 179] loss=0.143416, time=0.441s\n",
      "  [Batch 180] loss=0.119142, time=0.421s\n",
      "  [Batch 181] loss=0.155640, time=0.421s\n",
      "  [Batch 182] loss=0.123464, time=0.313s\n",
      "  [Batch 183] loss=0.142999, time=0.183s\n",
      "  [Batch 184] loss=0.121863, time=0.299s\n",
      "  [Batch 185] loss=0.216656, time=0.395s\n",
      "  [Batch 186] loss=0.129896, time=0.234s\n",
      "  [Batch 187] loss=0.168437, time=0.379s\n",
      "  [Batch 188] loss=0.192748, time=0.204s\n",
      "  [Batch 189] loss=0.173435, time=0.428s\n",
      "  [Batch 190] loss=0.180178, time=0.315s\n",
      "  [Batch 191] loss=0.225140, time=0.263s\n",
      "  [Batch 192] loss=0.143433, time=0.192s\n",
      "  [Batch 193] loss=0.164648, time=0.437s\n",
      "  [Batch 194] loss=0.169153, time=0.204s\n",
      "  [Batch 195] loss=0.152249, time=0.332s\n",
      "  [Batch 196] loss=0.254629, time=0.424s\n",
      "  [Batch 197] loss=0.167151, time=0.203s\n",
      "  [Batch 198] loss=0.146708, time=0.455s\n",
      "  [Batch 199] loss=0.139081, time=0.244s\n",
      "  [Batch 200] loss=0.189694, time=0.405s\n",
      "  [Batch 201] loss=0.148780, time=0.428s\n",
      "  [Batch 202] loss=0.153922, time=0.197s\n",
      "  [Batch 203] loss=0.121670, time=0.308s\n",
      "  [Batch 204] loss=0.124690, time=0.409s\n",
      "  [Batch 205] loss=0.125758, time=0.384s\n",
      "  [Batch 206] loss=0.146965, time=0.379s\n",
      "  [Batch 207] loss=0.134171, time=0.427s\n",
      "  [Batch 208] loss=0.132216, time=0.213s\n",
      "  [Batch 209] loss=0.203152, time=0.210s\n",
      "  [Batch 210] loss=0.135859, time=0.397s\n",
      "  [Batch 211] loss=0.126358, time=0.262s\n",
      "  [Batch 212] loss=0.126756, time=0.467s\n",
      "  [Batch 213] loss=0.131007, time=0.199s\n",
      "  [Batch 214] loss=0.131738, time=0.202s\n",
      "  [Batch 215] loss=0.129000, time=0.253s\n",
      "  [Batch 216] loss=0.135239, time=0.369s\n",
      "  [Batch 217] loss=0.175958, time=0.323s\n",
      "  [Batch 218] loss=0.144543, time=0.390s\n",
      "  [Batch 219] loss=0.218228, time=0.289s\n",
      "  [Batch 220] loss=0.137067, time=0.208s\n",
      "  [Batch 221] loss=0.157818, time=0.431s\n",
      "  [Batch 222] loss=0.101966, time=0.249s\n",
      "  [Batch 223] loss=0.213638, time=0.297s\n",
      "  [Batch 224] loss=0.293043, time=0.286s\n",
      "  [Batch 225] loss=0.133959, time=0.455s\n",
      "  [Batch 226] loss=0.127520, time=0.389s\n",
      "  [Batch 227] loss=0.236411, time=0.221s\n",
      "  [Batch 228] loss=0.199723, time=0.378s\n",
      "  [Batch 229] loss=0.183897, time=0.340s\n",
      "  [Batch 230] loss=0.125699, time=0.434s\n",
      "  [Batch 231] loss=0.180687, time=0.450s\n",
      "  [Batch 232] loss=0.165380, time=0.326s\n",
      "  [Batch 233] loss=0.123854, time=0.390s\n",
      "  [Batch 234] loss=0.183272, time=0.259s\n",
      "  [Batch 235] loss=0.139253, time=0.195s\n",
      "  [Batch 236] loss=0.167509, time=0.423s\n",
      "  [Batch 237] loss=0.169957, time=0.438s\n",
      "  [Batch 238] loss=0.157036, time=0.214s\n",
      "  [Batch 239] loss=0.166391, time=0.241s\n",
      "  [Batch 240] loss=0.150764, time=0.412s\n",
      "  [Batch 241] loss=0.175396, time=0.233s\n",
      "  [Batch 242] loss=0.136031, time=0.308s\n",
      "  [Batch 243] loss=0.178098, time=0.324s\n",
      "  [Batch 244] loss=0.204709, time=0.319s\n",
      "  [Batch 245] loss=0.184418, time=0.292s\n",
      "  [Batch 246] loss=0.129205, time=0.199s\n",
      "  [Batch 247] loss=0.170305, time=0.383s\n",
      "  [Batch 248] loss=0.145002, time=0.419s\n",
      "  [Batch 249] loss=0.205827, time=0.376s\n",
      "  [Batch 250] loss=0.143738, time=0.204s\n",
      "  [Batch 251] loss=2.281254, time=0.050s\n",
      "\n",
      "[Validation] Loss: 0.331986\n",
      "  Size= 2 : MSE=0.001297 MAE=0.029362 MAPE=14.94%\n",
      "  Size= 4 : MSE=0.005387 MAE=0.051090 MAPE=14.78%\n",
      "  Size= 6 : MSE=0.006693 MAE=0.056300 MAPE=21.90%\n",
      "  Size= 8 : MSE=0.015450 MAE=0.087334 MAPE=21.29%\n",
      "  Size=10 : MSE=0.024921 MAE=0.110776 MAPE=92.21%\n",
      "  Size=12 : MSE=0.033532 MAE=0.128085 MAPE=24.83%\n",
      "[Info] New best model saved (val_loss=0.331986)\n",
      "\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amssa\\anaconda3\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Batch 000] loss=0.134367, time=0.769s\n",
      "  [Batch 001] loss=0.144960, time=0.419s\n",
      "  [Batch 002] loss=0.133900, time=0.401s\n",
      "  [Batch 003] loss=0.209837, time=0.438s\n",
      "  [Batch 004] loss=0.172093, time=0.447s\n",
      "  [Batch 005] loss=0.166316, time=0.366s\n",
      "  [Batch 006] loss=0.251494, time=0.379s\n",
      "  [Batch 007] loss=0.127275, time=0.444s\n",
      "  [Batch 008] loss=0.137079, time=0.328s\n",
      "  [Batch 009] loss=0.167733, time=0.359s\n",
      "  [Batch 010] loss=0.178246, time=0.405s\n",
      "  [Batch 011] loss=0.121714, time=0.255s\n",
      "  [Batch 012] loss=0.131296, time=0.456s\n",
      "  [Batch 013] loss=0.131064, time=0.254s\n",
      "  [Batch 014] loss=0.148200, time=0.449s\n",
      "  [Batch 015] loss=0.133569, time=0.206s\n",
      "  [Batch 016] loss=0.141378, time=0.231s\n",
      "  [Batch 017] loss=0.149433, time=0.307s\n",
      "  [Batch 018] loss=0.215824, time=0.331s\n",
      "  [Batch 019] loss=0.197950, time=0.438s\n",
      "  [Batch 020] loss=0.134449, time=0.384s\n",
      "  [Batch 021] loss=0.178260, time=0.381s\n",
      "  [Batch 022] loss=0.199816, time=0.241s\n",
      "  [Batch 023] loss=0.168748, time=0.463s\n",
      "  [Batch 024] loss=0.187846, time=0.371s\n",
      "  [Batch 025] loss=0.149308, time=0.476s\n",
      "  [Batch 026] loss=0.105265, time=0.350s\n",
      "  [Batch 027] loss=0.118924, time=0.352s\n",
      "  [Batch 028] loss=0.184533, time=0.341s\n",
      "  [Batch 029] loss=0.142462, time=0.317s\n",
      "  [Batch 030] loss=0.179343, time=0.321s\n",
      "  [Batch 031] loss=0.196907, time=0.447s\n",
      "  [Batch 032] loss=0.152917, time=0.405s\n",
      "  [Batch 033] loss=0.151986, time=0.388s\n",
      "  [Batch 034] loss=0.118497, time=0.434s\n",
      "  [Batch 035] loss=0.158914, time=0.236s\n",
      "  [Batch 036] loss=0.131571, time=0.196s\n",
      "  [Batch 037] loss=0.206940, time=0.427s\n",
      "  [Batch 038] loss=0.160900, time=0.321s\n",
      "  [Batch 039] loss=0.142834, time=0.205s\n",
      "  [Batch 040] loss=0.108034, time=0.315s\n",
      "  [Batch 041] loss=0.104785, time=0.223s\n",
      "  [Batch 042] loss=0.168763, time=0.281s\n",
      "  [Batch 043] loss=0.124820, time=0.198s\n",
      "  [Batch 044] loss=0.168666, time=0.296s\n",
      "  [Batch 045] loss=0.133492, time=0.341s\n",
      "  [Batch 046] loss=0.170115, time=0.285s\n",
      "  [Batch 047] loss=0.179666, time=0.314s\n",
      "  [Batch 048] loss=0.118734, time=0.442s\n",
      "  [Batch 049] loss=0.146137, time=0.385s\n",
      "  [Batch 050] loss=0.123815, time=0.369s\n",
      "  [Batch 051] loss=0.212213, time=0.351s\n",
      "  [Batch 052] loss=0.163442, time=0.280s\n",
      "  [Batch 053] loss=0.188178, time=0.332s\n",
      "  [Batch 054] loss=0.145227, time=0.288s\n",
      "  [Batch 055] loss=0.154197, time=0.213s\n",
      "  [Batch 056] loss=0.148688, time=0.230s\n",
      "  [Batch 057] loss=0.148533, time=0.169s\n",
      "  [Batch 058] loss=0.135482, time=0.293s\n",
      "  [Batch 059] loss=0.177069, time=0.346s\n",
      "  [Batch 060] loss=0.216256, time=0.275s\n",
      "  [Batch 061] loss=0.118268, time=0.378s\n",
      "  [Batch 062] loss=0.175709, time=0.488s\n",
      "  [Batch 063] loss=0.127656, time=0.401s\n",
      "  [Batch 064] loss=0.132254, time=0.324s\n",
      "  [Batch 065] loss=0.120492, time=0.266s\n",
      "  [Batch 066] loss=0.121147, time=0.244s\n",
      "  [Batch 067] loss=0.117913, time=0.305s\n",
      "  [Batch 068] loss=0.124174, time=0.421s\n",
      "  [Batch 069] loss=0.173424, time=0.282s\n",
      "  [Batch 070] loss=0.119999, time=0.306s\n",
      "  [Batch 071] loss=0.134758, time=0.433s\n",
      "  [Batch 072] loss=0.122693, time=0.259s\n",
      "  [Batch 073] loss=0.135791, time=0.402s\n",
      "  [Batch 074] loss=0.151752, time=0.194s\n",
      "  [Batch 075] loss=0.099445, time=0.262s\n",
      "  [Batch 076] loss=0.142715, time=0.279s\n",
      "  [Batch 077] loss=0.142555, time=0.352s\n",
      "  [Batch 078] loss=0.158559, time=0.446s\n",
      "  [Batch 079] loss=0.167936, time=0.488s\n",
      "  [Batch 080] loss=0.144232, time=0.399s\n",
      "  [Batch 081] loss=0.221624, time=0.438s\n",
      "  [Batch 082] loss=0.109814, time=0.342s\n",
      "  [Batch 083] loss=0.106697, time=0.411s\n",
      "  [Batch 084] loss=0.133527, time=0.440s\n",
      "  [Batch 085] loss=0.154475, time=0.455s\n",
      "  [Batch 086] loss=0.128169, time=0.439s\n",
      "  [Batch 087] loss=0.141674, time=0.392s\n",
      "  [Batch 088] loss=0.152482, time=0.309s\n",
      "  [Batch 089] loss=0.129187, time=0.201s\n",
      "  [Batch 090] loss=0.100497, time=0.322s\n",
      "  [Batch 091] loss=0.173545, time=0.397s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split, WeightedRandomSampler\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from torch_geometric.nn import GINEConv, GlobalAttention, Set2Set\n",
    "from torch_geometric.nn.norm import BatchNorm\n",
    "from torch_geometric.nn import TransformerConv\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# --------------------------------------------------------------------------------\n",
    "CONFIG = {\n",
    "    'processed_dir': './processed',\n",
    "    'batch_size': 32768,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-4,\n",
    "    'hidden_channels': 256,\n",
    "    'num_epochs': 500,           # shorter for demo\n",
    "    'patience': 50,\n",
    "    'random_seed': 42,\n",
    "    'best_model_path': 'best_gnn_model.pth',\n",
    "    'dropout_p': 0.4,\n",
    "    'scheduler_factor': 0.5,\n",
    "    'scheduler_patience': 10,\n",
    "    'grad_clip': 1.0,\n",
    "    'curriculum_steps': 5  # Number of curriculum stages\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Logging Setup\n",
    "# --------------------------------------------------------------------------------\n",
    "def setup_logging():\n",
    "    \"\"\"\n",
    "    Configure logging so that each message is immediately flushed.\n",
    "    This ensures real-time updates on cluster logs.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "        handlers=[logging.StreamHandler(sys.stdout)]\n",
    "    )\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Dataset\n",
    "# --------------------------------------------------------------------------------\n",
    "class SpinSystemDataset(InMemoryDataset):\n",
    "    def __init__(self, root='.', transform=None, pre_transform=None):\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Utilities\n",
    "# --------------------------------------------------------------------------------\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Custom Layers\n",
    "# --------------------------------------------------------------------------------\n",
    "class SizeAwareNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Node-level normalization layer that adapts to system size.\n",
    "    Assumes system_size has a shape matching the node dimension (num_nodes_in_batch).\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(hidden_channels)\n",
    "        self.size_scale = nn.Sequential(\n",
    "            nn.Linear(1, hidden_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_channels, hidden_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, size):\n",
    "        # size should be [num_nodes_in_batch] or [num_nodes_in_batch, 1]\n",
    "        if size.dim() == 1:\n",
    "            size = size.unsqueeze(-1)\n",
    "        size_factor = self.size_scale(size)  # [num_nodes_in_batch, hidden_channels]\n",
    "        return self.norm(x) * torch.sigmoid(size_factor)\n",
    "\n",
    "class RelativeFeatureTransform(nn.Module):\n",
    "    \"\"\"\n",
    "    Transforms set2set (or other global) embeddings so they become size-invariant.\n",
    "    Expects 'size' to have shape [batch_size] or [batch_size,1].\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.transform = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_channels, hidden_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, size):\n",
    "        if size.dim() == 2 and size.size(1) == 1:\n",
    "            size = size.squeeze(-1)\n",
    "        x_scaled = x / (size.unsqueeze(-1) + 1e-6)\n",
    "        return self.transform(x_scaled)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Model\n",
    "# --------------------------------------------------------------------------------\n",
    "class EnhancedPhysicsGNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A more advanced GNN model:\n",
    "      - Replaces or augments some GINEConv layers with TransformerConv.\n",
    "      - Uses a deeper stacking of message-passing blocks (with skip).\n",
    "      - Combines Set2Set and a global attention readout for extra expressiveness.\n",
    "      - Maintains size-aware normalization + relative transform logic.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 num_node_features,\n",
    "                 edge_attr_dim,\n",
    "                 hidden_channels=256,\n",
    "                 num_layers=8,\n",
    "                 dropout_p=0.4):\n",
    "        super().__init__()\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        # 1) Initial Node Embedding\n",
    "        self.init_transform = nn.Sequential(\n",
    "            nn.Linear(num_node_features, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "        # 2) Build Multiple Message Passing Blocks (alternate GINE and Transformer)\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            if i % 2 == 0:\n",
    "                # GINE-based MLP\n",
    "                mp_mlp = nn.Sequential(\n",
    "                    nn.Linear(hidden_channels, hidden_channels),\n",
    "                    nn.LayerNorm(hidden_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Dropout(dropout_p),\n",
    "                    nn.Linear(hidden_channels, hidden_channels)\n",
    "                )\n",
    "                conv = GINEConv(mp_mlp, edge_dim=edge_attr_dim)\n",
    "            else:\n",
    "                # Transformer-based conv\n",
    "                conv = TransformerConv(\n",
    "                    hidden_channels, hidden_channels // 4,\n",
    "                    heads=4,\n",
    "                    edge_dim=edge_attr_dim,\n",
    "                    dropout=dropout_p,\n",
    "                    beta=True\n",
    "                )\n",
    "            self.convs.append(conv)\n",
    "            self.norms.append(BatchNorm(hidden_channels))\n",
    "\n",
    "        # 3) Global Readouts\n",
    "        self.set2set_readout = Set2Set(hidden_channels, processing_steps=4)\n",
    "        self.gate_nn = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels // 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_channels // 2, 1)\n",
    "        )\n",
    "        self.global_attention = GlobalAttention(gate_nn=self.gate_nn)\n",
    "\n",
    "        # 4) Global Feature Transform (8 -> hidden_channels)\n",
    "        self.global_transform = nn.Sequential(\n",
    "            nn.Linear(8, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "        )\n",
    "\n",
    "        # 5) Final MLP\n",
    "        combined_in_dim = 2*hidden_channels + hidden_channels + hidden_channels  # 4*hidden_channels\n",
    "        self.final_mlp = nn.Sequential(\n",
    "            nn.Linear(combined_in_dim, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_channels, hidden_channels // 2),\n",
    "            nn.LayerNorm(hidden_channels // 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_channels // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        # Initial transform\n",
    "        h = self.init_transform(x)\n",
    "\n",
    "        # Message passing\n",
    "        for i in range(self.num_layers):\n",
    "            h_new = self.convs[i](h, edge_index, edge_attr)\n",
    "            h_new = self.norms[i](h_new)\n",
    "            h = h + h_new  # Residual\n",
    "\n",
    "        # 1) Set2Set\n",
    "        s2s = self.set2set_readout(h, batch)  # [batch_size, 2*hidden_channels]\n",
    "\n",
    "        # 2) Global Attention\n",
    "        ga = self.global_attention(h, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3) Global features\n",
    "        Omega = data.Omega.squeeze(-1)\n",
    "        Delta = data.Delta.squeeze(-1)\n",
    "        Energy = data.Energy.squeeze(-1)\n",
    "        total_rydberg = data.total_rydberg\n",
    "        system_size = data.system_size.squeeze(-1)\n",
    "        config_entropy = data.config_entropy.squeeze(-1)\n",
    "        rydberg_density = data.rydberg_density\n",
    "        relative_entropy = config_entropy / torch.log(system_size + 1e-6)\n",
    "\n",
    "        global_feats = torch.stack([\n",
    "            Omega, Delta,\n",
    "            Energy / system_size,\n",
    "            (total_rydberg / system_size),\n",
    "            rydberg_density,\n",
    "            system_size,\n",
    "            config_entropy,\n",
    "            relative_entropy,\n",
    "        ], dim=1)  # [batch_size, 8]\n",
    "\n",
    "        gf_out = self.global_transform(global_feats)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # Combine\n",
    "        combined = torch.cat([s2s, ga, gf_out], dim=-1)  # => [batch_size, 4*hidden_channels]\n",
    "        out = self.final_mlp(combined)                   # => [batch_size, 1]\n",
    "\n",
    "        return out.squeeze(-1)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Custom Loss\n",
    "# --------------------------------------------------------------------------------\n",
    "class PhysicalScaleAwareLoss(nn.Module):\n",
    "    def __init__(self, base_size=4, scaling_power=1.5, physics_weight=1.0):\n",
    "        super().__init__()\n",
    "        self.base_size = base_size\n",
    "        self.scaling_power = scaling_power\n",
    "        self.physics_weight = physics_weight\n",
    "        \n",
    "    def get_entropy_bounds(self, system_size, subsystem_size):\n",
    "        \"\"\"\n",
    "        Calculate physical bounds for von Neumann entropy:\n",
    "        0 ≤ S(ρₐ) ≤ min(nA, nB) * ln(2)\n",
    "        \"\"\"\n",
    "        # Lower bound is 0\n",
    "        lower_bound = torch.zeros_like(system_size, dtype=torch.float)\n",
    "        \n",
    "        # Upper bound from dimensionality\n",
    "        remaining_size = system_size - subsystem_size\n",
    "        min_size = torch.minimum(subsystem_size.float(), remaining_size.float())\n",
    "        upper_bound = min_size * torch.log(torch.tensor(2.0, device=system_size.device))\n",
    "        \n",
    "        return lower_bound, upper_bound\n",
    "        \n",
    "    def forward(self, pred, target, system_size, subsystem_size):\n",
    "        if system_size.dim() == 2:\n",
    "            system_size = system_size.squeeze(-1)\n",
    "        if subsystem_size.dim() == 2:\n",
    "            subsystem_size = subsystem_size.squeeze(-1)\n",
    "            \n",
    "        # predicted log-entropy -> exponentiate\n",
    "        pred_entropy = torch.exp(pred)\n",
    "        \n",
    "        # bounds\n",
    "        lower_bound, upper_bound = self.get_entropy_bounds(system_size, subsystem_size)\n",
    "        \n",
    "        # constraint: 0 <= S(A) <= min(nA, nB)*ln(2)\n",
    "        lower_violation = F.smooth_l1_loss(\n",
    "            torch.maximum(lower_bound, pred_entropy),\n",
    "            pred_entropy,\n",
    "            reduction='none'\n",
    "        )\n",
    "        upper_violation = F.smooth_l1_loss(\n",
    "            torch.minimum(upper_bound, pred_entropy),\n",
    "            pred_entropy,\n",
    "            reduction='none'\n",
    "        )\n",
    "        physics_loss = lower_violation + upper_violation\n",
    "        \n",
    "        # base loss on log-entropy\n",
    "        log_target = torch.log(target + 1e-10)\n",
    "        base_loss = F.mse_loss(pred, log_target, reduction='none')\n",
    "        \n",
    "        # scale weighting\n",
    "        size_weight = (system_size.float() / self.base_size) ** self.scaling_power\n",
    "        weighted_loss = base_loss * size_weight\n",
    "        \n",
    "        # combine\n",
    "        total_loss = weighted_loss + self.physics_weight * physics_loss\n",
    "        return total_loss.mean()\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Curriculum Sampler\n",
    "# --------------------------------------------------------------------------------\n",
    "def get_curriculum_sampler(dataset, epoch, max_epochs):\n",
    "    system_sizes = [data.system_size.item() for data in dataset]\n",
    "    max_size = max(system_sizes)\n",
    "    progress = min(1.0, epoch / (max_epochs * 0.7))\n",
    "    \n",
    "    weights = []\n",
    "    for size in system_sizes:\n",
    "        if size < 8:\n",
    "            weight = 1.0\n",
    "        else:\n",
    "            weight = progress * (1.0 - (size - 8) / (max_size - 8 + 1e-6))\n",
    "            weight = max(0.1, weight)\n",
    "        weights.append(weight)\n",
    "    \n",
    "    return WeightedRandomSampler(weights=weights, num_samples=len(dataset), replacement=True)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Training Loop\n",
    "# --------------------------------------------------------------------------------\n",
    "def train_epoch(model, loader, optimizer, criterion, device, clip_grad=None):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(data)\n",
    "        \n",
    "        # subsystem size from mask\n",
    "        subsystem_size = torch.tensor([\n",
    "            torch.sum(data.x[data.batch == i, 3]) \n",
    "            for i in range(data.num_graphs)\n",
    "        ], device=device)\n",
    "        \n",
    "        loss = criterion(out, data.y.squeeze(), data.system_size, subsystem_size)\n",
    "        loss.backward()\n",
    "        if clip_grad is not None:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "\n",
    "    # Return average loss over dataset\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device, name='Eval'):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    predictions, targets, sizes = [], [], []\n",
    "    \n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        \n",
    "        subsystem_size = torch.tensor([\n",
    "            torch.sum(data.x[data.batch == i, 3])\n",
    "            for i in range(data.num_graphs)\n",
    "        ], device=device)\n",
    "        \n",
    "        loss = criterion(out, data.y.squeeze(), data.system_size, subsystem_size)\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        \n",
    "        predictions.append(torch.exp(out).cpu())\n",
    "        targets.append(data.y.squeeze().cpu())\n",
    "        sizes.append(data.system_size.squeeze().cpu())\n",
    "    \n",
    "    predictions = torch.cat(predictions, dim=0).numpy()\n",
    "    targets = torch.cat(targets, dim=0).numpy()\n",
    "    sizes = torch.cat(sizes, dim=0).numpy()\n",
    "    \n",
    "    size_metrics = {}\n",
    "    for size_val in np.unique(sizes):\n",
    "        mask = (sizes == size_val)\n",
    "        size_preds = predictions[mask]\n",
    "        size_targets = targets[mask]\n",
    "        size_metrics[int(size_val)] = {\n",
    "            'mse': mean_squared_error(size_targets, size_preds),\n",
    "            'mae': mean_absolute_error(size_targets, size_preds),\n",
    "            'mape': np.mean(np.abs(size_preds - size_targets) / (size_targets + 1e-10)) * 100\n",
    "        }\n",
    "    \n",
    "    mean_loss = total_loss / len(loader.dataset) if len(loader.dataset) > 0 else 0.0\n",
    "\n",
    "    # Log metrics for this evaluation\n",
    "    logging.info(f\"[{name}] Loss: {mean_loss:.6f}\")\n",
    "    for sz, met in size_metrics.items():\n",
    "        logging.info(\n",
    "            f\"  Size={sz:2d} : \"\n",
    "            f\"MSE={met['mse']:.6f} \"\n",
    "            f\"MAE={met['mae']:.6f} \"\n",
    "            f\"MAPE={met['mape']:.2f}%\"\n",
    "        )\n",
    "    \n",
    "    return {\n",
    "        'loss': mean_loss,\n",
    "        'predictions': predictions,\n",
    "        'targets': targets,\n",
    "        'sizes': sizes,\n",
    "        'size_metrics': size_metrics\n",
    "    }\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Main\n",
    "# --------------------------------------------------------------------------------\n",
    "def main():\n",
    "    setup_logging()  # ensure logging is configured\n",
    "    set_seed(CONFIG['random_seed'])\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # 1) Load the dataset\n",
    "    dataset = SpinSystemDataset(root=CONFIG['processed_dir'])\n",
    "    \n",
    "    # 2) Separate data by system size\n",
    "    dataset_sizes = [dataset[i].system_size.item() for i in range(len(dataset))]\n",
    "    train_val_subset = [dataset[i] for i in range(len(dataset)) if dataset_sizes[i] <= 12]\n",
    "    test_size14_subset = [dataset[i] for i in range(len(dataset)) if abs(dataset_sizes[i] - 14) < 1e-6]\n",
    "    \n",
    "    # Convert subsets to list-based datasets\n",
    "    class ListDataset(InMemoryDataset):\n",
    "        def __init__(self, data_list):\n",
    "            self._data_list = data_list\n",
    "        def __len__(self):\n",
    "            return len(self._data_list)\n",
    "        def __getitem__(self, idx):\n",
    "            return self._data_list[idx]\n",
    "\n",
    "    train_val_dataset = ListDataset(train_val_subset)\n",
    "    size14_dataset = ListDataset(test_size14_subset)\n",
    "    \n",
    "    # 3) Train/val split\n",
    "    train_size = int(0.8 * len(train_val_dataset))\n",
    "    val_size = len(train_val_dataset) - train_size\n",
    "    \n",
    "    if train_size == 0:\n",
    "        logging.info(\"No data found with system_size <= 12. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    train_dataset, val_dataset = random_split(\n",
    "        train_val_dataset,\n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(CONFIG['random_seed'])\n",
    "    )\n",
    "    \n",
    "    # 4) Create model\n",
    "    sample_data = next(iter(DataLoader(train_dataset, batch_size=1)))\n",
    "    model = EnhancedPhysicsGNN(\n",
    "        num_node_features=sample_data.x.size(1),\n",
    "        edge_attr_dim=sample_data.edge_attr.size(1),\n",
    "        hidden_channels=CONFIG['hidden_channels'],\n",
    "        dropout_p=CONFIG['dropout_p']\n",
    "    ).to(device)\n",
    "    \n",
    "    # 5) Training components\n",
    "    criterion = PhysicalScaleAwareLoss(physics_weight=0.5) \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=CONFIG['learning_rate'],\n",
    "        weight_decay=CONFIG['weight_decay']\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=CONFIG['scheduler_factor'],\n",
    "        patience=CONFIG['scheduler_patience']\n",
    "    )\n",
    "    \n",
    "    # 6) Train/Val loaders\n",
    "    def create_dataloader(dataset_, epoch_, shuffle_=False):\n",
    "        sampler = None\n",
    "        if shuffle_:\n",
    "            sampler = get_curriculum_sampler(dataset_, epoch_, CONFIG['num_epochs'])\n",
    "        return DataLoader(dataset_, batch_size=CONFIG['batch_size'], sampler=sampler)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # 7) Main loop\n",
    "    for epoch in range(CONFIG['num_epochs']):\n",
    "        logging.info(f\"Epoch {epoch+1}/{CONFIG['num_epochs']}\")\n",
    "        \n",
    "        # Create loaders each epoch to incorporate curriculum (weighted) sampler\n",
    "        train_loader = create_dataloader(train_dataset, epoch, shuffle_=True)\n",
    "        val_loader = create_dataloader(val_dataset, epoch, shuffle_=False)\n",
    "        \n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device, clip_grad=CONFIG['grad_clip'])\n",
    "        logging.info(f\"  Training Loss: {train_loss:.6f}\")\n",
    "\n",
    "        val_metrics = evaluate(model, val_loader, criterion, device, name='Validation')\n",
    "        val_loss = val_metrics['loss']\n",
    "\n",
    "        # Scheduler step\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Check if best\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), CONFIG['best_model_path'])\n",
    "            logging.info(f\"  [Info] New best model saved (val_loss={best_val_loss:.6f})\")\n",
    "\n",
    "    # 8) Final evaluation on the size-14 subset\n",
    "    if len(size14_dataset) > 0:\n",
    "        size14_loader = DataLoader(size14_dataset, batch_size=CONFIG['batch_size'])\n",
    "        model.load_state_dict(torch.load(CONFIG['best_model_path'], map_location=device))\n",
    "        test_metrics_14 = evaluate(model, size14_loader, criterion, device, name='Size14-Test')\n",
    "    else:\n",
    "        logging.warning(\"No data found with system_size == 14. Skipping test.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53efef12-7a4c-44b6-b4e8-4e4b84dc501c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
