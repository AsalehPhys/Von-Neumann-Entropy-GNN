{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70e78569-48fb-417f-b521-d1effd0fd1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-28 01:09:41 [INFO] Epoch 1/500\n",
      "2025-01-28 01:15:29 [INFO]   Training Loss (excl. size16/18): 38.344450\n",
      "2025-01-28 01:16:09 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-28 01:16:09 [INFO] Mean Loss (excl. size 16,18): 0.395942\n",
      "2025-01-28 01:16:09 [INFO] Size-specific metrics:\n",
      "2025-01-28 01:16:09 [INFO]   Size 12: MSE=2.07e-01 MAE=3.23e-01 MAPE=61426418.8%\n",
      "2025-01-28 01:16:09 [INFO]   [Info] New best model saved (val_loss=0.395942)\n",
      "2025-01-28 01:16:09 [INFO] Epoch 2/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amssa\\anaconda3\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-28 01:19:05 [INFO]   Training Loss (excl. size16/18): 0.402175\n",
      "2025-01-28 01:19:27 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-28 01:19:27 [INFO] Mean Loss (excl. size 16,18): 0.425187\n",
      "2025-01-28 01:19:27 [INFO] Size-specific metrics:\n",
      "2025-01-28 01:19:27 [INFO]   Size 12: MSE=1.06e-01 MAE=2.25e-01 MAPE=382788.7%\n",
      "2025-01-28 01:19:27 [INFO] Epoch 3/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amssa\\anaconda3\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-28 01:23:55 [INFO]   Training Loss (excl. size16/18): 0.352178\n",
      "2025-01-28 01:24:27 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-28 01:24:27 [INFO] Mean Loss (excl. size 16,18): 0.421166\n",
      "2025-01-28 01:24:27 [INFO] Size-specific metrics:\n",
      "2025-01-28 01:24:27 [INFO]   Size 12: MSE=1.13e-01 MAE=2.36e-01 MAPE=3352.8%\n",
      "2025-01-28 01:24:27 [INFO] Epoch 4/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amssa\\anaconda3\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-28 01:30:06 [INFO]   Training Loss (excl. size16/18): 0.325889\n",
      "2025-01-28 01:31:02 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-28 01:31:02 [INFO] Mean Loss (excl. size 16,18): 0.369336\n",
      "2025-01-28 01:31:02 [INFO] Size-specific metrics:\n",
      "2025-01-28 01:31:02 [INFO]   Size 12: MSE=9.70e-02 MAE=2.25e-01 MAPE=14682.3%\n",
      "2025-01-28 01:31:02 [INFO]   [Info] New best model saved (val_loss=0.369336)\n",
      "2025-01-28 01:31:02 [INFO] Epoch 5/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amssa\\anaconda3\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-28 01:37:11 [INFO]   Training Loss (excl. size16/18): 0.312759\n",
      "2025-01-28 01:37:27 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-28 01:37:27 [INFO] Mean Loss (excl. size 16,18): 0.335456\n",
      "2025-01-28 01:37:27 [INFO] Size-specific metrics:\n",
      "2025-01-28 01:37:27 [INFO]   Size 12: MSE=9.82e-02 MAE=2.21e-01 MAPE=702257.6%\n",
      "2025-01-28 01:37:27 [INFO]   [Info] New best model saved (val_loss=0.335456)\n",
      "2025-01-28 01:37:27 [INFO] Epoch 6/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amssa\\anaconda3\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-28 01:39:31 [INFO]   Training Loss (excl. size16/18): 0.297871\n",
      "2025-01-28 01:39:47 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-28 01:39:47 [INFO] Mean Loss (excl. size 16,18): 0.292756\n",
      "2025-01-28 01:39:47 [INFO] Size-specific metrics:\n",
      "2025-01-28 01:39:47 [INFO]   Size 12: MSE=9.61e-02 MAE=2.21e-01 MAPE=37744.3%\n",
      "2025-01-28 01:39:48 [INFO]   [Info] New best model saved (val_loss=0.292756)\n",
      "2025-01-28 01:39:48 [INFO] Epoch 7/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amssa\\anaconda3\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-28 01:41:52 [INFO]   Training Loss (excl. size16/18): 0.290237\n",
      "2025-01-28 01:42:08 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-28 01:42:08 [INFO] Mean Loss (excl. size 16,18): 0.316166\n",
      "2025-01-28 01:42:08 [INFO] Size-specific metrics:\n",
      "2025-01-28 01:42:08 [INFO]   Size 12: MSE=1.05e-01 MAE=2.26e-01 MAPE=8395.8%\n",
      "2025-01-28 01:42:08 [INFO] Epoch 8/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amssa\\anaconda3\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 565\u001b[0m\n\u001b[0;32m    562\u001b[0m         logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data found with system_size in [16,18]. Skipping final test.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 565\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[18], line 534\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    531\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m create_dataloader(val_dataset, epoch, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    533\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m--> 534\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_epoch(\n\u001b[0;32m    535\u001b[0m     model, train_loader, optimizer, criterion,\n\u001b[0;32m    536\u001b[0m     device, clip_grad\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrad_clip\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    537\u001b[0m )\n\u001b[0;32m    538\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Training Loss (excl. size16/18): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    540\u001b[0m \u001b[38;5;66;03m# Validate\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 367\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, loader, optimizer, criterion, device, clip_grad)\u001b[0m\n\u001b[0;32m    364\u001b[0m         nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip_grad)\n\u001b[0;32m    365\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 367\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m mask\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(loader\u001b[38;5;241m.\u001b[39mdataset)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_geometric.nn import (\n",
    "    GINEConv,\n",
    "    TransformerConv,\n",
    "    Set2Set,\n",
    "    BatchNorm\n",
    ")\n",
    "from torch.utils.data import random_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Configuration\n",
    "# -----------------------------------------------------------\n",
    "CONFIG = {\n",
    "    'processed_dir': './processed_experimental12',\n",
    "    'processed_file_name': 'data.pt',\n",
    "    'batch_size': 1024,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-4,\n",
    "    'hidden_channels': 512,\n",
    "    'num_epochs': 100,\n",
    "    'patience': 75,\n",
    "    'random_seed': 42,\n",
    "    'best_model_path': 'best_model.pth',\n",
    "    'dropout_p': 0.4,\n",
    "    'scheduler_factor': 0.5,\n",
    "    'scheduler_patience': 10,\n",
    "    'grad_clip': 1.0,\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Logging & Utilities\n",
    "# -----------------------------------------------------------\n",
    "def setup_logging():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "        handlers=[logging.StreamHandler(sys.stdout)]\n",
    "    )\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# SpinSystemDataset\n",
    "# -----------------------------------------------------------\n",
    "class SpinSystemDataset(InMemoryDataset):\n",
    "    \"\"\"Loads the data.pt from processed_dir (old-style).\"\"\"\n",
    "    def __init__(self, root='.', transform=None, pre_transform=None):\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0], weights_only=False)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [CONFIG['processed_file_name']]\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        pass\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# PhysicalScaleAwareLoss (Single-Head)\n",
    "# -----------------------------------------------------------\n",
    "class PhysicalScaleAwareLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    1) Predicts log(S/N).\n",
    "    2) Applies physical bounding penalty for 0 <= S <= min(A, B) * log(2).\n",
    "    3) Uses size-weighted MSE on log(S/N).\n",
    "    \"\"\"\n",
    "    def __init__(self, base_size=4, scaling_power=1.5, physics_weight=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          base_size: Reference system size for weighting\n",
    "          scaling_power: Exponent for how strongly bigger systems are weighted\n",
    "          physics_weight: How heavily to penalize out-of-bounds predictions\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.base_size = base_size\n",
    "        self.scaling_power = scaling_power\n",
    "        self.physics_weight = physics_weight\n",
    "\n",
    "    def get_entropy_bounds(self, system_size, subsystem_size):\n",
    "        \"\"\"\n",
    "        Lower bound = 0\n",
    "        Upper bound = min(subsystem_size, system_size - subsystem_size) * ln(2)\n",
    "        \"\"\"\n",
    "        lower_bound = torch.zeros_like(system_size, dtype=torch.float)\n",
    "        min_size = torch.minimum(subsystem_size.float(), (system_size - subsystem_size).float())\n",
    "        upper_bound = min_size * torch.log(torch.tensor(2.0, device=system_size.device))\n",
    "        return lower_bound, upper_bound\n",
    "\n",
    "    def forward(self, pred_log_s_over_n, target_s, system_size, subsystem_size):\n",
    "        \"\"\"\n",
    "        pred_log_s_over_n: shape [batch]\n",
    "        target_s: shape [batch], the true absolute entropies\n",
    "        system_size, subsystem_size: shapes [batch]\n",
    "        \"\"\"\n",
    "        # Convert log(S/N) -> S_pred\n",
    "        pred_entropy = torch.exp(pred_log_s_over_n * system_size)\n",
    "\n",
    "        # Physical bounds\n",
    "        lower_bound, upper_bound = self.get_entropy_bounds(system_size, subsystem_size)\n",
    "\n",
    "        # Smooth L1 penalty if pred_entropy < 0 or > upper bound\n",
    "        lower_violation = F.smooth_l1_loss(\n",
    "            torch.maximum(lower_bound, pred_entropy),\n",
    "            pred_entropy,\n",
    "            reduction='none'\n",
    "        )\n",
    "        upper_violation = F.smooth_l1_loss(\n",
    "            torch.minimum(upper_bound, pred_entropy),\n",
    "            pred_entropy,\n",
    "            reduction='none'\n",
    "        )\n",
    "        physics_loss = lower_violation + upper_violation\n",
    "\n",
    "        # MSE on log(S/N)\n",
    "        # actual log(S/N) = log(target_s + small_eps) / system_size\n",
    "        log_target = torch.log(target_s + 1e-10) / system_size\n",
    "        base_loss = F.mse_loss(pred_log_s_over_n, log_target, reduction='none')\n",
    "\n",
    "        # System-size-based weighting\n",
    "        size_weight = (system_size.float() / self.base_size) ** self.scaling_power\n",
    "        weighted_loss = base_loss * size_weight\n",
    "\n",
    "        total_loss = weighted_loss + self.physics_weight * physics_loss\n",
    "        return total_loss.mean()\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# GNN Model (Single Head: log(S/N)) + Global Features\n",
    "# -----------------------------------------------------------\n",
    "class ExperimentalGNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_channels=64,\n",
    "        num_layers=10,\n",
    "        dropout_p=0.4\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        # Node feature indices for clarity\n",
    "        self.feature_indices = {\n",
    "            'position': slice(0, 2),           # x, y coordinates\n",
    "            'rydberg_val': 2,                  # Rydberg probability\n",
    "            'mask': 3,                         # Subsystem mask\n",
    "        }\n",
    "\n",
    "        # Node feature transformation\n",
    "        # Input: positions (2) + Rydberg probability (1) + mask (1) = 4 features\n",
    "        self.node_encoder = nn.Sequential(\n",
    "            nn.Linear(4, hidden_channels),  # 4 input features\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        # Edge feature transformation\n",
    "        # Input: two-point correlation (1) + distance (1) = 2 features\n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(2, hidden_channels // 2),  # 2 input features\n",
    "            nn.LayerNorm(hidden_channels // 2),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        # Stack message passing layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            if i % 2 == 0:\n",
    "                mp_mlp = nn.Sequential(\n",
    "                    nn.Linear(hidden_channels, hidden_channels),\n",
    "                    nn.LayerNorm(hidden_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Dropout(dropout_p),\n",
    "                    nn.Linear(hidden_channels, hidden_channels)\n",
    "                )\n",
    "                conv = GINEConv(mp_mlp, edge_dim=hidden_channels // 2)\n",
    "            else:\n",
    "                conv = TransformerConv(\n",
    "                    hidden_channels,\n",
    "                    hidden_channels // 4,\n",
    "                    heads=4,\n",
    "                    edge_dim=hidden_channels // 2,\n",
    "                    dropout=dropout_p,\n",
    "                    beta=True\n",
    "                )\n",
    "            self.convs.append(conv)\n",
    "            self.norms.append(BatchNorm(hidden_channels))\n",
    "\n",
    "        # Readout\n",
    "        self.readout = Set2Set(hidden_channels, processing_steps=4)\n",
    "\n",
    "        # Global features: nA/N, nB/N\n",
    "        self.global_mlp = nn.Sequential(\n",
    "            nn.Linear(2, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_channels, hidden_channels)\n",
    "        )\n",
    "\n",
    "        # Final MLP -> single output: log(S/N)\n",
    "        combined_dim = (2 * hidden_channels) + hidden_channels  # readout + global\n",
    "        self.final_mlp = nn.Sequential(\n",
    "            nn.Linear(combined_dim, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "\n",
    "            nn.Linear(hidden_channels, hidden_channels // 2),\n",
    "            nn.LayerNorm(hidden_channels // 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "\n",
    "            nn.Linear(hidden_channels // 2, 1)  # single head\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = (\n",
    "            data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        )\n",
    "\n",
    "        # Select only the desired node features\n",
    "        node_features = torch.cat([\n",
    "            x[:, self.feature_indices['position']],  # positions (x, y)\n",
    "            x[:, self.feature_indices['rydberg_val']].unsqueeze(-1),  # Rydberg probability\n",
    "            x[:, self.feature_indices['mask']].unsqueeze(-1)  # subsystem mask\n",
    "        ], dim=1)\n",
    "\n",
    "        # Select only the desired edge features\n",
    "        edge_features = torch.stack([\n",
    "            edge_attr[:, 1],  # two-point correlation\n",
    "            edge_attr[:, 2],  # distance\n",
    "        ], dim=1)\n",
    "\n",
    "        # Node & edge encodings\n",
    "        x_enc = self.node_encoder(node_features)\n",
    "        e_enc = self.edge_encoder(edge_features)\n",
    "\n",
    "        # Message Passing\n",
    "        h = x_enc\n",
    "        for i in range(self.num_layers):\n",
    "            h_new = self.convs[i](h, edge_index, e_enc)\n",
    "            h_new = self.norms[i](h_new)\n",
    "            h = h + h_new  # residual\n",
    "\n",
    "        # Readout -> shape [batch_size, 2 * hidden_channels]\n",
    "        h_readout = self.readout(h, batch)\n",
    "\n",
    "        # Global features: nA/N, nB/N\n",
    "        nA_over_N = data.nA.squeeze(-1) / (data.system_size.squeeze(-1) + 1e-10)\n",
    "        nB_over_N = data.nB.squeeze(-1) / (data.system_size.squeeze(-1) + 1e-10)\n",
    "        global_feats = torch.stack([nA_over_N, nB_over_N], dim=1)\n",
    "        gf_out = self.global_mlp(global_feats)\n",
    "\n",
    "        # Combine readout + global features\n",
    "        combined = torch.cat([h_readout, gf_out], dim=1)\n",
    "\n",
    "        # Single output: log(S/N)\n",
    "        out = self.final_mlp(combined).squeeze(-1)\n",
    "        return out\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Training & Evaluation\n",
    "# -----------------------------------------------------------\n",
    "def train_epoch(model, loader, optimizer, criterion, device, clip_grad=None):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Single-head output: log(S/N)\n",
    "        pred_log_s_over_n = model(data)\n",
    "        targets = data.y.squeeze()\n",
    "        system_size = data.system_size.squeeze(-1)\n",
    "        subsystem_size = data.nA.squeeze(-1)\n",
    "\n",
    "        loss = criterion(pred_log_s_over_n, targets, system_size, subsystem_size)\n",
    "        loss.backward()\n",
    "\n",
    "        if clip_grad is not None:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = data.num_graphs\n",
    "        n_samples += batch_size\n",
    "        total_loss += loss.item() * batch_size\n",
    "\n",
    "    return total_loss / n_samples if n_samples > 0 else 0.0\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device, name='Eval'):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    all_preds_abs = []\n",
    "    all_targets = []\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred_log_s_over_n = model(data)\n",
    "\n",
    "        targets = data.y.squeeze()\n",
    "        system_size = data.system_size.squeeze(-1)\n",
    "        subsystem_size = data.nA.squeeze(-1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(pred_log_s_over_n, targets, system_size, subsystem_size)\n",
    "        batch_size = data.num_graphs\n",
    "        total_loss += loss.item() * batch_size\n",
    "        n_samples += batch_size\n",
    "\n",
    "        # Convert predicted log(S/N) -> absolute S\n",
    "        pred_entropy_abs = torch.exp(pred_log_s_over_n * system_size)\n",
    "        all_preds_abs.append(pred_entropy_abs.cpu())\n",
    "        all_targets.append(targets.cpu())\n",
    "\n",
    "    mean_loss = total_loss / n_samples if n_samples > 0 else 0.0\n",
    "\n",
    "    # Metrics: MSE, MAE, MAPE for absolute S\n",
    "    all_preds_abs = torch.cat(all_preds_abs).numpy()\n",
    "    all_targets = torch.cat(all_targets).numpy()\n",
    "\n",
    "    mse_abs = mean_squared_error(all_targets, all_preds_abs)\n",
    "    mae_abs = mean_absolute_error(all_targets, all_preds_abs)\n",
    "    mape_abs = np.mean(np.abs((all_preds_abs - all_targets) / (all_targets + 1e-10))) * 100\n",
    "\n",
    "    logging.info(f\"\\n{name} Summary:\")\n",
    "    logging.info(f\"  Loss: {mean_loss:.6f}\")\n",
    "    logging.info(f\"  MSE (Absolute S): {mse_abs:.6f}\")\n",
    "    logging.info(f\"  MAE (Absolute S): {mae_abs:.6f}\")\n",
    "    logging.info(f\"  MAPE (Absolute S): {mape_abs:.2f}%\")\n",
    "\n",
    "    return {\n",
    "        'loss': mean_loss,\n",
    "        'mse_abs': mse_abs,\n",
    "        'mae_abs': mae_abs,\n",
    "        'mape_abs': mape_abs,\n",
    "        'predictions_abs': all_preds_abs,\n",
    "        'targets': all_targets\n",
    "    }\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Main\n",
    "# -----------------------------------------------------------\n",
    "def main():\n",
    "    setup_logging()\n",
    "    set_seed(CONFIG['random_seed'])\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = SpinSystemDataset(root=CONFIG['processed_dir'])\n",
    "    if len(dataset) == 0:\n",
    "        logging.error(\"Loaded dataset is empty. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Train/Val split\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        dataset,\n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(CONFIG['random_seed'])\n",
    "    )\n",
    "\n",
    "    # Example data to infer input dimensions\n",
    "    sample_data = next(iter(DataLoader(train_dataset, batch_size=1)))\n",
    "    # For node: (x=3 => positions + P_rydberg)\n",
    "    # For edges: (2 => correlation + distance)\n",
    "    model = ExperimentalGNN(\n",
    "        num_node_features=sample_data.x.size(1),\n",
    "        edge_attr_dim=sample_data.edge_attr.size(1),\n",
    "        hidden_channels=CONFIG['hidden_channels'],\n",
    "        num_layers=10,\n",
    "        dropout_p=CONFIG['dropout_p']\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = PhysicalScaleAwareLoss(physics_weight=1.0)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=CONFIG['learning_rate'],\n",
    "        weight_decay=CONFIG['weight_decay']\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=CONFIG['scheduler_factor'],\n",
    "        patience=CONFIG['scheduler_patience']\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(CONFIG['num_epochs']):\n",
    "        logging.info(f\"Epoch {epoch+1}/{CONFIG['num_epochs']}\")\n",
    "        train_loss = train_epoch(\n",
    "            model, train_loader, optimizer, criterion, device,\n",
    "            clip_grad=CONFIG['grad_clip']\n",
    "        )\n",
    "        logging.info(f\"  Training Loss: {train_loss:.6f}\")\n",
    "\n",
    "        val_metrics = evaluate(model, val_loader, criterion, device, name='Validation')\n",
    "        val_loss = val_metrics['loss']\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), CONFIG['best_model_path'])\n",
    "            logging.info(f\"  [Info] Best model saved (val_loss={best_val_loss:.6f})\")\n",
    "\n",
    "    logging.info(\"Training complete. Loading best model for final validation...\")\n",
    "    model.load_state_dict(torch.load(CONFIG['best_model_path'], map_location=device))\n",
    "    _ = evaluate(model, val_loader, criterion, device, name='Final Validation')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f4a32a-bd3a-4dcd-8664-913283180ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
