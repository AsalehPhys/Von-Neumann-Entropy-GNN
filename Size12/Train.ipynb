{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "70e78569-48fb-417f-b521-d1effd0fd1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-26 00:17:02 [INFO] Epoch 1/100\n",
      "2025-01-26 00:18:56 [INFO]   Training Loss: 3884401148.076604\n",
      "2025-01-26 00:19:12 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 00:19:12 [INFO]   Loss: 5.884071\n",
      "2025-01-26 00:19:12 [INFO]   MSE (Absolute Entropy): 0.387598\n",
      "2025-01-26 00:19:12 [INFO]   MAE (Absolute Entropy): 0.469329\n",
      "2025-01-26 00:19:12 [INFO]   MAPE (Absolute Entropy): 1042.18%\n",
      "2025-01-26 00:19:12 [INFO]   [Info] New best model saved (val_loss=5.884071)\n",
      "2025-01-26 00:19:12 [INFO] Epoch 2/100\n",
      "2025-01-26 00:20:33 [INFO]   Training Loss: 1171.676932\n",
      "2025-01-26 00:20:45 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 00:20:45 [INFO]   Loss: 4.377482\n",
      "2025-01-26 00:20:45 [INFO]   MSE (Absolute Entropy): 0.404510\n",
      "2025-01-26 00:20:45 [INFO]   MAE (Absolute Entropy): 0.483312\n",
      "2025-01-26 00:20:45 [INFO]   MAPE (Absolute Entropy): 95.07%\n",
      "2025-01-26 00:20:45 [INFO]   [Info] New best model saved (val_loss=4.377482)\n",
      "2025-01-26 00:20:45 [INFO] Epoch 3/100\n",
      "2025-01-26 00:22:01 [INFO]   Training Loss: 53.469386\n",
      "2025-01-26 00:22:09 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 00:22:09 [INFO]   Loss: 1.233798\n",
      "2025-01-26 00:22:09 [INFO]   MSE (Absolute Entropy): 0.268583\n",
      "2025-01-26 00:22:09 [INFO]   MAE (Absolute Entropy): 0.369775\n",
      "2025-01-26 00:22:09 [INFO]   MAPE (Absolute Entropy): 90.62%\n",
      "2025-01-26 00:22:09 [INFO]   [Info] New best model saved (val_loss=1.233798)\n",
      "2025-01-26 00:22:09 [INFO] Epoch 4/100\n",
      "2025-01-26 00:23:24 [INFO]   Training Loss: 62.575302\n",
      "2025-01-26 00:23:36 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 00:23:36 [INFO]   Loss: 0.935909\n",
      "2025-01-26 00:23:36 [INFO]   MSE (Absolute Entropy): 0.281947\n",
      "2025-01-26 00:23:36 [INFO]   MAE (Absolute Entropy): 0.377750\n",
      "2025-01-26 00:23:36 [INFO]   MAPE (Absolute Entropy): 130.97%\n",
      "2025-01-26 00:23:36 [INFO]   [Info] New best model saved (val_loss=0.935909)\n",
      "2025-01-26 00:23:36 [INFO] Epoch 5/100\n",
      "2025-01-26 00:24:51 [INFO]   Training Loss: 2.474812\n",
      "2025-01-26 00:24:59 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 00:24:59 [INFO]   Loss: 1.072498\n",
      "2025-01-26 00:24:59 [INFO]   MSE (Absolute Entropy): 0.269723\n",
      "2025-01-26 00:24:59 [INFO]   MAE (Absolute Entropy): 0.369227\n",
      "2025-01-26 00:24:59 [INFO]   MAPE (Absolute Entropy): 97.48%\n",
      "2025-01-26 00:24:59 [INFO] Epoch 6/100\n",
      "2025-01-26 00:26:13 [INFO]   Training Loss: 575.472961\n",
      "2025-01-26 00:26:25 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 00:26:25 [INFO]   Loss: 0.889360\n",
      "2025-01-26 00:26:25 [INFO]   MSE (Absolute Entropy): 0.285009\n",
      "2025-01-26 00:26:25 [INFO]   MAE (Absolute Entropy): 0.379057\n",
      "2025-01-26 00:26:25 [INFO]   MAPE (Absolute Entropy): 167.07%\n",
      "2025-01-26 00:26:25 [INFO]   [Info] New best model saved (val_loss=0.889360)\n",
      "2025-01-26 00:26:25 [INFO] Epoch 7/100\n",
      "2025-01-26 00:27:40 [INFO]   Training Loss: 49.193290\n",
      "2025-01-26 00:27:48 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 00:27:48 [INFO]   Loss: 0.862121\n",
      "2025-01-26 00:27:48 [INFO]   MSE (Absolute Entropy): 0.203378\n",
      "2025-01-26 00:27:48 [INFO]   MAE (Absolute Entropy): 0.312186\n",
      "2025-01-26 00:27:48 [INFO]   MAPE (Absolute Entropy): 184.08%\n",
      "2025-01-26 00:27:48 [INFO]   [Info] New best model saved (val_loss=0.862121)\n",
      "2025-01-26 00:27:48 [INFO] Epoch 8/100\n",
      "2025-01-26 00:29:02 [INFO]   Training Loss: 1093.149659\n",
      "2025-01-26 00:29:14 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 00:29:14 [INFO]   Loss: 0.894511\n",
      "2025-01-26 00:29:14 [INFO]   MSE (Absolute Entropy): 0.259259\n",
      "2025-01-26 00:29:14 [INFO]   MAE (Absolute Entropy): 0.359041\n",
      "2025-01-26 00:29:14 [INFO]   MAPE (Absolute Entropy): 129.13%\n",
      "2025-01-26 00:29:14 [INFO] Epoch 9/100\n",
      "2025-01-26 00:30:29 [INFO]   Training Loss: 14.760424\n",
      "2025-01-26 00:30:37 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 00:30:37 [INFO]   Loss: 0.891215\n",
      "2025-01-26 00:30:37 [INFO]   MSE (Absolute Entropy): 0.248706\n",
      "2025-01-26 00:30:37 [INFO]   MAE (Absolute Entropy): 0.348204\n",
      "2025-01-26 00:30:37 [INFO]   MAPE (Absolute Entropy): 128.02%\n",
      "2025-01-26 00:30:37 [INFO] Epoch 10/100\n",
      "2025-01-26 00:31:51 [INFO]   Training Loss: 259.250482\n",
      "2025-01-26 00:32:03 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 00:32:03 [INFO]   Loss: 0.936354\n",
      "2025-01-26 00:32:03 [INFO]   MSE (Absolute Entropy): 0.221589\n",
      "2025-01-26 00:32:03 [INFO]   MAE (Absolute Entropy): 0.332004\n",
      "2025-01-26 00:32:03 [INFO]   MAPE (Absolute Entropy): 117.07%\n",
      "2025-01-26 00:32:03 [INFO] Epoch 11/100\n",
      "2025-01-26 00:33:16 [INFO]   Training Loss: 19.889567\n",
      "2025-01-26 00:33:25 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 00:33:25 [INFO]   Loss: 0.859467\n",
      "2025-01-26 00:33:25 [INFO]   MSE (Absolute Entropy): 0.209709\n",
      "2025-01-26 00:33:25 [INFO]   MAE (Absolute Entropy): 0.320925\n",
      "2025-01-26 00:33:25 [INFO]   MAPE (Absolute Entropy): 285.57%\n",
      "2025-01-26 00:33:25 [INFO]   [Info] New best model saved (val_loss=0.859467)\n",
      "2025-01-26 00:33:25 [INFO] Epoch 12/100\n",
      "2025-01-26 00:34:39 [INFO]   Training Loss: 19.237130\n",
      "2025-01-26 00:34:50 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 00:34:50 [INFO]   Loss: 0.953700\n",
      "2025-01-26 00:34:50 [INFO]   MSE (Absolute Entropy): 0.210856\n",
      "2025-01-26 00:34:50 [INFO]   MAE (Absolute Entropy): 0.318006\n",
      "2025-01-26 00:34:50 [INFO]   MAPE (Absolute Entropy): 122.21%\n",
      "2025-01-26 00:34:50 [INFO] Epoch 13/100\n",
      "2025-01-26 00:36:05 [INFO]   Training Loss: 23.690864\n",
      "2025-01-26 00:36:13 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 00:36:13 [INFO]   Loss: 0.895324\n",
      "2025-01-26 00:36:13 [INFO]   MSE (Absolute Entropy): 0.231226\n",
      "2025-01-26 00:36:13 [INFO]   MAE (Absolute Entropy): 0.340972\n",
      "2025-01-26 00:36:13 [INFO]   MAPE (Absolute Entropy): 139.87%\n",
      "2025-01-26 00:36:13 [INFO] Epoch 14/100\n",
      "2025-01-26 00:37:27 [INFO]   Training Loss: 128.163451\n",
      "2025-01-26 00:37:39 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 00:37:39 [INFO]   Loss: 0.956184\n",
      "2025-01-26 00:37:39 [INFO]   MSE (Absolute Entropy): 0.229954\n",
      "2025-01-26 00:37:39 [INFO]   MAE (Absolute Entropy): 0.337172\n",
      "2025-01-26 00:37:39 [INFO]   MAPE (Absolute Entropy): 116.18%\n",
      "2025-01-26 00:37:39 [INFO] Epoch 15/100\n",
      "2025-01-26 00:38:53 [INFO]   Training Loss: 60.187370\n",
      "2025-01-26 00:39:02 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 00:39:02 [INFO]   Loss: 10305.094926\n",
      "2025-01-26 00:39:02 [INFO]   MSE (Absolute Entropy): 0.229750\n",
      "2025-01-26 00:39:02 [INFO]   MAE (Absolute Entropy): 0.334385\n",
      "2025-01-26 00:39:02 [INFO]   MAPE (Absolute Entropy): 10304743.75%\n",
      "2025-01-26 00:39:02 [INFO] Epoch 16/100\n",
      "2025-01-26 00:40:20 [INFO]   Training Loss: 242.810678\n",
      "2025-01-26 00:40:28 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 00:40:28 [INFO]   Loss: 0.888064\n",
      "2025-01-26 00:40:28 [INFO]   MSE (Absolute Entropy): 0.217613\n",
      "2025-01-26 00:40:28 [INFO]   MAE (Absolute Entropy): 0.326613\n",
      "2025-01-26 00:40:28 [INFO]   MAPE (Absolute Entropy): 124.11%\n",
      "2025-01-26 00:40:28 [INFO] Epoch 17/100\n",
      "2025-01-26 00:41:42 [INFO]   Training Loss: 118.777539\n",
      "2025-01-26 00:41:51 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 00:41:51 [INFO]   Loss: 1038.417128\n",
      "2025-01-26 00:41:51 [INFO]   MSE (Absolute Entropy): 0.208710\n",
      "2025-01-26 00:41:51 [INFO]   MAE (Absolute Entropy): 0.319853\n",
      "2025-01-26 00:41:51 [INFO]   MAPE (Absolute Entropy): 1037823.24%\n",
      "2025-01-26 00:41:51 [INFO] Epoch 18/100\n",
      "2025-01-26 00:43:09 [INFO]   Training Loss: 282.500031\n",
      "2025-01-26 00:43:17 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 00:43:17 [INFO]   Loss: 0.859157\n",
      "2025-01-26 00:43:17 [INFO]   MSE (Absolute Entropy): 0.193290\n",
      "2025-01-26 00:43:17 [INFO]   MAE (Absolute Entropy): 0.303202\n",
      "2025-01-26 00:43:17 [INFO]   MAPE (Absolute Entropy): 219.22%\n",
      "2025-01-26 00:43:17 [INFO]   [Info] New best model saved (val_loss=0.859157)\n",
      "2025-01-26 00:43:17 [INFO] Epoch 19/100\n",
      "2025-01-26 00:44:32 [INFO]   Training Loss: 31.929915\n",
      "2025-01-26 00:44:40 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 00:44:40 [INFO]   Loss: 0.836946\n",
      "2025-01-26 00:44:40 [INFO]   MSE (Absolute Entropy): 0.174903\n",
      "2025-01-26 00:44:40 [INFO]   MAE (Absolute Entropy): 0.289419\n",
      "2025-01-26 00:44:40 [INFO]   MAPE (Absolute Entropy): 155.18%\n",
      "2025-01-26 00:44:40 [INFO]   [Info] New best model saved (val_loss=0.836946)\n",
      "2025-01-26 00:44:40 [INFO] Epoch 20/100\n",
      "2025-01-26 00:45:59 [INFO]   Training Loss: 227.291645\n",
      "2025-01-26 00:46:07 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 00:46:07 [INFO]   Loss: 0.808716\n",
      "2025-01-26 00:46:07 [INFO]   MSE (Absolute Entropy): 0.216233\n",
      "2025-01-26 00:46:07 [INFO]   MAE (Absolute Entropy): 0.325214\n",
      "2025-01-26 00:46:07 [INFO]   MAPE (Absolute Entropy): 157.27%\n",
      "2025-01-26 00:46:07 [INFO]   [Info] New best model saved (val_loss=0.808716)\n",
      "2025-01-26 00:46:07 [INFO] Epoch 21/100\n",
      "2025-01-26 00:47:21 [INFO]   Training Loss: 8.979267\n",
      "2025-01-26 00:47:29 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 00:47:29 [INFO]   Loss: 24691.168585\n",
      "2025-01-26 00:47:29 [INFO]   MSE (Absolute Entropy): 0.134533\n",
      "2025-01-26 00:47:29 [INFO]   MAE (Absolute Entropy): 0.257852\n",
      "2025-01-26 00:47:29 [INFO]   MAPE (Absolute Entropy): 24690746.88%\n",
      "2025-01-26 00:47:29 [INFO] Epoch 22/100\n",
      "2025-01-26 00:48:47 [INFO]   Training Loss: 5.832533\n",
      "2025-01-26 00:48:55 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 00:48:55 [INFO]   Loss: 67.234770\n",
      "2025-01-26 00:48:55 [INFO]   MSE (Absolute Entropy): 0.165457\n",
      "2025-01-26 00:48:55 [INFO]   MAE (Absolute Entropy): 0.280081\n",
      "2025-01-26 00:48:55 [INFO]   MAPE (Absolute Entropy): 66643.24%\n",
      "2025-01-26 00:48:55 [INFO] Epoch 23/100\n",
      "2025-01-26 00:50:10 [INFO]   Training Loss: 8.871251\n",
      "2025-01-26 00:50:18 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 00:50:18 [INFO]   Loss: 2.456802\n",
      "2025-01-26 00:50:18 [INFO]   MSE (Absolute Entropy): 0.145387\n",
      "2025-01-26 00:50:18 [INFO]   MAE (Absolute Entropy): 0.268142\n",
      "2025-01-26 00:50:18 [INFO]   MAPE (Absolute Entropy): 1933.72%\n",
      "2025-01-26 00:50:18 [INFO] Epoch 24/100\n",
      "2025-01-26 00:51:37 [INFO]   Training Loss: 6.109937\n",
      "2025-01-26 00:51:46 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 00:51:46 [INFO]   Loss: 0.802030\n",
      "2025-01-26 00:51:46 [INFO]   MSE (Absolute Entropy): 0.169772\n",
      "2025-01-26 00:51:46 [INFO]   MAE (Absolute Entropy): 0.286746\n",
      "2025-01-26 00:51:46 [INFO]   MAPE (Absolute Entropy): 158.72%\n",
      "2025-01-26 00:51:46 [INFO]   [Info] New best model saved (val_loss=0.802030)\n",
      "2025-01-26 00:51:46 [INFO] Epoch 25/100\n",
      "2025-01-26 00:53:01 [INFO]   Training Loss: 68.134127\n",
      "2025-01-26 00:53:09 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 00:53:09 [INFO]   Loss: 0.890532\n",
      "2025-01-26 00:53:09 [INFO]   MSE (Absolute Entropy): 0.154046\n",
      "2025-01-26 00:53:09 [INFO]   MAE (Absolute Entropy): 0.274500\n",
      "2025-01-26 00:53:09 [INFO]   MAPE (Absolute Entropy): 123.19%\n",
      "2025-01-26 00:53:09 [INFO] Epoch 26/100\n",
      "2025-01-26 00:54:28 [INFO]   Training Loss: 43.046005\n",
      "2025-01-26 00:54:36 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 00:54:36 [INFO]   Loss: 2.016926\n",
      "2025-01-26 00:54:36 [INFO]   MSE (Absolute Entropy): 0.126840\n",
      "2025-01-26 00:54:36 [INFO]   MAE (Absolute Entropy): 0.248526\n",
      "2025-01-26 00:54:36 [INFO]   MAPE (Absolute Entropy): 1519.51%\n",
      "2025-01-26 00:54:36 [INFO] Epoch 27/100\n",
      "2025-01-26 00:55:52 [INFO]   Training Loss: 28.253926\n",
      "2025-01-26 00:56:00 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 00:56:00 [INFO]   Loss: 3628.214813\n",
      "2025-01-26 00:56:00 [INFO]   MSE (Absolute Entropy): 0.192762\n",
      "2025-01-26 00:56:00 [INFO]   MAE (Absolute Entropy): 0.305132\n",
      "2025-01-26 00:56:00 [INFO]   MAPE (Absolute Entropy): 3627698.44%\n",
      "2025-01-26 00:56:00 [INFO] Epoch 28/100\n",
      "2025-01-26 00:57:19 [INFO]   Training Loss: 4.166269\n",
      "2025-01-26 00:57:28 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 00:57:28 [INFO]   Loss: 129.912795\n",
      "2025-01-26 00:57:28 [INFO]   MSE (Absolute Entropy): 0.125772\n",
      "2025-01-26 00:57:28 [INFO]   MAE (Absolute Entropy): 0.249896\n",
      "2025-01-26 00:57:28 [INFO]   MAPE (Absolute Entropy): 129403.94%\n",
      "2025-01-26 00:57:28 [INFO] Epoch 29/100\n",
      "2025-01-26 00:58:43 [INFO]   Training Loss: 54.251872\n",
      "2025-01-26 00:58:51 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 00:58:51 [INFO]   Loss: 0.818484\n",
      "2025-01-26 00:58:51 [INFO]   MSE (Absolute Entropy): 0.131148\n",
      "2025-01-26 00:58:51 [INFO]   MAE (Absolute Entropy): 0.252825\n",
      "2025-01-26 00:58:51 [INFO]   MAPE (Absolute Entropy): 281.59%\n",
      "2025-01-26 00:58:51 [INFO] Epoch 30/100\n",
      "2025-01-26 01:00:11 [INFO]   Training Loss: 8.678315\n",
      "2025-01-26 01:00:19 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 01:00:19 [INFO]   Loss: 0.958557\n",
      "2025-01-26 01:00:19 [INFO]   MSE (Absolute Entropy): 0.230716\n",
      "2025-01-26 01:00:19 [INFO]   MAE (Absolute Entropy): 0.338883\n",
      "2025-01-26 01:00:19 [INFO]   MAPE (Absolute Entropy): 96.39%\n",
      "2025-01-26 01:00:19 [INFO] Epoch 31/100\n",
      "2025-01-26 01:01:35 [INFO]   Training Loss: 128.238465\n",
      "2025-01-26 01:01:44 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 01:01:44 [INFO]   Loss: 0.886589\n",
      "2025-01-26 01:01:44 [INFO]   MSE (Absolute Entropy): 0.206598\n",
      "2025-01-26 01:01:44 [INFO]   MAE (Absolute Entropy): 0.315946\n",
      "2025-01-26 01:01:44 [INFO]   MAPE (Absolute Entropy): 105.21%\n",
      "2025-01-26 01:01:44 [INFO] Epoch 32/100\n",
      "2025-01-26 01:03:02 [INFO]   Training Loss: 61.213810\n",
      "2025-01-26 01:03:11 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 01:03:11 [INFO]   Loss: 0.825205\n",
      "2025-01-26 01:03:11 [INFO]   MSE (Absolute Entropy): 0.174394\n",
      "2025-01-26 01:03:11 [INFO]   MAE (Absolute Entropy): 0.288931\n",
      "2025-01-26 01:03:11 [INFO]   MAPE (Absolute Entropy): 124.07%\n",
      "2025-01-26 01:03:11 [INFO] Epoch 33/100\n",
      "2025-01-26 01:04:26 [INFO]   Training Loss: 21.934216\n",
      "2025-01-26 01:04:35 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 01:04:35 [INFO]   Loss: 4962.900390\n",
      "2025-01-26 01:04:35 [INFO]   MSE (Absolute Entropy): 0.121363\n",
      "2025-01-26 01:04:35 [INFO]   MAE (Absolute Entropy): 0.243205\n",
      "2025-01-26 01:04:35 [INFO]   MAPE (Absolute Entropy): 4962372.27%\n",
      "2025-01-26 01:04:35 [INFO] Epoch 34/100\n",
      "2025-01-26 01:05:53 [INFO]   Training Loss: 19.912475\n",
      "2025-01-26 01:06:01 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 01:06:01 [INFO]   Loss: 0.754597\n",
      "2025-01-26 01:06:01 [INFO]   MSE (Absolute Entropy): 0.138200\n",
      "2025-01-26 01:06:01 [INFO]   MAE (Absolute Entropy): 0.264025\n",
      "2025-01-26 01:06:01 [INFO]   MAPE (Absolute Entropy): 137.27%\n",
      "2025-01-26 01:06:01 [INFO]   [Info] New best model saved (val_loss=0.754597)\n",
      "2025-01-26 01:06:01 [INFO] Epoch 35/100\n",
      "2025-01-26 01:07:16 [INFO]   Training Loss: 90.087070\n",
      "2025-01-26 01:07:24 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 01:07:24 [INFO]   Loss: 6822.702006\n",
      "2025-01-26 01:07:24 [INFO]   MSE (Absolute Entropy): 0.141203\n",
      "2025-01-26 01:07:24 [INFO]   MAE (Absolute Entropy): 0.259496\n",
      "2025-01-26 01:07:24 [INFO]   MAPE (Absolute Entropy): 6822306.25%\n",
      "2025-01-26 01:07:24 [INFO] Epoch 36/100\n",
      "2025-01-26 01:08:44 [INFO]   Training Loss: 93.512758\n",
      "2025-01-26 01:08:52 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 01:08:52 [INFO]   Loss: 0.861194\n",
      "2025-01-26 01:08:52 [INFO]   MSE (Absolute Entropy): 0.138576\n",
      "2025-01-26 01:08:52 [INFO]   MAE (Absolute Entropy): 0.260950\n",
      "2025-01-26 01:08:52 [INFO]   MAPE (Absolute Entropy): 122.36%\n",
      "2025-01-26 01:08:52 [INFO] Epoch 37/100\n",
      "2025-01-26 01:10:07 [INFO]   Training Loss: 324.242259\n",
      "2025-01-26 01:10:15 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 01:10:15 [INFO]   Loss: 0.704992\n",
      "2025-01-26 01:10:15 [INFO]   MSE (Absolute Entropy): 0.142426\n",
      "2025-01-26 01:10:15 [INFO]   MAE (Absolute Entropy): 0.267721\n",
      "2025-01-26 01:10:15 [INFO]   MAPE (Absolute Entropy): 156.72%\n",
      "2025-01-26 01:10:15 [INFO]   [Info] New best model saved (val_loss=0.704992)\n",
      "2025-01-26 01:10:15 [INFO] Epoch 38/100\n",
      "2025-01-26 01:11:34 [INFO]   Training Loss: 9.464276\n",
      "2025-01-26 01:11:42 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 01:11:42 [INFO]   Loss: 9.105578\n",
      "2025-01-26 01:11:42 [INFO]   MSE (Absolute Entropy): 0.105766\n",
      "2025-01-26 01:11:42 [INFO]   MAE (Absolute Entropy): 0.226702\n",
      "2025-01-26 01:11:42 [INFO]   MAPE (Absolute Entropy): 8737.15%\n",
      "2025-01-26 01:11:42 [INFO] Epoch 39/100\n",
      "2025-01-26 01:13:01 [INFO]   Training Loss: 16.549403\n",
      "2025-01-26 01:13:09 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 01:13:09 [INFO]   Loss: 344259.578100\n",
      "2025-01-26 01:13:09 [INFO]   MSE (Absolute Entropy): 0.132511\n",
      "2025-01-26 01:13:09 [INFO]   MAE (Absolute Entropy): 0.255005\n",
      "2025-01-26 01:13:09 [INFO]   MAPE (Absolute Entropy): 344259000.00%\n",
      "2025-01-26 01:13:09 [INFO] Epoch 40/100\n",
      "2025-01-26 01:14:33 [INFO]   Training Loss: 24.132828\n",
      "2025-01-26 01:14:43 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 01:14:43 [INFO]   Loss: 0.628341\n",
      "2025-01-26 01:14:43 [INFO]   MSE (Absolute Entropy): 0.122097\n",
      "2025-01-26 01:14:43 [INFO]   MAE (Absolute Entropy): 0.240762\n",
      "2025-01-26 01:14:43 [INFO]   MAPE (Absolute Entropy): 191.37%\n",
      "2025-01-26 01:14:43 [INFO]   [Info] New best model saved (val_loss=0.628341)\n",
      "2025-01-26 01:14:43 [INFO] Epoch 41/100\n",
      "2025-01-26 01:16:02 [INFO]   Training Loss: 33.625409\n",
      "2025-01-26 01:16:11 [INFO] \n",
      "Validation Summary:\n",
      "2025-01-26 01:16:11 [INFO]   Loss: 0.666801\n",
      "2025-01-26 01:16:11 [INFO]   MSE (Absolute Entropy): 0.141401\n",
      "2025-01-26 01:16:11 [INFO]   MAE (Absolute Entropy): 0.259252\n",
      "2025-01-26 01:16:11 [INFO]   MAPE (Absolute Entropy): 161.08%\n",
      "2025-01-26 01:16:11 [INFO] Epoch 42/100\n",
      "2025-01-26 01:18:28 [INFO]   Training Loss: 6.460753\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 574\u001b[0m\n\u001b[0;32m    571\u001b[0m     _ \u001b[38;5;241m=\u001b[39m evaluate(model, create_dataloader(val_dataset, epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), criterion, device, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinal Validation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 574\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[77], line 559\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    556\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    558\u001b[0m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[1;32m--> 559\u001b[0m val_metrics \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion, device, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    560\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m val_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    561\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep(val_loss)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[77], line 438\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, loader, criterion, device, name)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m    437\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 438\u001b[0m     preds \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[0;32m    440\u001b[0m     subsystem_size \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mnA\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    441\u001b[0m     system_size_ \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msystem_size\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[77], line 259\u001b[0m, in \u001b[0;36mExperimentalGNN.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    256\u001b[0m     h \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m+\u001b[39m h_new\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Readouts\u001b[39;00m\n\u001b[1;32m--> 259\u001b[0m s2s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset2set_readout(h, batch)\n\u001b[0;32m    260\u001b[0m ga \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_attention(h, batch)\n\u001b[0;32m    262\u001b[0m system_size \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msystem_size\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch_geometric\\experimental.py:117\u001b[0m, in \u001b[0;36mdisable_dynamic_shapes.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_experimental_mode_enabled(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisable_dynamic_shapes\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m required_arg \u001b[38;5;129;01min\u001b[39;00m required_args:\n\u001b[0;32m    120\u001b[0m         index \u001b[38;5;241m=\u001b[39m required_args_pos[required_arg]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch_geometric\\nn\\aggr\\base.py:128\u001b[0m, in \u001b[0;36mAggregation.__call__\u001b[1;34m(self, x, index, ptr, dim_size, dim, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered invalid \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdim_size\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m                          \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but expected \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    125\u001b[0m                          \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mptr\u001b[38;5;241m.\u001b[39mnumel()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 128\u001b[0m     dim_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(index\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(x, index\u001b[38;5;241m=\u001b[39mindex, ptr\u001b[38;5;241m=\u001b[39mptr, dim_size\u001b[38;5;241m=\u001b[39mdim_size,\n\u001b[0;32m    132\u001b[0m                             dim\u001b[38;5;241m=\u001b[39mdim, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_geometric.nn import AttentionalAggregation\n",
    "from torch.utils.data import random_split, WeightedRandomSampler\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from torch_geometric.nn import (\n",
    "    GINEConv, \n",
    "    Set2Set, \n",
    "    TransformerConv\n",
    ")\n",
    "from torch_geometric.nn.norm import BatchNorm\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Configuration\n",
    "# -----------------------------------------------------------\n",
    "CONFIG = {\n",
    "    'processed_dir': './processed_experimental12',  \n",
    "    'processed_file_name': 'data.pt',\n",
    "    'batch_size': 1024,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-4,\n",
    "    'hidden_channels': 512,\n",
    "    'num_epochs': 100,\n",
    "    'patience': 75,\n",
    "    'random_seed': 42,\n",
    "    'best_model_path': 'best_model.pth',\n",
    "    'dropout_p': 0.4,\n",
    "    'scheduler_factor': 0.5,\n",
    "    'scheduler_patience': 10,\n",
    "    'grad_clip': 1.0,\n",
    "    'curriculum_alpha': 0.5,\n",
    "    'num_layers': 10\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Logging & Utilities\n",
    "# -----------------------------------------------------------\n",
    "def setup_logging():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "        handlers=[logging.StreamHandler(sys.stdout)]\n",
    "    )\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# SpinSystemDataset\n",
    "# -----------------------------------------------------------\n",
    "class SpinSystemDataset(InMemoryDataset):\n",
    "    \"\"\"Loads the data.pt from processed_dir (old-style).\"\"\"\n",
    "    def __init__(self, root='.', transform=None, pre_transform=None):\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0], weights_only=False)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [CONFIG['processed_file_name']]\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        pass\n",
    "\n",
    "class ListDataset(InMemoryDataset):\n",
    "    \"\"\"Simple dataset class that wraps a list of data objects.\"\"\"\n",
    "    def __init__(self, data_list):\n",
    "        super().__init__()\n",
    "        self._data_list = data_list\n",
    "        \n",
    "    def __len__(self):  \n",
    "        return len(self._data_list)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        return self._data_list[idx]\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# ExperimentalGNN\n",
    "# -----------------------------------------------------------\n",
    "class ExperimentalGNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_node_features,      # Total number of node features\n",
    "        edge_attr_dim=3,        # [angle, correlation, normalized_distance]\n",
    "        hidden_channels=64,\n",
    "        num_layers=4,\n",
    "        dropout_p=0.4\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        # Node feature indices (for clarity)\n",
    "        self.feature_indices = {\n",
    "            'position': slice(0, 2),           # x, y coordinates\n",
    "            'rydberg_val': slice(2, 3),        # Site occupation/correlation value\n",
    "            'mask': slice(3, 4),               # Subsystem mask\n",
    "            'boundaries': slice(4, 8),         # Distances to boundaries\n",
    "            'radial': slice(8, 9),             # Radial distance from center\n",
    "            'angle': slice(9, 10),             # Angular position\n",
    "            'neighbors': slice(10, 11),        # Normalized neighbor count\n",
    "            'quantum_windows': slice(11, None) # 4 features × 3 windows\n",
    "        }\n",
    "\n",
    "        # Process spatial features: position, boundaries, radial distance\n",
    "        spatial_dim = 2 + 4 + 1  # pos(2) + boundaries(4) + radial(1)\n",
    "        self.spatial_transform = nn.Sequential(\n",
    "            nn.Linear(spatial_dim, hidden_channels // 2),\n",
    "            nn.LayerNorm(hidden_channels // 2),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "        # Process quantum features: rydberg value + window features\n",
    "        quantum_dim = 1 + (4 * 3)  # rydberg(1) + (4 features × 3 windows)\n",
    "        self.quantum_transform = nn.Sequential(\n",
    "            nn.Linear(quantum_dim, hidden_channels // 2),\n",
    "            nn.LayerNorm(hidden_channels // 2),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "        # Edge feature processing\n",
    "        self.edge_transform = nn.Sequential(\n",
    "            nn.Linear(edge_attr_dim, hidden_channels // 2),\n",
    "            nn.LayerNorm(hidden_channels // 2),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "        # Combine transformed features\n",
    "        self.feature_combiner = nn.Sequential(\n",
    "            nn.Linear(hidden_channels + 3, hidden_channels),  # +3 for angle, neighbors, mask\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_channels, hidden_channels)\n",
    "        )\n",
    "\n",
    "        # Message Passing layers (alternating GINE and TransformerConv)\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            if i % 2 == 0:\n",
    "                mp_mlp = nn.Sequential(\n",
    "                    nn.Linear(hidden_channels, hidden_channels),\n",
    "                    nn.LayerNorm(hidden_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Dropout(dropout_p),\n",
    "                    nn.Linear(hidden_channels, hidden_channels)\n",
    "                )\n",
    "                conv = GINEConv(mp_mlp, edge_dim=hidden_channels // 2)\n",
    "            else:\n",
    "                conv = TransformerConv(\n",
    "                    hidden_channels, hidden_channels // 4,\n",
    "                    heads=4,\n",
    "                    edge_dim=hidden_channels // 2,\n",
    "                    dropout=dropout_p,\n",
    "                    beta=True\n",
    "                )\n",
    "            self.convs.append(conv)\n",
    "            self.norms.append(BatchNorm(hidden_channels))\n",
    "\n",
    "        # Readouts\n",
    "        self.set2set_readout = Set2Set(hidden_channels, processing_steps=4)\n",
    "        self.gate_nn = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels // 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_channels // 2, 1)\n",
    "        )\n",
    "        self.global_attention = AttentionalAggregation(gate_nn=self.gate_nn)\n",
    "\n",
    "        self.global_transform = nn.Sequential(\n",
    "            nn.Linear(3, hidden_channels),  # [nA/N, nB/N, N]\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_channels, hidden_channels)\n",
    "        )\n",
    "\n",
    "        # Size encoder (still present, even if you only train on 1 size)\n",
    "        self.size_encoder = nn.Sequential(\n",
    "            nn.Linear(1, hidden_channels // 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_channels // 2, hidden_channels // 2)\n",
    "        )\n",
    "\n",
    "        combined_dim = (2 * hidden_channels) + (2 * hidden_channels) + (hidden_channels // 2)\n",
    "        self.final_mlp = nn.Sequential(\n",
    "            nn.Linear(combined_dim, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_channels, hidden_channels // 2),\n",
    "            nn.LayerNorm(hidden_channels // 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_channels // 2, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        # Split node features\n",
    "        spatial_features = torch.cat([\n",
    "            x[:, self.feature_indices['position']],\n",
    "            x[:, self.feature_indices['boundaries']],\n",
    "            x[:, self.feature_indices['radial']]\n",
    "        ], dim=1)\n",
    "\n",
    "        quantum_features = torch.cat([\n",
    "            x[:, self.feature_indices['rydberg_val']],\n",
    "            x[:, self.feature_indices['quantum_windows']]\n",
    "        ], dim=1)\n",
    "\n",
    "        angle_features = x[:, self.feature_indices['angle']]\n",
    "        neighbor_features = x[:, self.feature_indices['neighbors']]\n",
    "        mask_features = x[:, self.feature_indices['mask']]\n",
    "\n",
    "        # Transform features\n",
    "        spatial_out = self.spatial_transform(spatial_features)\n",
    "        quantum_out = self.quantum_transform(quantum_features)\n",
    "        edge_features = self.edge_transform(edge_attr)\n",
    "\n",
    "        # Combine node features\n",
    "        h = self.feature_combiner(torch.cat([\n",
    "            spatial_out, quantum_out,\n",
    "            angle_features, neighbor_features, mask_features\n",
    "        ], dim=1))\n",
    "\n",
    "        # Message passing\n",
    "        for i in range(self.num_layers):\n",
    "            h_new = self.convs[i](h, edge_index, edge_features)\n",
    "            h_new = self.norms[i](h_new)\n",
    "            h = h + h_new\n",
    "\n",
    "        # Readouts\n",
    "        s2s = self.set2set_readout(h, batch)\n",
    "        ga = self.global_attention(h, batch)\n",
    "\n",
    "        system_size = data.system_size.squeeze(-1)\n",
    "        nA = data.nA.squeeze(-1)\n",
    "        nB = data.nB.squeeze(-1)\n",
    "\n",
    "        global_feats = torch.stack([\n",
    "            nA / system_size,\n",
    "            nB / system_size,\n",
    "            system_size,\n",
    "        ], dim=1)\n",
    "\n",
    "        gf_out = self.global_transform(global_feats)\n",
    "        size_encoded = self.size_encoder(system_size.unsqueeze(-1))\n",
    "        \n",
    "        combined = torch.cat([s2s, ga, gf_out, size_encoded], dim=-1)\n",
    "        out = self.final_mlp(combined)\n",
    "        return out\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# PhysicalScaleAwareLoss\n",
    "# -----------------------------------------------------------\n",
    "class PhysicalScaleAwareLoss(nn.Module):\n",
    "    def __init__(self, base_size=4, scaling_power=1.5, physics_weight=1.0):\n",
    "        super().__init__()\n",
    "        self.base_size = base_size\n",
    "        self.scaling_power = scaling_power\n",
    "        self.physics_weight = physics_weight\n",
    "\n",
    "    def get_entropy_bounds(self, system_size, subsystem_size):\n",
    "        \"\"\"\n",
    "        Lower bound is 0, \n",
    "        upper bound is min(A, B)*log(2).\n",
    "        \"\"\"\n",
    "        lower_bound = torch.zeros_like(system_size, dtype=torch.float)\n",
    "        min_size = torch.minimum(subsystem_size.float(), (system_size - subsystem_size).float())\n",
    "        upper_bound = min_size * torch.log(torch.tensor(2.0, device=system_size.device))\n",
    "        return lower_bound, upper_bound\n",
    "\n",
    "    def forward(self, pred_log_s_over_n, target, system_size, subsystem_size):\n",
    "        # Convert prediction to absolute entropy\n",
    "        pred_entropy = torch.exp(pred_log_s_over_n * system_size)\n",
    "\n",
    "        # Physical bounds\n",
    "        lower_bound, upper_bound = self.get_entropy_bounds(system_size, subsystem_size)\n",
    "\n",
    "        # Out-of-bounds penalty\n",
    "        lower_violation = F.smooth_l1_loss(\n",
    "            torch.maximum(lower_bound, pred_entropy),\n",
    "            pred_entropy,\n",
    "            reduction='none'\n",
    "        )\n",
    "        upper_violation = F.smooth_l1_loss(\n",
    "            torch.minimum(upper_bound, pred_entropy),\n",
    "            pred_entropy,\n",
    "            reduction='none'\n",
    "        )\n",
    "        physics_loss = lower_violation + upper_violation\n",
    "\n",
    "        # MSE on log(S)/N\n",
    "        log_target = torch.log(target + 1e-10) / system_size\n",
    "        base_loss = F.mse_loss(pred_log_s_over_n, log_target, reduction='none')\n",
    "\n",
    "        # Size-based weighting\n",
    "        size_weight = (system_size.float() / self.base_size) ** self.scaling_power\n",
    "        weighted_loss = base_loss * size_weight\n",
    "\n",
    "        return (weighted_loss + self.physics_weight * physics_loss).mean()\n",
    "\n",
    "def mape_loss(pred, target, eps=1e-10):\n",
    "    \"\"\"\n",
    "    Standard MAPE:\n",
    "      mean( |(pred - target) / (target + eps)| )\n",
    "    We omit the *100 since it's just a scaling factor.\n",
    "    \"\"\"\n",
    "    return torch.mean(torch.abs((pred - target) / (target + eps)))\n",
    "\n",
    "class MultiTaskLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Adds a MAPE term on absolute S with a small weight to keep training smooth.\n",
    "    \"\"\"\n",
    "    def __init__(self, physics_weight=0.3, alpha_s_over_n=0.2, alpha_mape=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          physics_weight: weight for physical bound penalty (inside PhysicalScaleAwareLoss).\n",
    "          alpha_s_over_n: weight for MSE on second output (S/N).\n",
    "          alpha_mape:     weight for MAPE on absolute S.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.phys_loss = PhysicalScaleAwareLoss(physics_weight=physics_weight)\n",
    "        self.alpha_s_over_n = alpha_s_over_n\n",
    "        self.alpha_mape = alpha_mape\n",
    "\n",
    "    def forward(self, preds, targets, system_size, subsystem_size):\n",
    "        # First head predicts log(S/N), second head predicts (S/N).\n",
    "        pred_log_s_over_n = preds[:, 0]\n",
    "        pred_s_over_n     = preds[:, 1]\n",
    "\n",
    "        # (1) Physical scale aware loss (log(S/N) head)\n",
    "        loss1 = self.phys_loss(pred_log_s_over_n, targets, system_size, subsystem_size)\n",
    "\n",
    "        # (2) MSE on second head\n",
    "        true_s_over_n = targets / (system_size + 1e-10)\n",
    "        loss2 = F.mse_loss(pred_s_over_n, true_s_over_n)\n",
    "\n",
    "        # (3) MAPE on absolute S\n",
    "        pred_entropy_abs = torch.exp(pred_log_s_over_n * system_size)\n",
    "        loss_mape = mape_loss(pred_entropy_abs, targets)\n",
    "\n",
    "        # Combine\n",
    "        total_loss = loss1 + self.alpha_s_over_n * loss2 + self.alpha_mape * loss_mape\n",
    "        return total_loss\n",
    "# -----------------------------------------------------------\n",
    "# Curriculum Sampler\n",
    "# -----------------------------------------------------------\n",
    "def get_curriculum_sampler(dataset, epoch, max_epochs, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Adjust sampling probabilities based on system size and training progress.\n",
    "    This has little effect if the dataset has only one size, but is kept for completeness.\n",
    "    \"\"\"\n",
    "    system_sizes = [data.system_size.item() for data in dataset]\n",
    "    if len(system_sizes) == 0:\n",
    "        return None\n",
    "\n",
    "    max_size = max(system_sizes)\n",
    "    progress = min(1.0, epoch / (max_epochs * 0.7))\n",
    "\n",
    "    weights = []\n",
    "    for size in system_sizes:\n",
    "        size_ratio = (size - 4) / (max_size - 4 + 1e-6)\n",
    "        w = 1.0 + (progress ** alpha) * size_ratio\n",
    "        weights.append(w if w > 0 else 1e-6)\n",
    "\n",
    "    return WeightedRandomSampler(weights=weights, num_samples=len(dataset), replacement=True)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Training & Evaluation\n",
    "# -----------------------------------------------------------\n",
    "def train_epoch(model, loader, optimizer, criterion, device, clip_grad=None):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(data)\n",
    "        subsystem_size = data.nA.squeeze(-1)\n",
    "        system_size_ = data.system_size.squeeze(-1)\n",
    "        targets_ = data.y.squeeze()\n",
    "\n",
    "        loss = criterion(preds, targets_, system_size_, subsystem_size)\n",
    "        loss.backward()\n",
    "\n",
    "        if clip_grad is not None:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = data.num_graphs\n",
    "        n_samples += batch_size\n",
    "        total_loss += loss.item() * batch_size\n",
    "\n",
    "    return total_loss / n_samples if n_samples > 0 else 0.0\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device, name='Eval'):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    # Lists to accumulate predictions and targets\n",
    "    all_preds_abs = []\n",
    "    all_preds_s_over_n = []\n",
    "    all_targets = []\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        preds = model(data)\n",
    "\n",
    "        subsystem_size = data.nA.squeeze(-1)\n",
    "        system_size_ = data.system_size.squeeze(-1)\n",
    "        targets_ = data.y.squeeze()\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(preds, targets_, system_size_, subsystem_size)\n",
    "        batch_size = data.num_graphs\n",
    "        total_loss += loss.item() * batch_size\n",
    "        n_samples += batch_size\n",
    "\n",
    "        # Convert first head (log(S/N)) -> absolute entropy\n",
    "        pred_log_s_over_n = preds[:, 0]\n",
    "        pred_entropy_abs = torch.exp(pred_log_s_over_n * system_size_)\n",
    "\n",
    "        # Accumulate for metrics\n",
    "        all_preds_abs.append(pred_entropy_abs.cpu())\n",
    "        all_preds_s_over_n.append(preds[:, 1].cpu())\n",
    "        all_targets.append(targets_.cpu())\n",
    "\n",
    "    mean_loss = total_loss / n_samples if n_samples > 0 else 0.0\n",
    "\n",
    "    # Convert all predictions/targets to NumPy for sklearn metrics\n",
    "    all_preds_abs = torch.cat(all_preds_abs).numpy()\n",
    "    all_preds_s_over_n = torch.cat(all_preds_s_over_n).numpy()\n",
    "    all_targets = torch.cat(all_targets).numpy()\n",
    "\n",
    "    # Compute MSE, MAE, MAPE on absolute entropy\n",
    "    mse_abs = mean_squared_error(all_targets, all_preds_abs)\n",
    "    mae_abs = mean_absolute_error(all_targets, all_preds_abs)\n",
    "    # Small epsilon is added in the denominator to avoid division by zero\n",
    "    mape_abs = np.mean(np.abs((all_preds_abs - all_targets) / (all_targets + 1e-10))) * 100\n",
    "\n",
    "    logging.info(f\"\\n{name} Summary:\")\n",
    "    logging.info(f\"  Loss: {mean_loss:.6f}\")\n",
    "    logging.info(f\"  MSE (Absolute Entropy): {mse_abs:.6f}\")\n",
    "    logging.info(f\"  MAE (Absolute Entropy): {mae_abs:.6f}\")\n",
    "    logging.info(f\"  MAPE (Absolute Entropy): {mape_abs:.2f}%\")\n",
    "\n",
    "    return {\n",
    "        'loss': mean_loss,\n",
    "        'mse_abs': mse_abs,\n",
    "        'mae_abs': mae_abs,\n",
    "        'mape_abs': mape_abs,\n",
    "        'predictions_abs': all_preds_abs,        # S_pred\n",
    "        'predictions_s_over_n': all_preds_s_over_n,  # (S_pred)/N\n",
    "        'targets': all_targets                   # Actual S\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Main\n",
    "# -----------------------------------------------------------\n",
    "def main():\n",
    "    setup_logging()\n",
    "    set_seed(CONFIG['random_seed'])\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = SpinSystemDataset(root=CONFIG['processed_dir'])\n",
    "    if len(dataset) == 0:\n",
    "        logging.error(\"Loaded dataset is empty. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Since we train on a single size, dataset likely already contains only one size.\n",
    "    # Still, we do a standard train/val split:\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        dataset,\n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(CONFIG['random_seed'])\n",
    "    )\n",
    "\n",
    "    # Build model\n",
    "    sample_data = next(iter(DataLoader(train_dataset, batch_size=1)))\n",
    "    model = ExperimentalGNN(\n",
    "        num_node_features=sample_data.x.size(1),\n",
    "        edge_attr_dim=sample_data.edge_attr.size(1),\n",
    "        hidden_channels=CONFIG['hidden_channels'],\n",
    "        dropout_p=CONFIG['dropout_p'],\n",
    "        num_layers=CONFIG['num_layers']\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = MultiTaskLoss(physics_weight=0.3, alpha_s_over_n=0.2)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=CONFIG['learning_rate'],\n",
    "        weight_decay=CONFIG['weight_decay']\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=CONFIG['scheduler_factor'],\n",
    "        patience=CONFIG['scheduler_patience']\n",
    "    )\n",
    "\n",
    "    def create_dataloader(ds, epoch, shuffle=False):\n",
    "        \"\"\"Optional curriculum sampler; has minimal effect if ds has only one size.\"\"\"\n",
    "        sampler = None\n",
    "        if shuffle:\n",
    "            sampler = get_curriculum_sampler(\n",
    "                ds, epoch, CONFIG['num_epochs'], alpha=CONFIG['curriculum_alpha']\n",
    "            )\n",
    "        return DataLoader(ds, batch_size=CONFIG['batch_size'], sampler=sampler, shuffle=(sampler is None))\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(CONFIG['num_epochs']):\n",
    "        logging.info(f\"Epoch {epoch+1}/{CONFIG['num_epochs']}\")\n",
    "        train_loader = create_dataloader(train_dataset, epoch, shuffle=True)\n",
    "        val_loader = create_dataloader(val_dataset, epoch, shuffle=False)\n",
    "\n",
    "        # Train\n",
    "        train_loss = train_epoch(\n",
    "            model, train_loader, optimizer, criterion,\n",
    "            device, clip_grad=CONFIG['grad_clip']\n",
    "        )\n",
    "        logging.info(f\"  Training Loss: {train_loss:.6f}\")\n",
    "\n",
    "        # Validate\n",
    "        val_metrics = evaluate(model, val_loader, criterion, device, name='Validation')\n",
    "        val_loss = val_metrics['loss']\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Check if best\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), CONFIG['best_model_path'])\n",
    "            logging.info(f\"  [Info] New best model saved (val_loss={best_val_loss:.6f})\")\n",
    "\n",
    "    logging.info(\"Training complete. Loading best model for final validation...\")\n",
    "    model.load_state_dict(torch.load(CONFIG['best_model_path'], map_location=device))\n",
    "    _ = evaluate(model, create_dataloader(val_dataset, epoch=0), criterion, device, name='Final Validation')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f4a32a-bd3a-4dcd-8664-913283180ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
