{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6104e3be-e789-47b4-9c7d-4bb4eb71d6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-12 06:15:17 [INFO] Epoch 1/500\n",
      "2025-02-12 06:17:24 [INFO]   Training Loss: 7302988.847043\n",
      "2025-02-12 06:22:53 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 06:22:53 [INFO]   Loss: 2.764213\n",
      "2025-02-12 06:22:53 [INFO]   MSE (Absolute S): 1.057625\n",
      "2025-02-12 06:22:53 [INFO]   MAE (Absolute S): 0.705009\n",
      "2025-02-12 06:22:53 [INFO]   MAPE (Absolute S): 270.90%\n",
      "2025-02-12 06:22:53 [INFO]   [Info] Best model saved (val_loss=2.764213)\n",
      "2025-02-12 06:22:53 [INFO] Epoch 2/500\n",
      "2025-02-12 06:25:36 [INFO]   Training Loss: 15554.775728\n",
      "2025-02-12 06:29:32 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 06:29:32 [INFO]   Loss: 0.233402\n",
      "2025-02-12 06:29:32 [INFO]   MSE (Absolute S): 0.012987\n",
      "2025-02-12 06:29:32 [INFO]   MAE (Absolute S): 0.070817\n",
      "2025-02-12 06:29:32 [INFO]   MAPE (Absolute S): 46.04%\n",
      "2025-02-12 06:29:32 [INFO]   [Info] Best model saved (val_loss=0.233402)\n",
      "2025-02-12 06:29:32 [INFO] Epoch 3/500\n",
      "2025-02-12 06:32:16 [INFO]   Training Loss: 6.844970\n",
      "2025-02-12 06:36:17 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 06:36:17 [INFO]   Loss: 0.236897\n",
      "2025-02-12 06:36:17 [INFO]   MSE (Absolute S): 0.010868\n",
      "2025-02-12 06:36:17 [INFO]   MAE (Absolute S): 0.073971\n",
      "2025-02-12 06:36:17 [INFO]   MAPE (Absolute S): 46.84%\n",
      "2025-02-12 06:36:17 [INFO] Epoch 4/500\n",
      "2025-02-12 06:39:03 [INFO]   Training Loss: 53.283572\n",
      "2025-02-12 06:43:25 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 06:43:25 [INFO]   Loss: 0.212520\n",
      "2025-02-12 06:43:25 [INFO]   MSE (Absolute S): 0.005030\n",
      "2025-02-12 06:43:25 [INFO]   MAE (Absolute S): 0.044441\n",
      "2025-02-12 06:43:25 [INFO]   MAPE (Absolute S): 42.25%\n",
      "2025-02-12 06:43:25 [INFO]   [Info] Best model saved (val_loss=0.212520)\n",
      "2025-02-12 06:43:25 [INFO] Epoch 5/500\n",
      "2025-02-12 06:46:11 [INFO]   Training Loss: 1039.673367\n",
      "2025-02-12 06:50:26 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 06:50:26 [INFO]   Loss: 0.199215\n",
      "2025-02-12 06:50:26 [INFO]   MSE (Absolute S): 0.004344\n",
      "2025-02-12 06:50:26 [INFO]   MAE (Absolute S): 0.043226\n",
      "2025-02-12 06:50:26 [INFO]   MAPE (Absolute S): 39.63%\n",
      "2025-02-12 06:50:26 [INFO]   [Info] Best model saved (val_loss=0.199215)\n",
      "2025-02-12 06:50:26 [INFO] Epoch 6/500\n",
      "2025-02-12 06:53:09 [INFO]   Training Loss: 62.093404\n",
      "2025-02-12 06:57:25 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 06:57:25 [INFO]   Loss: 828437.384409\n",
      "2025-02-12 06:57:25 [INFO]   MSE (Absolute S): 0.002282\n",
      "2025-02-12 06:57:25 [INFO]   MAE (Absolute S): 0.031987\n",
      "2025-02-12 06:57:25 [INFO]   MAPE (Absolute S): 167519500.00%\n",
      "2025-02-12 06:57:25 [INFO] Epoch 7/500\n",
      "2025-02-12 07:00:12 [INFO]   Training Loss: 11.699312\n",
      "2025-02-12 07:04:22 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 07:04:22 [INFO]   Loss: 0.224296\n",
      "2025-02-12 07:04:22 [INFO]   MSE (Absolute S): 0.008430\n",
      "2025-02-12 07:04:22 [INFO]   MAE (Absolute S): 0.062486\n",
      "2025-02-12 07:04:22 [INFO]   MAPE (Absolute S): 44.44%\n",
      "2025-02-12 07:04:22 [INFO] Epoch 8/500\n",
      "2025-02-12 07:07:05 [INFO]   Training Loss: 173.800211\n",
      "2025-02-12 07:10:56 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 07:10:56 [INFO]   Loss: 0.212183\n",
      "2025-02-12 07:10:56 [INFO]   MSE (Absolute S): 0.003646\n",
      "2025-02-12 07:10:56 [INFO]   MAE (Absolute S): 0.041522\n",
      "2025-02-12 07:10:56 [INFO]   MAPE (Absolute S): 42.26%\n",
      "2025-02-12 07:10:56 [INFO] Epoch 9/500\n",
      "2025-02-12 07:13:41 [INFO]   Training Loss: 12.786630\n",
      "2025-02-12 07:18:11 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 07:18:11 [INFO]   Loss: 0.193177\n",
      "2025-02-12 07:18:11 [INFO]   MSE (Absolute S): 0.002671\n",
      "2025-02-12 07:18:11 [INFO]   MAE (Absolute S): 0.034645\n",
      "2025-02-12 07:18:11 [INFO]   MAPE (Absolute S): 38.50%\n",
      "2025-02-12 07:18:11 [INFO]   [Info] Best model saved (val_loss=0.193177)\n",
      "2025-02-12 07:18:11 [INFO] Epoch 10/500\n",
      "2025-02-12 07:20:54 [INFO]   Training Loss: 2.952586\n",
      "2025-02-12 07:24:50 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 07:24:50 [INFO]   Loss: 0.199706\n",
      "2025-02-12 07:24:50 [INFO]   MSE (Absolute S): 0.002351\n",
      "2025-02-12 07:24:50 [INFO]   MAE (Absolute S): 0.031987\n",
      "2025-02-12 07:24:50 [INFO]   MAPE (Absolute S): 39.83%\n",
      "2025-02-12 07:24:50 [INFO] Epoch 11/500\n",
      "2025-02-12 07:27:36 [INFO]   Training Loss: 40.318400\n",
      "2025-02-12 07:31:35 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 07:31:35 [INFO]   Loss: 379.564867\n",
      "2025-02-12 07:31:35 [INFO]   MSE (Absolute S): 0.005803\n",
      "2025-02-12 07:31:35 [INFO]   MAE (Absolute S): 0.052321\n",
      "2025-02-12 07:31:35 [INFO]   MAPE (Absolute S): 76698.21%\n",
      "2025-02-12 07:31:35 [INFO] Epoch 12/500\n",
      "2025-02-12 07:34:19 [INFO]   Training Loss: 0.624925\n",
      "2025-02-12 07:38:28 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 07:38:28 [INFO]   Loss: 0.204405\n",
      "2025-02-12 07:38:28 [INFO]   MSE (Absolute S): 0.003866\n",
      "2025-02-12 07:38:28 [INFO]   MAE (Absolute S): 0.041737\n",
      "2025-02-12 07:38:28 [INFO]   MAPE (Absolute S): 40.69%\n",
      "2025-02-12 07:38:28 [INFO] Epoch 13/500\n",
      "2025-02-12 07:41:16 [INFO]   Training Loss: 3.503123\n",
      "2025-02-12 07:45:56 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 07:45:56 [INFO]   Loss: 0.202446\n",
      "2025-02-12 07:45:56 [INFO]   MSE (Absolute S): 0.002193\n",
      "2025-02-12 07:45:56 [INFO]   MAE (Absolute S): 0.032831\n",
      "2025-02-12 07:45:56 [INFO]   MAPE (Absolute S): 40.38%\n",
      "2025-02-12 07:45:56 [INFO] Epoch 14/500\n",
      "2025-02-12 07:48:49 [INFO]   Training Loss: 594.522889\n",
      "2025-02-12 07:52:52 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 07:52:52 [INFO]   Loss: 546577.292267\n",
      "2025-02-12 07:52:52 [INFO]   MSE (Absolute S): 0.007617\n",
      "2025-02-12 07:52:52 [INFO]   MAE (Absolute S): 0.061617\n",
      "2025-02-12 07:52:52 [INFO]   MAPE (Absolute S): 110515137.50%\n",
      "2025-02-12 07:52:52 [INFO] Epoch 15/500\n",
      "2025-02-12 07:55:41 [INFO]   Training Loss: 649.338420\n",
      "2025-02-12 08:00:25 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 08:00:25 [INFO]   Loss: 1222038.085674\n",
      "2025-02-12 08:00:25 [INFO]   MSE (Absolute S): 0.006055\n",
      "2025-02-12 08:00:25 [INFO]   MAE (Absolute S): 0.053612\n",
      "2025-02-12 08:00:25 [INFO]   MAPE (Absolute S): 247087275.00%\n",
      "2025-02-12 08:00:25 [INFO] Epoch 16/500\n",
      "2025-02-12 08:03:18 [INFO]   Training Loss: 106.134329\n",
      "2025-02-12 08:07:43 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 08:07:43 [INFO]   Loss: 0.171769\n",
      "2025-02-12 08:07:43 [INFO]   MSE (Absolute S): 0.000580\n",
      "2025-02-12 08:07:43 [INFO]   MAE (Absolute S): 0.013835\n",
      "2025-02-12 08:07:43 [INFO]   MAPE (Absolute S): 34.33%\n",
      "2025-02-12 08:07:44 [INFO]   [Info] Best model saved (val_loss=0.171769)\n",
      "2025-02-12 08:07:44 [INFO] Epoch 17/500\n",
      "2025-02-12 08:10:34 [INFO]   Training Loss: 270.894008\n",
      "2025-02-12 08:15:18 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 08:15:18 [INFO]   Loss: 0.210676\n",
      "2025-02-12 08:15:18 [INFO]   MSE (Absolute S): 0.004447\n",
      "2025-02-12 08:15:18 [INFO]   MAE (Absolute S): 0.046521\n",
      "2025-02-12 08:15:18 [INFO]   MAPE (Absolute S): 41.91%\n",
      "2025-02-12 08:15:18 [INFO] Epoch 18/500\n",
      "2025-02-12 08:18:05 [INFO]   Training Loss: 2.124498\n",
      "2025-02-12 08:22:38 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 08:22:38 [INFO]   Loss: 0.196588\n",
      "2025-02-12 08:22:38 [INFO]   MSE (Absolute S): 0.005381\n",
      "2025-02-12 08:22:38 [INFO]   MAE (Absolute S): 0.051414\n",
      "2025-02-12 08:22:38 [INFO]   MAPE (Absolute S): 39.05%\n",
      "2025-02-12 08:22:38 [INFO] Epoch 19/500\n",
      "2025-02-12 08:25:26 [INFO]   Training Loss: 23.866394\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 478\u001b[0m\n\u001b[0;32m    475\u001b[0m     _ \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion, device, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinal Validation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 478\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[16], line 464\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    461\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_epoch(model, train_loader, optimizer, criterion, device, clip_grad\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrad_clip\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    462\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 464\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion, device, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    465\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Step the scheduler after each epoch\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loss \u001b[38;5;241m<\u001b[39m best_val_loss:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[16], line 374\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, loader, criterion, device, name)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m    373\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 374\u001b[0m     pred_s \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[0;32m    376\u001b[0m     targets \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m    377\u001b[0m     system_size \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msystem_size\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[16], line 304\u001b[0m, in \u001b[0;36mExperimentalGNN.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    301\u001b[0m e_enc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_convs[i](e_enc)\n\u001b[0;32m    303\u001b[0m \u001b[38;5;66;03m# Perform convolution with edge attention\u001b[39;00m\n\u001b[1;32m--> 304\u001b[0m h_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs[i](h, edge_index, e_enc \u001b[38;5;241m*\u001b[39m edge_weights\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    306\u001b[0m \u001b[38;5;66;03m# BatchNorm on node outputs\u001b[39;00m\n\u001b[0;32m    307\u001b[0m h_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorms[i](h_new)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch_geometric\\nn\\conv\\transformer_conv.py:177\u001b[0m, in \u001b[0;36mTransformerConv.forward\u001b[1;34m(self, x, edge_index, edge_attr, return_attention_weights)\u001b[0m\n\u001b[0;32m    174\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_value(x[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, H, C)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;66;03m# propagate_type: (query: Tensor, key:Tensor, value: Tensor, edge_attr: OptTensor) # noqa\u001b[39;00m\n\u001b[1;32m--> 177\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, query\u001b[38;5;241m=\u001b[39mquery, key\u001b[38;5;241m=\u001b[39mkey, value\u001b[38;5;241m=\u001b[39mvalue,\n\u001b[0;32m    178\u001b[0m                      edge_attr\u001b[38;5;241m=\u001b[39medge_attr, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    180\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alpha\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:484\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[1;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    482\u001b[0m         aggr_kwargs \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m res\n\u001b[1;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate(out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39maggr_kwargs)\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_forward_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m    487\u001b[0m     res \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, (aggr_kwargs, ), out)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:608\u001b[0m, in \u001b[0;36mMessagePassing.aggregate\u001b[1;34m(self, inputs, index, ptr, dim_size)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maggregate\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Tensor, index: Tensor,\n\u001b[0;32m    596\u001b[0m               ptr: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    597\u001b[0m               dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    598\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Aggregates messages from neighbors as\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m    :math:`\\bigoplus_{j \\in \\mathcal{N}(i)}`.\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03m    as specified in :meth:`__init__` by the :obj:`aggr` argument.\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggr_module(inputs, index, ptr\u001b[38;5;241m=\u001b[39mptr, dim_size\u001b[38;5;241m=\u001b[39mdim_size,\n\u001b[0;32m    609\u001b[0m                             dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch_geometric\\nn\\aggr\\base.py:109\u001b[0m, in \u001b[0;36mAggregation.__call__\u001b[1;34m(self, x, index, ptr, dim_size, dim, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m     dim_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(index\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(x, index, ptr, dim_size, dim, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch_geometric\\nn\\aggr\\basic.py:21\u001b[0m, in \u001b[0;36mSumAggregation.forward\u001b[1;34m(self, x, index, ptr, dim_size, dim)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, index: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     19\u001b[0m             ptr: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     20\u001b[0m             dim: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(x, index, ptr, dim_size, dim, reduce\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch_geometric\\nn\\aggr\\base.py:155\u001b[0m, in \u001b[0;36mAggregation.reduce\u001b[1;34m(self, x, index, ptr, dim_size, dim, reduce)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m segment(x, ptr, reduce\u001b[38;5;241m=\u001b[39mreduce)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scatter(x, index, dim, dim_size, reduce)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch_geometric\\utils\\scatter.py:74\u001b[0m, in \u001b[0;36mscatter\u001b[1;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     73\u001b[0m     index \u001b[38;5;241m=\u001b[39m broadcast(index, src, dim)\n\u001b[1;32m---> 74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m src\u001b[38;5;241m.\u001b[39mnew_zeros(size)\u001b[38;5;241m.\u001b[39mscatter_add_(dim, index, src)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     77\u001b[0m     count \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mnew_zeros(dim_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import dropout_adj\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_geometric.nn import (\n",
    "    GINEConv,\n",
    "    TransformerConv,\n",
    "    Set2Set,\n",
    "    BatchNorm,           # We'll use this for batch normalization\n",
    "    global_add_pool\n",
    ")\n",
    "from torch.utils.data import random_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from torch_geometric.nn import (\n",
    "    GINEConv, TransformerConv, Set2Set, GraphNorm\n",
    ")\n",
    "from torch_geometric.utils import dropout_adj\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Configuration\n",
    "# -----------------------------------------------------------\n",
    "CONFIG = {\n",
    "    'processed_dir': './processed_experimentalrung1',\n",
    "    'processed_file_name': 'data.pt',\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 3e-4,\n",
    "    'weight_decay': 1e-4,\n",
    "    'hidden_channels': 512,\n",
    "    'num_epochs': 500,\n",
    "    'patience': 75,\n",
    "    'random_seed': 42,\n",
    "    'best_model_path': 'best_model_rung1.pth',\n",
    "    'dropout_p': 0.3,\n",
    "    'scheduler_factor': 0.5,\n",
    "    'scheduler_patience': 10,\n",
    "    'grad_clip': 1.0,\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Logging & Utilities\n",
    "# -----------------------------------------------------------\n",
    "def setup_logging():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "        handlers=[logging.StreamHandler(sys.stdout)]\n",
    "    )\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Weight Initialization\n",
    "# -----------------------------------------------------------\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu') \n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.BatchNorm1d):\n",
    "        nn.init.ones_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# SpinSystemDataset\n",
    "# -----------------------------------------------------------\n",
    "class SpinSystemDataset(InMemoryDataset):\n",
    "    \"\"\"Loads the data.pt from processed_dir (old-style).\"\"\"\n",
    "    def __init__(self, root='.', transform=None, pre_transform=None):\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0], weights_only=False)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [CONFIG['processed_file_name']]\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        pass\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# PhysicalScaleAwareLoss (Single-Head)\n",
    "# -----------------------------------------------------------\n",
    "class PhysicalScaleAwareLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    A combined loss that balances:\n",
    "      1) Relative error (to reduce percentage error).\n",
    "      2) A small MSE or log-cosh term for stability.\n",
    "      3) Physics-based bounds as soft constraints.\n",
    "    \"\"\"\n",
    "    def __init__(self, physics_weight=1.0, rel_weight=0.5, eps=1e-10):\n",
    "        super().__init__()\n",
    "        self.physics_weight = physics_weight\n",
    "        self.rel_weight = rel_weight\n",
    "        self.eps = eps\n",
    "\n",
    "    def get_entropy_bounds(self, system_size, subsystem_size):\n",
    "        lower_bound = torch.zeros_like(system_size, dtype=torch.float)\n",
    "        min_size = torch.minimum(subsystem_size.float(), \n",
    "                                 (system_size - subsystem_size).float())\n",
    "        upper_bound = min_size * torch.log(torch.tensor(2.0, \n",
    "                                             device=system_size.device))\n",
    "        return lower_bound, upper_bound\n",
    "\n",
    "    def forward(self, pred_s, target_s, system_size, subsystem_size):\n",
    "        # 1. Base MSE/log-cosh for stability\n",
    "        error = pred_s - target_s\n",
    "        logcosh = torch.log(torch.cosh(error + self.eps))\n",
    "        logcosh_loss = logcosh.mean()\n",
    "\n",
    "        # 2. Relative loss for percentage error\n",
    "        rel_error = torch.abs(error) / (torch.abs(target_s) + self.eps)\n",
    "        rel_loss = rel_error.mean()\n",
    "\n",
    "        # 3. Soft physics-based constraints\n",
    "        lower_bound, upper_bound = self.get_entropy_bounds(system_size, subsystem_size)\n",
    "        bounds_violation = (\n",
    "            torch.relu(lower_bound - pred_s) +\n",
    "            torch.relu(pred_s - upper_bound)\n",
    "        ).mean()\n",
    "\n",
    "        total_loss = (1 - self.rel_weight) * logcosh_loss + \\\n",
    "                     self.rel_weight * rel_loss + \\\n",
    "                     self.physics_weight * bounds_violation\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# GNN Model (Single Head: log(S/N)) + Global Features\n",
    "# Using BatchNorm Instead of LayerNorm\n",
    "# -----------------------------------------------------------\n",
    "class ExperimentalGNN(nn.Module):\n",
    "    def __init__(self, hidden_channels=64, num_layers=6, dropout_p=0.4):\n",
    "        super().__init__()\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        # Define feature indices for slicing input features\n",
    "        self.feature_indices = {\n",
    "            'position': slice(0, 2),  # First two columns are position features\n",
    "            'rydberg_val': 2,        # Third column is Rydberg value\n",
    "            'mask': 3,                # Fourth column is mask\n",
    "        }\n",
    "\n",
    "        # Node and Edge Encoders\n",
    "        self.node_encoder = nn.Sequential(\n",
    "            nn.Linear(4, hidden_channels),\n",
    "            BatchNorm(hidden_channels),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(3, hidden_channels),\n",
    "            BatchNorm(hidden_channels),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        # Edge-Attention Layers\n",
    "        self.edge_attention = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_channels, hidden_channels),\n",
    "                BatchNorm(hidden_channels),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(hidden_channels, 1),\n",
    "                nn.Sigmoid()  # Attention weights between 0 and 1\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Edge-Node Co-Processing Layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.edge_convs = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            if i % 2 == 0:\n",
    "                # GINEConv with enhanced edge features\n",
    "                mp_mlp = nn.Sequential(\n",
    "                    nn.Linear(hidden_channels, hidden_channels),\n",
    "                    BatchNorm(hidden_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Dropout(dropout_p),\n",
    "                    nn.Linear(hidden_channels, hidden_channels)\n",
    "                )\n",
    "                conv = GINEConv(mp_mlp, edge_dim=hidden_channels)\n",
    "            else:\n",
    "                # TransformerConv with edge-aware attention\n",
    "                conv = TransformerConv(\n",
    "                    hidden_channels,\n",
    "                    hidden_channels // 8,\n",
    "                    heads=8,\n",
    "                    edge_dim=hidden_channels,\n",
    "                    dropout=dropout_p,\n",
    "                    beta=True,\n",
    "                    concat=True\n",
    "                )\n",
    "            self.convs.append(conv)\n",
    "\n",
    "            # Edge MLP with BatchNorm\n",
    "            self.edge_convs.append(nn.Sequential(\n",
    "                nn.Linear(hidden_channels, hidden_channels),\n",
    "                BatchNorm(hidden_channels),\n",
    "                nn.SiLU(),\n",
    "                nn.Dropout(dropout_p)\n",
    "            ))\n",
    "\n",
    "            # Node features BN after convolution\n",
    "            self.norms.append(BatchNorm(hidden_channels))\n",
    "\n",
    "        # Edge-Preserving Pooling\n",
    "        self.pool = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_channels, hidden_channels),\n",
    "                BatchNorm(hidden_channels),\n",
    "                nn.SiLU(),\n",
    "                nn.Dropout(dropout_p)\n",
    "            ) for _ in range(num_layers // 2)\n",
    "        ])\n",
    "\n",
    "        # Readout with Edge Information\n",
    "        self.readout = nn.ModuleList([\n",
    "            Set2Set(hidden_channels, processing_steps=4) for _ in range(2)  # 2 heads\n",
    "        ])\n",
    "        \n",
    "        # Dimension reduction for readout outputs\n",
    "        self.readout_projection = nn.Sequential(\n",
    "            nn.Linear(4 * hidden_channels, 2 * hidden_channels),  # 4 = 2 heads * 2 (Set2Set doubles dim)\n",
    "            BatchNorm(2 * hidden_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_p)\n",
    "        )\n",
    "\n",
    "        # Global Features MLP\n",
    "        self.global_mlp = nn.Sequential(\n",
    "            nn.Linear(2, hidden_channels),\n",
    "            BatchNorm(hidden_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_channels, hidden_channels)\n",
    "        )\n",
    "\n",
    "        # Final MLP with corrected dimensions\n",
    "        combined_dim = (2 * hidden_channels) + hidden_channels  # projected readout + global features\n",
    "        self.final_mlp = nn.Sequential(\n",
    "            nn.Linear(combined_dim, hidden_channels),\n",
    "            BatchNorm(hidden_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_channels, hidden_channels // 2),\n",
    "            BatchNorm(hidden_channels // 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_channels // 2, 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = (\n",
    "            data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        )\n",
    "\n",
    "        # Node and Edge Feature Encoding\n",
    "        node_features = torch.cat([\n",
    "            x[:, self.feature_indices['position']],\n",
    "            x[:, self.feature_indices['rydberg_val']].unsqueeze(-1),\n",
    "            x[:, self.feature_indices['mask']].unsqueeze(-1),\n",
    "        ], dim=1)\n",
    "\n",
    "        edge_features = edge_attr\n",
    "\n",
    "        x_enc = self.node_encoder(node_features)\n",
    "        e_enc = self.edge_encoder(edge_features)\n",
    "\n",
    "        # Message Passing with Edge-Attention\n",
    "        h = x_enc\n",
    "        for i in range(self.num_layers):\n",
    "            # Compute edge attention weights\n",
    "            edge_weights = self.edge_attention[i](e_enc).squeeze(-1)\n",
    "\n",
    "            # Update edge features\n",
    "            e_enc = self.edge_convs[i](e_enc)\n",
    "\n",
    "            # Perform convolution with edge attention\n",
    "            h_new = self.convs[i](h, edge_index, e_enc * edge_weights.unsqueeze(-1))\n",
    "\n",
    "            # BatchNorm on node outputs\n",
    "            h_new = self.norms[i](h_new)\n",
    "\n",
    "            # Residual connection\n",
    "            h = h + h_new\n",
    "\n",
    "            # Edge-Preserving Pooling\n",
    "            if i % 2 == 0 and i // 2 < len(self.pool):\n",
    "                h = self.pool[i // 2](h)\n",
    "\n",
    "        # Multi-Head Readout with dimension reduction\n",
    "        readouts = [rd(h, batch) for rd in self.readout]\n",
    "        h_readout = torch.cat(readouts, dim=1)\n",
    "        h_readout = self.readout_projection(h_readout)\n",
    "\n",
    "        # Global Features\n",
    "        nA_over_N = data.nA.squeeze(-1) / (data.system_size.squeeze(-1) + 1e-10)\n",
    "        nB_over_N = data.nB.squeeze(-1) / (data.system_size.squeeze(-1) + 1e-10)\n",
    "        global_feats = torch.stack([nA_over_N, nB_over_N], dim=1)\n",
    "        gf_out = self.global_mlp(global_feats)\n",
    "\n",
    "        # Combine features with correct dimensions\n",
    "        combined = torch.cat([h_readout, gf_out], dim=1)\n",
    "        out = self.final_mlp(combined).squeeze(-1)\n",
    "        return out\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Training & Evaluation\n",
    "# -----------------------------------------------------------\n",
    "def train_epoch(model, loader, optimizer, criterion, device, clip_grad=None):\n",
    "    model.train()  # BatchNorm will track running stats here\n",
    "    total_loss = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred_s = model(data)\n",
    "        targets = data.y.squeeze()\n",
    "        system_size = data.system_size.squeeze(-1)\n",
    "        subsystem_size = data.nA.squeeze(-1)\n",
    "\n",
    "        loss = criterion(pred_s, targets, system_size, subsystem_size)\n",
    "        loss.backward()\n",
    "\n",
    "        if clip_grad is not None:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = data.num_graphs\n",
    "        n_samples += batch_size\n",
    "        total_loss += loss.item() * batch_size\n",
    "\n",
    "    return total_loss / n_samples if n_samples > 0 else 0.0\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device, name='Eval'):\n",
    "    model.eval()  # BatchNorm uses stored running_mean/var here\n",
    "    total_loss = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    all_preds_abs = []\n",
    "    all_targets = []\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred_s = model(data)\n",
    "\n",
    "        targets = data.y.squeeze()\n",
    "        system_size = data.system_size.squeeze(-1)\n",
    "        subsystem_size = data.nA.squeeze(-1)\n",
    "\n",
    "        loss = criterion(pred_s, targets, system_size, subsystem_size)\n",
    "        batch_size = data.num_graphs\n",
    "        total_loss += loss.item() * batch_size\n",
    "        n_samples += batch_size\n",
    "\n",
    "        all_preds_abs.append(pred_s.cpu())\n",
    "        all_targets.append(targets.cpu())\n",
    "\n",
    "    mean_loss = total_loss / n_samples if n_samples > 0 else 0.0\n",
    "\n",
    "    all_preds_abs = torch.cat(all_preds_abs).numpy()\n",
    "    all_targets = torch.cat(all_targets).numpy()\n",
    "\n",
    "    mse_abs = mean_squared_error(all_targets, all_preds_abs)\n",
    "    mae_abs = mean_absolute_error(all_targets, all_preds_abs)\n",
    "    mape_abs = np.mean(np.abs((all_preds_abs - all_targets) / (all_targets + 1e-10))) * 100\n",
    "\n",
    "    logging.info(f\"\\n{name} Summary:\")\n",
    "    logging.info(f\"  Loss: {mean_loss:.6f}\")\n",
    "    logging.info(f\"  MSE (Absolute S): {mse_abs:.6f}\")\n",
    "    logging.info(f\"  MAE (Absolute S): {mae_abs:.6f}\")\n",
    "    logging.info(f\"  MAPE (Absolute S): {mape_abs:.2f}%\")\n",
    "\n",
    "    return mean_loss\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Main\n",
    "# -----------------------------------------------------------\n",
    "def main():\n",
    "    setup_logging()\n",
    "    set_seed(CONFIG['random_seed'])\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = SpinSystemDataset(root=CONFIG['processed_dir'])\n",
    "    if len(dataset) == 0:\n",
    "        logging.error(\"Loaded dataset is empty. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Train/Val split\n",
    "    train_size = int(0.1 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        dataset,\n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(CONFIG['random_seed'])\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "    # Initialize the model (now using BatchNorm)\n",
    "    model = ExperimentalGNN(\n",
    "        hidden_channels=CONFIG['hidden_channels'],\n",
    "        dropout_p=CONFIG['dropout_p']\n",
    "    ).to(device)\n",
    "\n",
    "    # Apply weight initialization\n",
    "    model.apply(init_weights)\n",
    "\n",
    "    criterion = PhysicalScaleAwareLoss(physics_weight=2.0)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=CONFIG['learning_rate'],\n",
    "        weight_decay=CONFIG['weight_decay']\n",
    "    )\n",
    "\n",
    "    # Use Cosine Annealing with Warm Restarts\n",
    "    scheduler = CosineAnnealingWarmRestarts(\n",
    "        optimizer,\n",
    "        T_0=50,  # Number of iterations for the first restart\n",
    "        T_mult=2,  # A factor increases T_0 after a restart\n",
    "        eta_min=1e-6,  # Minimum learning rate\n",
    "    )\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(CONFIG['num_epochs']):\n",
    "        logging.info(f\"Epoch {epoch+1}/{CONFIG['num_epochs']}\")\n",
    "\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device, clip_grad=CONFIG['grad_clip'])\n",
    "        logging.info(f\"  Training Loss: {train_loss:.6f}\")\n",
    "\n",
    "        val_loss = evaluate(model, val_loader, criterion, device, name='Validation')\n",
    "        scheduler.step()  # Step the scheduler after each epoch\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), CONFIG['best_model_path'])\n",
    "            logging.info(f\"  [Info] Best model saved (val_loss={best_val_loss:.6f})\")\n",
    "\n",
    "    logging.info(\"Training complete. Loading best model for final validation...\")\n",
    "    model.load_state_dict(torch.load(CONFIG['best_model_path'], map_location=device))\n",
    "    model.eval()  # Ensure BatchNorm uses stored running stats\n",
    "    _ = evaluate(model, val_loader, criterion, device, name='Final Validation')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1582d3a-d211-4c40-a877-0d637669a7f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
