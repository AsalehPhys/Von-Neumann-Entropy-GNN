{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6104e3be-e789-47b4-9c7d-4bb4eb71d6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-12 06:15:17 [INFO] Epoch 1/500\n",
      "2025-02-12 06:17:24 [INFO]   Training Loss: 7302988.847043\n",
      "2025-02-12 06:22:53 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 06:22:53 [INFO]   Loss: 2.764213\n",
      "2025-02-12 06:22:53 [INFO]   MSE (Absolute S): 1.057625\n",
      "2025-02-12 06:22:53 [INFO]   MAE (Absolute S): 0.705009\n",
      "2025-02-12 06:22:53 [INFO]   MAPE (Absolute S): 270.90%\n",
      "2025-02-12 06:22:53 [INFO]   [Info] Best model saved (val_loss=2.764213)\n",
      "2025-02-12 06:22:53 [INFO] Epoch 2/500\n",
      "2025-02-12 06:25:36 [INFO]   Training Loss: 15554.775728\n",
      "2025-02-12 06:29:32 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 06:29:32 [INFO]   Loss: 0.233402\n",
      "2025-02-12 06:29:32 [INFO]   MSE (Absolute S): 0.012987\n",
      "2025-02-12 06:29:32 [INFO]   MAE (Absolute S): 0.070817\n",
      "2025-02-12 06:29:32 [INFO]   MAPE (Absolute S): 46.04%\n",
      "2025-02-12 06:29:32 [INFO]   [Info] Best model saved (val_loss=0.233402)\n",
      "2025-02-12 06:29:32 [INFO] Epoch 3/500\n",
      "2025-02-12 06:32:16 [INFO]   Training Loss: 6.844970\n",
      "2025-02-12 06:36:17 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 06:36:17 [INFO]   Loss: 0.236897\n",
      "2025-02-12 06:36:17 [INFO]   MSE (Absolute S): 0.010868\n",
      "2025-02-12 06:36:17 [INFO]   MAE (Absolute S): 0.073971\n",
      "2025-02-12 06:36:17 [INFO]   MAPE (Absolute S): 46.84%\n",
      "2025-02-12 06:36:17 [INFO] Epoch 4/500\n",
      "2025-02-12 06:39:03 [INFO]   Training Loss: 53.283572\n",
      "2025-02-12 06:43:25 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 06:43:25 [INFO]   Loss: 0.212520\n",
      "2025-02-12 06:43:25 [INFO]   MSE (Absolute S): 0.005030\n",
      "2025-02-12 06:43:25 [INFO]   MAE (Absolute S): 0.044441\n",
      "2025-02-12 06:43:25 [INFO]   MAPE (Absolute S): 42.25%\n",
      "2025-02-12 06:43:25 [INFO]   [Info] Best model saved (val_loss=0.212520)\n",
      "2025-02-12 06:43:25 [INFO] Epoch 5/500\n",
      "2025-02-12 06:46:11 [INFO]   Training Loss: 1039.673367\n",
      "2025-02-12 06:50:26 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 06:50:26 [INFO]   Loss: 0.199215\n",
      "2025-02-12 06:50:26 [INFO]   MSE (Absolute S): 0.004344\n",
      "2025-02-12 06:50:26 [INFO]   MAE (Absolute S): 0.043226\n",
      "2025-02-12 06:50:26 [INFO]   MAPE (Absolute S): 39.63%\n",
      "2025-02-12 06:50:26 [INFO]   [Info] Best model saved (val_loss=0.199215)\n",
      "2025-02-12 06:50:26 [INFO] Epoch 6/500\n",
      "2025-02-12 06:53:09 [INFO]   Training Loss: 62.093404\n",
      "2025-02-12 06:57:25 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 06:57:25 [INFO]   Loss: 828437.384409\n",
      "2025-02-12 06:57:25 [INFO]   MSE (Absolute S): 0.002282\n",
      "2025-02-12 06:57:25 [INFO]   MAE (Absolute S): 0.031987\n",
      "2025-02-12 06:57:25 [INFO]   MAPE (Absolute S): 167519500.00%\n",
      "2025-02-12 06:57:25 [INFO] Epoch 7/500\n",
      "2025-02-12 07:00:12 [INFO]   Training Loss: 11.699312\n",
      "2025-02-12 07:04:22 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 07:04:22 [INFO]   Loss: 0.224296\n",
      "2025-02-12 07:04:22 [INFO]   MSE (Absolute S): 0.008430\n",
      "2025-02-12 07:04:22 [INFO]   MAE (Absolute S): 0.062486\n",
      "2025-02-12 07:04:22 [INFO]   MAPE (Absolute S): 44.44%\n",
      "2025-02-12 07:04:22 [INFO] Epoch 8/500\n",
      "2025-02-12 07:07:05 [INFO]   Training Loss: 173.800211\n",
      "2025-02-12 07:10:56 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 07:10:56 [INFO]   Loss: 0.212183\n",
      "2025-02-12 07:10:56 [INFO]   MSE (Absolute S): 0.003646\n",
      "2025-02-12 07:10:56 [INFO]   MAE (Absolute S): 0.041522\n",
      "2025-02-12 07:10:56 [INFO]   MAPE (Absolute S): 42.26%\n",
      "2025-02-12 07:10:56 [INFO] Epoch 9/500\n",
      "2025-02-12 07:13:41 [INFO]   Training Loss: 12.786630\n",
      "2025-02-12 07:18:11 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 07:18:11 [INFO]   Loss: 0.193177\n",
      "2025-02-12 07:18:11 [INFO]   MSE (Absolute S): 0.002671\n",
      "2025-02-12 07:18:11 [INFO]   MAE (Absolute S): 0.034645\n",
      "2025-02-12 07:18:11 [INFO]   MAPE (Absolute S): 38.50%\n",
      "2025-02-12 07:18:11 [INFO]   [Info] Best model saved (val_loss=0.193177)\n",
      "2025-02-12 07:18:11 [INFO] Epoch 10/500\n",
      "2025-02-12 07:20:54 [INFO]   Training Loss: 2.952586\n",
      "2025-02-12 07:24:50 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 07:24:50 [INFO]   Loss: 0.199706\n",
      "2025-02-12 07:24:50 [INFO]   MSE (Absolute S): 0.002351\n",
      "2025-02-12 07:24:50 [INFO]   MAE (Absolute S): 0.031987\n",
      "2025-02-12 07:24:50 [INFO]   MAPE (Absolute S): 39.83%\n",
      "2025-02-12 07:24:50 [INFO] Epoch 11/500\n",
      "2025-02-12 07:27:36 [INFO]   Training Loss: 40.318400\n",
      "2025-02-12 07:31:35 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 07:31:35 [INFO]   Loss: 379.564867\n",
      "2025-02-12 07:31:35 [INFO]   MSE (Absolute S): 0.005803\n",
      "2025-02-12 07:31:35 [INFO]   MAE (Absolute S): 0.052321\n",
      "2025-02-12 07:31:35 [INFO]   MAPE (Absolute S): 76698.21%\n",
      "2025-02-12 07:31:35 [INFO] Epoch 12/500\n",
      "2025-02-12 07:34:19 [INFO]   Training Loss: 0.624925\n",
      "2025-02-12 07:38:28 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 07:38:28 [INFO]   Loss: 0.204405\n",
      "2025-02-12 07:38:28 [INFO]   MSE (Absolute S): 0.003866\n",
      "2025-02-12 07:38:28 [INFO]   MAE (Absolute S): 0.041737\n",
      "2025-02-12 07:38:28 [INFO]   MAPE (Absolute S): 40.69%\n",
      "2025-02-12 07:38:28 [INFO] Epoch 13/500\n",
      "2025-02-12 07:41:16 [INFO]   Training Loss: 3.503123\n",
      "2025-02-12 07:45:56 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 07:45:56 [INFO]   Loss: 0.202446\n",
      "2025-02-12 07:45:56 [INFO]   MSE (Absolute S): 0.002193\n",
      "2025-02-12 07:45:56 [INFO]   MAE (Absolute S): 0.032831\n",
      "2025-02-12 07:45:56 [INFO]   MAPE (Absolute S): 40.38%\n",
      "2025-02-12 07:45:56 [INFO] Epoch 14/500\n",
      "2025-02-12 07:48:49 [INFO]   Training Loss: 594.522889\n",
      "2025-02-12 07:52:52 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 07:52:52 [INFO]   Loss: 546577.292267\n",
      "2025-02-12 07:52:52 [INFO]   MSE (Absolute S): 0.007617\n",
      "2025-02-12 07:52:52 [INFO]   MAE (Absolute S): 0.061617\n",
      "2025-02-12 07:52:52 [INFO]   MAPE (Absolute S): 110515137.50%\n",
      "2025-02-12 07:52:52 [INFO] Epoch 15/500\n",
      "2025-02-12 07:55:41 [INFO]   Training Loss: 649.338420\n",
      "2025-02-12 08:00:25 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 08:00:25 [INFO]   Loss: 1222038.085674\n",
      "2025-02-12 08:00:25 [INFO]   MSE (Absolute S): 0.006055\n",
      "2025-02-12 08:00:25 [INFO]   MAE (Absolute S): 0.053612\n",
      "2025-02-12 08:00:25 [INFO]   MAPE (Absolute S): 247087275.00%\n",
      "2025-02-12 08:00:25 [INFO] Epoch 16/500\n",
      "2025-02-12 08:03:18 [INFO]   Training Loss: 106.134329\n",
      "2025-02-12 08:07:43 [INFO] \n",
      "Validation Summary:\n",
      "2025-02-12 08:07:43 [INFO]   Loss: 0.171769\n",
      "2025-02-12 08:07:43 [INFO]   MSE (Absolute S): 0.000580\n",
      "2025-02-12 08:07:43 [INFO]   MAE (Absolute S): 0.013835\n",
      "2025-02-12 08:07:43 [INFO]   MAPE (Absolute S): 34.33%\n",
      "2025-02-12 08:07:44 [INFO]   [Info] Best model saved (val_loss=0.171769)\n",
      "2025-02-12 08:07:44 [INFO] Epoch 17/500\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import dropout_adj\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_geometric.nn import (\n",
    "    GINEConv,\n",
    "    TransformerConv,\n",
    "    Set2Set,\n",
    "    BatchNorm,           # We'll use this for batch normalization\n",
    "    global_add_pool\n",
    ")\n",
    "from torch.utils.data import random_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from torch_geometric.nn import (\n",
    "    GINEConv, TransformerConv, Set2Set, GraphNorm\n",
    ")\n",
    "from torch_geometric.utils import dropout_adj\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Configuration\n",
    "# -----------------------------------------------------------\n",
    "CONFIG = {\n",
    "    'processed_dir': './processed_experimentalrung1',\n",
    "    'processed_file_name': 'data.pt',\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 3e-4,\n",
    "    'weight_decay': 1e-4,\n",
    "    'hidden_channels': 512,\n",
    "    'num_epochs': 500,\n",
    "    'patience': 75,\n",
    "    'random_seed': 42,\n",
    "    'best_model_path': 'best_model_rung1.pth',\n",
    "    'dropout_p': 0.3,\n",
    "    'scheduler_factor': 0.5,\n",
    "    'scheduler_patience': 10,\n",
    "    'grad_clip': 1.0,\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Logging & Utilities\n",
    "# -----------------------------------------------------------\n",
    "def setup_logging():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "        handlers=[logging.StreamHandler(sys.stdout)]\n",
    "    )\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Weight Initialization\n",
    "# -----------------------------------------------------------\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu') \n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.BatchNorm1d):\n",
    "        nn.init.ones_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# SpinSystemDataset\n",
    "# -----------------------------------------------------------\n",
    "class SpinSystemDataset(InMemoryDataset):\n",
    "    \"\"\"Loads the data.pt from processed_dir (old-style).\"\"\"\n",
    "    def __init__(self, root='.', transform=None, pre_transform=None):\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0], weights_only=False)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [CONFIG['processed_file_name']]\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        pass\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# PhysicalScaleAwareLoss (Single-Head)\n",
    "# -----------------------------------------------------------\n",
    "class PhysicalScaleAwareLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    A combined loss that balances:\n",
    "      1) Relative error (to reduce percentage error).\n",
    "      2) A small MSE or log-cosh term for stability.\n",
    "      3) Physics-based bounds as soft constraints.\n",
    "    \"\"\"\n",
    "    def __init__(self, physics_weight=1.0, rel_weight=0.5, eps=1e-10):\n",
    "        super().__init__()\n",
    "        self.physics_weight = physics_weight\n",
    "        self.rel_weight = rel_weight\n",
    "        self.eps = eps\n",
    "\n",
    "    def get_entropy_bounds(self, system_size, subsystem_size):\n",
    "        lower_bound = torch.zeros_like(system_size, dtype=torch.float)\n",
    "        min_size = torch.minimum(subsystem_size.float(), \n",
    "                                 (system_size - subsystem_size).float())\n",
    "        upper_bound = min_size * torch.log(torch.tensor(2.0, \n",
    "                                             device=system_size.device))\n",
    "        return lower_bound, upper_bound\n",
    "\n",
    "    def forward(self, pred_s, target_s, system_size, subsystem_size):\n",
    "        # 1. Base MSE/log-cosh for stability\n",
    "        error = pred_s - target_s\n",
    "        logcosh = torch.log(torch.cosh(error + self.eps))\n",
    "        logcosh_loss = logcosh.mean()\n",
    "\n",
    "        # 2. Relative loss for percentage error\n",
    "        rel_error = torch.abs(error) / (torch.abs(target_s) + self.eps)\n",
    "        rel_loss = rel_error.mean()\n",
    "\n",
    "        # 3. Soft physics-based constraints\n",
    "        lower_bound, upper_bound = self.get_entropy_bounds(system_size, subsystem_size)\n",
    "        bounds_violation = (\n",
    "            torch.relu(lower_bound - pred_s) +\n",
    "            torch.relu(pred_s - upper_bound)\n",
    "        ).mean()\n",
    "\n",
    "        total_loss = (1 - self.rel_weight) * logcosh_loss + \\\n",
    "                     self.rel_weight * rel_loss + \\\n",
    "                     self.physics_weight * bounds_violation\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# GNN Model (Single Head: log(S/N)) + Global Features\n",
    "# Using BatchNorm Instead of LayerNorm\n",
    "# -----------------------------------------------------------\n",
    "class ExperimentalGNN(nn.Module):\n",
    "    def __init__(self, hidden_channels=64, num_layers=6, dropout_p=0.4):\n",
    "        super().__init__()\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        # Define feature indices for slicing input features\n",
    "        self.feature_indices = {\n",
    "            'position': slice(0, 2),  # First two columns are position features\n",
    "            'rydberg_val': 2,        # Third column is Rydberg value\n",
    "            'mask': 3,                # Fourth column is mask\n",
    "        }\n",
    "\n",
    "        # Node and Edge Encoders\n",
    "        self.node_encoder = nn.Sequential(\n",
    "            nn.Linear(4, hidden_channels),\n",
    "            BatchNorm(hidden_channels),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(3, hidden_channels),\n",
    "            BatchNorm(hidden_channels),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        # Edge-Attention Layers\n",
    "        self.edge_attention = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_channels, hidden_channels),\n",
    "                BatchNorm(hidden_channels),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(hidden_channels, 1),\n",
    "                nn.Sigmoid()  # Attention weights between 0 and 1\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Edge-Node Co-Processing Layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.edge_convs = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            if i % 2 == 0:\n",
    "                # GINEConv with enhanced edge features\n",
    "                mp_mlp = nn.Sequential(\n",
    "                    nn.Linear(hidden_channels, hidden_channels),\n",
    "                    BatchNorm(hidden_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Dropout(dropout_p),\n",
    "                    nn.Linear(hidden_channels, hidden_channels)\n",
    "                )\n",
    "                conv = GINEConv(mp_mlp, edge_dim=hidden_channels)\n",
    "            else:\n",
    "                # TransformerConv with edge-aware attention\n",
    "                conv = TransformerConv(\n",
    "                    hidden_channels,\n",
    "                    hidden_channels // 8,\n",
    "                    heads=8,\n",
    "                    edge_dim=hidden_channels,\n",
    "                    dropout=dropout_p,\n",
    "                    beta=True,\n",
    "                    concat=True\n",
    "                )\n",
    "            self.convs.append(conv)\n",
    "\n",
    "            # Edge MLP with BatchNorm\n",
    "            self.edge_convs.append(nn.Sequential(\n",
    "                nn.Linear(hidden_channels, hidden_channels),\n",
    "                BatchNorm(hidden_channels),\n",
    "                nn.SiLU(),\n",
    "                nn.Dropout(dropout_p)\n",
    "            ))\n",
    "\n",
    "            # Node features BN after convolution\n",
    "            self.norms.append(BatchNorm(hidden_channels))\n",
    "\n",
    "        # Edge-Preserving Pooling\n",
    "        self.pool = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_channels, hidden_channels),\n",
    "                BatchNorm(hidden_channels),\n",
    "                nn.SiLU(),\n",
    "                nn.Dropout(dropout_p)\n",
    "            ) for _ in range(num_layers // 2)\n",
    "        ])\n",
    "\n",
    "        # Readout with Edge Information\n",
    "        self.readout = nn.ModuleList([\n",
    "            Set2Set(hidden_channels, processing_steps=4) for _ in range(2)  # 2 heads\n",
    "        ])\n",
    "        \n",
    "        # Dimension reduction for readout outputs\n",
    "        self.readout_projection = nn.Sequential(\n",
    "            nn.Linear(4 * hidden_channels, 2 * hidden_channels),  # 4 = 2 heads * 2 (Set2Set doubles dim)\n",
    "            BatchNorm(2 * hidden_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_p)\n",
    "        )\n",
    "\n",
    "        # Global Features MLP\n",
    "        self.global_mlp = nn.Sequential(\n",
    "            nn.Linear(2, hidden_channels),\n",
    "            BatchNorm(hidden_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_channels, hidden_channels)\n",
    "        )\n",
    "\n",
    "        # Final MLP with corrected dimensions\n",
    "        combined_dim = (2 * hidden_channels) + hidden_channels  # projected readout + global features\n",
    "        self.final_mlp = nn.Sequential(\n",
    "            nn.Linear(combined_dim, hidden_channels),\n",
    "            BatchNorm(hidden_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_channels, hidden_channels // 2),\n",
    "            BatchNorm(hidden_channels // 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_channels // 2, 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = (\n",
    "            data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        )\n",
    "\n",
    "        # Node and Edge Feature Encoding\n",
    "        node_features = torch.cat([\n",
    "            x[:, self.feature_indices['position']],\n",
    "            x[:, self.feature_indices['rydberg_val']].unsqueeze(-1),\n",
    "            x[:, self.feature_indices['mask']].unsqueeze(-1),\n",
    "        ], dim=1)\n",
    "\n",
    "        edge_features = edge_attr\n",
    "\n",
    "        x_enc = self.node_encoder(node_features)\n",
    "        e_enc = self.edge_encoder(edge_features)\n",
    "\n",
    "        # Message Passing with Edge-Attention\n",
    "        h = x_enc\n",
    "        for i in range(self.num_layers):\n",
    "            # Compute edge attention weights\n",
    "            edge_weights = self.edge_attention[i](e_enc).squeeze(-1)\n",
    "\n",
    "            # Update edge features\n",
    "            e_enc = self.edge_convs[i](e_enc)\n",
    "\n",
    "            # Perform convolution with edge attention\n",
    "            h_new = self.convs[i](h, edge_index, e_enc * edge_weights.unsqueeze(-1))\n",
    "\n",
    "            # BatchNorm on node outputs\n",
    "            h_new = self.norms[i](h_new)\n",
    "\n",
    "            # Residual connection\n",
    "            h = h + h_new\n",
    "\n",
    "            # Edge-Preserving Pooling\n",
    "            if i % 2 == 0 and i // 2 < len(self.pool):\n",
    "                h = self.pool[i // 2](h)\n",
    "\n",
    "        # Multi-Head Readout with dimension reduction\n",
    "        readouts = [rd(h, batch) for rd in self.readout]\n",
    "        h_readout = torch.cat(readouts, dim=1)\n",
    "        h_readout = self.readout_projection(h_readout)\n",
    "\n",
    "        # Global Features\n",
    "        nA_over_N = data.nA.squeeze(-1) / (data.system_size.squeeze(-1) + 1e-10)\n",
    "        nB_over_N = data.nB.squeeze(-1) / (data.system_size.squeeze(-1) + 1e-10)\n",
    "        global_feats = torch.stack([nA_over_N, nB_over_N], dim=1)\n",
    "        gf_out = self.global_mlp(global_feats)\n",
    "\n",
    "        # Combine features with correct dimensions\n",
    "        combined = torch.cat([h_readout, gf_out], dim=1)\n",
    "        out = self.final_mlp(combined).squeeze(-1)\n",
    "        return out\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Training & Evaluation\n",
    "# -----------------------------------------------------------\n",
    "def train_epoch(model, loader, optimizer, criterion, device, clip_grad=None):\n",
    "    model.train()  # BatchNorm will track running stats here\n",
    "    total_loss = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred_s = model(data)\n",
    "        targets = data.y.squeeze()\n",
    "        system_size = data.system_size.squeeze(-1)\n",
    "        subsystem_size = data.nA.squeeze(-1)\n",
    "\n",
    "        loss = criterion(pred_s, targets, system_size, subsystem_size)\n",
    "        loss.backward()\n",
    "\n",
    "        if clip_grad is not None:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = data.num_graphs\n",
    "        n_samples += batch_size\n",
    "        total_loss += loss.item() * batch_size\n",
    "\n",
    "    return total_loss / n_samples if n_samples > 0 else 0.0\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device, name='Eval'):\n",
    "    model.eval()  # BatchNorm uses stored running_mean/var here\n",
    "    total_loss = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    all_preds_abs = []\n",
    "    all_targets = []\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred_s = model(data)\n",
    "\n",
    "        targets = data.y.squeeze()\n",
    "        system_size = data.system_size.squeeze(-1)\n",
    "        subsystem_size = data.nA.squeeze(-1)\n",
    "\n",
    "        loss = criterion(pred_s, targets, system_size, subsystem_size)\n",
    "        batch_size = data.num_graphs\n",
    "        total_loss += loss.item() * batch_size\n",
    "        n_samples += batch_size\n",
    "\n",
    "        all_preds_abs.append(pred_s.cpu())\n",
    "        all_targets.append(targets.cpu())\n",
    "\n",
    "    mean_loss = total_loss / n_samples if n_samples > 0 else 0.0\n",
    "\n",
    "    all_preds_abs = torch.cat(all_preds_abs).numpy()\n",
    "    all_targets = torch.cat(all_targets).numpy()\n",
    "\n",
    "    mse_abs = mean_squared_error(all_targets, all_preds_abs)\n",
    "    mae_abs = mean_absolute_error(all_targets, all_preds_abs)\n",
    "    mape_abs = np.mean(np.abs((all_preds_abs - all_targets) / (all_targets + 1e-10))) * 100\n",
    "\n",
    "    logging.info(f\"\\n{name} Summary:\")\n",
    "    logging.info(f\"  Loss: {mean_loss:.6f}\")\n",
    "    logging.info(f\"  MSE (Absolute S): {mse_abs:.6f}\")\n",
    "    logging.info(f\"  MAE (Absolute S): {mae_abs:.6f}\")\n",
    "    logging.info(f\"  MAPE (Absolute S): {mape_abs:.2f}%\")\n",
    "\n",
    "    return mean_loss\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Main\n",
    "# -----------------------------------------------------------\n",
    "def main():\n",
    "    setup_logging()\n",
    "    set_seed(CONFIG['random_seed'])\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = SpinSystemDataset(root=CONFIG['processed_dir'])\n",
    "    if len(dataset) == 0:\n",
    "        logging.error(\"Loaded dataset is empty. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Train/Val split\n",
    "    train_size = int(0.1 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        dataset,\n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(CONFIG['random_seed'])\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "    # Initialize the model (now using BatchNorm)\n",
    "    model = ExperimentalGNN(\n",
    "        hidden_channels=CONFIG['hidden_channels'],\n",
    "        dropout_p=CONFIG['dropout_p']\n",
    "    ).to(device)\n",
    "\n",
    "    # Apply weight initialization\n",
    "    model.apply(init_weights)\n",
    "\n",
    "    criterion = PhysicalScaleAwareLoss(physics_weight=2.0)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=CONFIG['learning_rate'],\n",
    "        weight_decay=CONFIG['weight_decay']\n",
    "    )\n",
    "\n",
    "    # Use Cosine Annealing with Warm Restarts\n",
    "    scheduler = CosineAnnealingWarmRestarts(\n",
    "        optimizer,\n",
    "        T_0=50,  # Number of iterations for the first restart\n",
    "        T_mult=2,  # A factor increases T_0 after a restart\n",
    "        eta_min=1e-6,  # Minimum learning rate\n",
    "    )\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(CONFIG['num_epochs']):\n",
    "        logging.info(f\"Epoch {epoch+1}/{CONFIG['num_epochs']}\")\n",
    "\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device, clip_grad=CONFIG['grad_clip'])\n",
    "        logging.info(f\"  Training Loss: {train_loss:.6f}\")\n",
    "\n",
    "        val_loss = evaluate(model, val_loader, criterion, device, name='Validation')\n",
    "        scheduler.step()  # Step the scheduler after each epoch\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), CONFIG['best_model_path'])\n",
    "            logging.info(f\"  [Info] Best model saved (val_loss={best_val_loss:.6f})\")\n",
    "\n",
    "    logging.info(\"Training complete. Loading best model for final validation...\")\n",
    "    model.load_state_dict(torch.load(CONFIG['best_model_path'], map_location=device))\n",
    "    model.eval()  # Ensure BatchNorm uses stored running stats\n",
    "    _ = evaluate(model, val_loader, criterion, device, name='Final Validation')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1582d3a-d211-4c40-a877-0d637669a7f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
