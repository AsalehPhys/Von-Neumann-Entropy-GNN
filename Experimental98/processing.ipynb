{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47c34303-84a2-4935-88a0-6fc112ed2c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "2025-01-22 03:42:37,574 [INFO] Processed 20000 rows so far...\n",
      "2025-01-22 03:43:09,388 [INFO] Processed 40000 rows so far...\n",
      "2025-01-22 03:43:25,983 [INFO] \n",
      "Final statistics:\n",
      "2025-01-22 03:43:25,984 [INFO] Total samples processed: 50000\n",
      "2025-01-22 03:43:25,984 [INFO] Samples kept: 50000\n",
      "Done!\n",
      "2025-01-22 03:43:27,596 [INFO] Loading existing processed dataset...\n",
      "C:\\Users\\amssa\\AppData\\Local\\Temp\\ipykernel_22096\\660904978.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, self.slices = torch.load(self.processed_paths[0])\n",
      "2025-01-22 03:43:27,625 [INFO] Finished processing. Dataset length: 50000\n",
      "2025-01-22 03:43:27,627 [INFO] Sample data object: Data(x=[6, 5], edge_index=[2, 15], edge_attr=[15, 3], y=[1], system_size=[1, 1], total_rydberg=[1], rydberg_density=[1], config_entropy=[1, 1], nA=[1, 1], nB=[1, 1])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'data_paths': [\n",
    "        'Rydbergtest.parquet',\n",
    "    ],\n",
    "    'processed_dir': './processed_experimental_test',\n",
    "    'processed_file_name': 'data.pt',\n",
    "    'distance_threshold': 25,   # example threshold for edges\n",
    "    'random_seed': 42,\n",
    "}\n",
    "\n",
    "def setup_logging():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        handlers=[logging.StreamHandler()]\n",
    "    )\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "class ExperimentalSpinSystemDataset(InMemoryDataset):\n",
    "    def __init__(self, dataframe, root='.', transform=None, pre_transform=None):\n",
    "        self.df = dataframe\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        if os.path.exists(self.processed_paths[0]):\n",
    "            logging.info(\"Loading existing processed dataset...\")\n",
    "            self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        else:\n",
    "            logging.info(\"Processing dataset from scratch...\")\n",
    "            self.process()\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [CONFIG['processed_file_name']]\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        data_list = []\n",
    "        total_count = 0\n",
    "\n",
    "        for idx, row in self.df.iterrows():\n",
    "            total_count += 1\n",
    "            \n",
    "            # Basic system parameters\n",
    "            Nx = row['Nx']\n",
    "            Ny = 2\n",
    "            N = Nx * Ny\n",
    "\n",
    "            # Build Node Features\n",
    "            x_spacing = row['x_spacing']\n",
    "            y_spacing = row['y_spacing']\n",
    "            min_spacing = min(x_spacing, y_spacing)\n",
    "            positions = np.array([\n",
    "                (col * x_spacing/min_spacing, row_idx * y_spacing/min_spacing)\n",
    "                for row_idx in range(Nx) for col in range(Ny)\n",
    "            ], dtype=np.float32)\n",
    "            positions_t = torch.tensor(positions, dtype=torch.float)\n",
    "\n",
    "            # Rebuild Rydberg excitations\n",
    "            top_indices = row['Top_Indices']\n",
    "            top_probs = row['Top_Probabilities']\n",
    "            \n",
    "            p_rydberg = torch.zeros(N, dtype=torch.float)\n",
    "            for state, prob in zip(top_indices, top_probs):\n",
    "                state = int(state)\n",
    "                for i_site in range(N):\n",
    "                    if (state & (1 << i_site)) != 0:\n",
    "                        p_rydberg[i_site] += prob\n",
    "            p_rydberg = p_rydberg.unsqueeze(1)\n",
    "\n",
    "            # Subsystem mask\n",
    "            subsystem_mask_str = row['Subsystem_Mask']\n",
    "            mask_tensor = torch.tensor([int(bit) for bit in subsystem_mask_str],\n",
    "                                     dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "            # Local features\n",
    "            boundary_dist = torch.zeros(N, 1)\n",
    "            for i in range(N):\n",
    "                mask_i = int(subsystem_mask_str[i])\n",
    "                min_dist = N\n",
    "                for j in range(N):\n",
    "                    if int(subsystem_mask_str[j]) != mask_i:\n",
    "                        dist = abs(i - j)\n",
    "                        min_dist = min(min_dist, dist)\n",
    "                boundary_dist[i] = min_dist\n",
    "\n",
    "            # Combine node features\n",
    "            node_features = torch.cat([\n",
    "                positions_t,      # 2 (geometric structure)\n",
    "                p_rydberg,       # 1 (experimental)\n",
    "                mask_tensor,     # 1 (partition info)\n",
    "                boundary_dist,   # 1 (geometric)\n",
    "            ], dim=1)\n",
    "\n",
    "            # Build Edges\n",
    "            nbrs = NearestNeighbors(radius=CONFIG['distance_threshold'], \n",
    "                                   algorithm='ball_tree').fit(positions)\n",
    "            indices = nbrs.radius_neighbors(positions, return_distance=False)\n",
    "\n",
    "            edges = []\n",
    "            for i_node in range(N):\n",
    "                for j_node in indices[i_node]:\n",
    "                    if i_node < j_node:\n",
    "                        edges.append((i_node, j_node))\n",
    "                        \n",
    "            if len(edges) == 0:\n",
    "                edge_index = torch.empty((2,0), dtype=torch.long)\n",
    "                edge_attr = torch.empty((0,3), dtype=torch.float)\n",
    "            else:\n",
    "                edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "                pos_i = positions_t[edge_index[0]]\n",
    "                pos_j = positions_t[edge_index[1]]\n",
    "                vec_ij = pos_j - pos_i\n",
    "                dist_ij = torch.norm(vec_ij, dim=1, keepdim=True)\n",
    "                angle_ij = torch.atan2(vec_ij[:,1], vec_ij[:,0]).unsqueeze(1)\n",
    "                \n",
    "                p_rydberg_i = p_rydberg[edge_index[0]]\n",
    "                p_rydberg_j = p_rydberg[edge_index[1]]\n",
    "                correlations = (p_rydberg_i * p_rydberg_j)\n",
    "\n",
    "                edge_attr = torch.cat([\n",
    "                    angle_ij,     # [E,1] geometric\n",
    "                    correlations, # [E,1] experimental\n",
    "                    dist_ij      # [E,1] normalized distances\n",
    "                ], dim=1)\n",
    "\n",
    "            # Create Data object\n",
    "            target_vne = torch.tensor([row['Von_Neumann_Entropy']], dtype=torch.float)\n",
    "            data = Data(\n",
    "                x=node_features,\n",
    "                edge_index=edge_index,\n",
    "                edge_attr=edge_attr,\n",
    "                y=target_vne,\n",
    "            )\n",
    "\n",
    "            # Add remaining properties\n",
    "            data.system_size = torch.tensor([[N]], dtype=torch.float)\n",
    "            total_ryd = p_rydberg.sum()\n",
    "            data.total_rydberg = total_ryd\n",
    "            data.rydberg_density = total_ryd / N\n",
    "            data.config_entropy = torch.tensor([[row['Von_Neumann_Entropy']]], dtype=torch.float)\n",
    "            \n",
    "            nA_val = float(mask_tensor.sum().item())\n",
    "            nB_val = N - nA_val\n",
    "            data.nA = torch.tensor([[nA_val]], dtype=torch.float)\n",
    "            data.nB = torch.tensor([[nB_val]], dtype=torch.float)\n",
    "\n",
    "            data_list.append(data)\n",
    "\n",
    "            if (idx+1) % 20000 == 0:\n",
    "                logging.info(f\"Processed {idx+1} rows so far...\")\n",
    "\n",
    "        # Final statistics\n",
    "        logging.info(f\"\\nFinal statistics:\")\n",
    "        logging.info(f\"Total samples processed: {total_count}\")\n",
    "        logging.info(f\"Samples kept: {len(data_list)}\")\n",
    "\n",
    "        # Save dataset\n",
    "        data_obj, slices = self.collate(data_list)\n",
    "        torch.save((data_obj, slices), self.processed_paths[0])\n",
    "        self.data, self.slices = data_obj, slices\n",
    "\n",
    "def load_data():\n",
    "    df_list = []\n",
    "    for path in CONFIG['data_paths']:\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Data file not found at {path}\")\n",
    "        df_temp = pq.read_table(path).to_pandas()\n",
    "        df_list.append(df_temp)\n",
    "\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    df_shuffled = df.sample(frac=1, random_state=CONFIG['random_seed']).reset_index(drop=True)\n",
    "    return ExperimentalSpinSystemDataset(dataframe=df_shuffled, root=CONFIG['processed_dir'])\n",
    "\n",
    "def main():\n",
    "    setup_logging()\n",
    "    set_seed(CONFIG['random_seed'])\n",
    "    dataset = load_data()\n",
    "    logging.info(f\"Finished processing. Dataset length: {len(dataset)}\")\n",
    "    logging.info(f\"Sample data object: {dataset[0]}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0fb4ac-79fd-4b26-ac7c-4983aac5684c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
