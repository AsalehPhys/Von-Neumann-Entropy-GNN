{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d19b3b-7df8-4a1f-8d13-eee791ac81aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amssa\\anaconda3\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "C:\\Users\\amssa\\AppData\\Local\\Temp\\ipykernel_32828\\314861059.py:174: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 227\u001b[0m\n\u001b[0;32m    224\u001b[0m     create_diagnostic_plots(diagnostics, save_plots\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, save_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiagnostic_plots\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 227\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[4], line 224\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    221\u001b[0m model_path \u001b[38;5;241m=\u001b[39m CONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    223\u001b[0m diagnostics \u001b[38;5;241m=\u001b[39m process_directory(data_dir, model_path)\n\u001b[1;32m--> 224\u001b[0m create_diagnostic_plots(diagnostics, save_plots\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, save_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiagnostic_plots\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Documents\\Codes\\New\\Von-Neumann-Entropy-GNN\\Experimental98\\diagnostic_plots.py:155\u001b[0m, in \u001b[0;36mcreate_diagnostic_plots\u001b[1;34m(diagnostics, save_plots, save_dir)\u001b[0m\n\u001b[0;32m    152\u001b[0m axes[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResiduals vs Predictions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# Relative Error Distribution\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m sns\u001b[38;5;241m.\u001b[39mhistplot(rel_error, kde\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, ax\u001b[38;5;241m=\u001b[39maxes[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m], color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m)\n\u001b[0;32m    156\u001b[0m axes[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRelative Error Distribution\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    157\u001b[0m axes[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mset_xlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRelative Error (\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\seaborn\\distributions.py:1416\u001b[0m, in \u001b[0;36mhistplot\u001b[1;34m(data, x, y, hue, weights, stat, bins, binwidth, binrange, discrete, cumulative, common_bins, common_norm, multiple, element, fill, shrink, kde, kde_kws, line_kws, thresh, pthresh, pmax, cbar, cbar_ax, cbar_kws, palette, hue_order, hue_norm, color, log_scale, legend, ax, **kwargs)\u001b[0m\n\u001b[0;32m   1405\u001b[0m estimate_kws \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m   1406\u001b[0m     stat\u001b[38;5;241m=\u001b[39mstat,\n\u001b[0;32m   1407\u001b[0m     bins\u001b[38;5;241m=\u001b[39mbins,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1411\u001b[0m     cumulative\u001b[38;5;241m=\u001b[39mcumulative,\n\u001b[0;32m   1412\u001b[0m )\n\u001b[0;32m   1414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39munivariate:\n\u001b[1;32m-> 1416\u001b[0m     p\u001b[38;5;241m.\u001b[39mplot_univariate_histogram(\n\u001b[0;32m   1417\u001b[0m         multiple\u001b[38;5;241m=\u001b[39mmultiple,\n\u001b[0;32m   1418\u001b[0m         element\u001b[38;5;241m=\u001b[39melement,\n\u001b[0;32m   1419\u001b[0m         fill\u001b[38;5;241m=\u001b[39mfill,\n\u001b[0;32m   1420\u001b[0m         shrink\u001b[38;5;241m=\u001b[39mshrink,\n\u001b[0;32m   1421\u001b[0m         common_norm\u001b[38;5;241m=\u001b[39mcommon_norm,\n\u001b[0;32m   1422\u001b[0m         common_bins\u001b[38;5;241m=\u001b[39mcommon_bins,\n\u001b[0;32m   1423\u001b[0m         kde\u001b[38;5;241m=\u001b[39mkde,\n\u001b[0;32m   1424\u001b[0m         kde_kws\u001b[38;5;241m=\u001b[39mkde_kws,\n\u001b[0;32m   1425\u001b[0m         color\u001b[38;5;241m=\u001b[39mcolor,\n\u001b[0;32m   1426\u001b[0m         legend\u001b[38;5;241m=\u001b[39mlegend,\n\u001b[0;32m   1427\u001b[0m         estimate_kws\u001b[38;5;241m=\u001b[39mestimate_kws,\n\u001b[0;32m   1428\u001b[0m         line_kws\u001b[38;5;241m=\u001b[39mline_kws,\n\u001b[0;32m   1429\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1430\u001b[0m     )\n\u001b[0;32m   1432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1434\u001b[0m     p\u001b[38;5;241m.\u001b[39mplot_bivariate_histogram(\n\u001b[0;32m   1435\u001b[0m         common_bins\u001b[38;5;241m=\u001b[39mcommon_bins,\n\u001b[0;32m   1436\u001b[0m         common_norm\u001b[38;5;241m=\u001b[39mcommon_norm,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1446\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1447\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\seaborn\\distributions.py:571\u001b[0m, in \u001b[0;36m_DistributionPlotter.plot_univariate_histogram\u001b[1;34m(self, multiple, element, fill, common_norm, common_bins, shrink, kde, kde_kws, color, legend, line_kws, estimate_kws, **plot_kws)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m element \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbars\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    567\u001b[0m \n\u001b[0;32m    568\u001b[0m     \u001b[38;5;66;03m# Use matplotlib bar plotting\u001b[39;00m\n\u001b[0;32m    570\u001b[0m     plot_func \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39mbar \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_variable \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m ax\u001b[38;5;241m.\u001b[39mbarh\n\u001b[1;32m--> 571\u001b[0m     artists \u001b[38;5;241m=\u001b[39m plot_func(\n\u001b[0;32m    572\u001b[0m         hist[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medges\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    573\u001b[0m         hist[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheights\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m bottom,\n\u001b[0;32m    574\u001b[0m         hist[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidths\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    575\u001b[0m         bottom,\n\u001b[0;32m    576\u001b[0m         align\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    577\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39martist_kws,\n\u001b[0;32m    578\u001b[0m     )\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m bar \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_variable \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\__init__.py:1473\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1473\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\n\u001b[0;32m   1474\u001b[0m             ax,\n\u001b[0;32m   1475\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(sanitize_sequence, args),\n\u001b[0;32m   1476\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: sanitize_sequence(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()})\n\u001b[0;32m   1478\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1479\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1480\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:2597\u001b[0m, in \u001b[0;36mAxes.bar\u001b[1;34m(self, x, height, width, bottom, align, **kwargs)\u001b[0m\n\u001b[0;32m   2595\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# horizontal\u001b[39;00m\n\u001b[0;32m   2596\u001b[0m         r\u001b[38;5;241m.\u001b[39msticky_edges\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mappend(l)\n\u001b[1;32m-> 2597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_patch(r)\n\u001b[0;32m   2598\u001b[0m     patches\u001b[38;5;241m.\u001b[39mappend(r)\n\u001b[0;32m   2600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xerr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m yerr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_base.py:2414\u001b[0m, in \u001b[0;36m_AxesBase.add_patch\u001b[1;34m(self, p)\u001b[0m\n\u001b[0;32m   2412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2413\u001b[0m     p\u001b[38;5;241m.\u001b[39mset_clip_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch)\n\u001b[1;32m-> 2414\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_patch_limits(p)\n\u001b[0;32m   2415\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_children\u001b[38;5;241m.\u001b[39mappend(p)\n\u001b[0;32m   2416\u001b[0m p\u001b[38;5;241m.\u001b[39m_remove_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_children\u001b[38;5;241m.\u001b[39mremove\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_base.py:2455\u001b[0m, in \u001b[0;36m_AxesBase._update_patch_limits\u001b[1;34m(self, patch)\u001b[0m\n\u001b[0;32m   2453\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m updatey \u001b[38;5;129;01mand\u001b[39;00m patch_trf \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xaxis_transform():\n\u001b[0;32m   2454\u001b[0m         updatey \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 2455\u001b[0m trf_to_data \u001b[38;5;241m=\u001b[39m patch_trf \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransData\n\u001b[0;32m   2456\u001b[0m xys \u001b[38;5;241m=\u001b[39m trf_to_data\u001b[38;5;241m.\u001b[39mtransform(vertices)\n\u001b[0;32m   2457\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_datalim(xys, updatex\u001b[38;5;241m=\u001b[39mupdatex, updatey\u001b[38;5;241m=\u001b[39mupdatey)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\transforms.py:1460\u001b[0m, in \u001b[0;36mTransform.__sub__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, Transform):\n\u001b[0;32m   1459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m-> 1460\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m remainder, sub_tree \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_break_from_left_to_right():\n\u001b[0;32m   1461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sub_tree \u001b[38;5;241m==\u001b[39m other:\n\u001b[0;32m   1462\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m remainder\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\transforms.py:2402\u001b[0m, in \u001b[0;36mCompositeGenericTransform._iter_break_from_left_to_right\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_iter_break_from_left_to_right\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 2402\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m left, right \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_a\u001b[38;5;241m.\u001b[39m_iter_break_from_left_to_right():\n\u001b[0;32m   2403\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m left, right \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_b\n\u001b[0;32m   2404\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m left, right \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_b\u001b[38;5;241m.\u001b[39m_iter_break_from_left_to_right():\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\transforms.py:2403\u001b[0m, in \u001b[0;36mCompositeGenericTransform._iter_break_from_left_to_right\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_iter_break_from_left_to_right\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   2402\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m left, right \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_a\u001b[38;5;241m.\u001b[39m_iter_break_from_left_to_right():\n\u001b[1;32m-> 2403\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m left, right \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_b\n\u001b[0;32m   2404\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m left, right \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_b\u001b[38;5;241m.\u001b[39m_iter_break_from_left_to_right():\n\u001b[0;32m   2405\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_a \u001b[38;5;241m+\u001b[39m left, right\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\transforms.py:1357\u001b[0m, in \u001b[0;36mTransform.__add__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1350\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__add__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m   1351\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1352\u001b[0m \u001b[38;5;124;03m    Compose two transforms together so that *self* is followed by *other*.\u001b[39;00m\n\u001b[0;32m   1353\u001b[0m \n\u001b[0;32m   1354\u001b[0m \u001b[38;5;124;03m    ``A + B`` returns a transform ``C`` so that\u001b[39;00m\n\u001b[0;32m   1355\u001b[0m \u001b[38;5;124;03m    ``C.transform(x) == B.transform(A.transform(x))``.\u001b[39;00m\n\u001b[0;32m   1356\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (composite_transform_factory(\u001b[38;5;28mself\u001b[39m, other)\n\u001b[0;32m   1358\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, Transform) \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[0;32m   1359\u001b[0m             \u001b[38;5;28mNotImplemented\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\transforms.py:2541\u001b[0m, in \u001b[0;36mcomposite_transform_factory\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m   2539\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, Affine2D) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(b, Affine2D):\n\u001b[0;32m   2540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CompositeAffine2D(a, b)\n\u001b[1;32m-> 2541\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompositeGenericTransform(a, b)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\transforms.py:2373\u001b[0m, in \u001b[0;36mCompositeGenericTransform.__init__\u001b[1;34m(self, a, b, **kwargs)\u001b[0m\n\u001b[0;32m   2370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dims \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39minput_dims\n\u001b[0;32m   2371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dims \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39moutput_dims\n\u001b[1;32m-> 2373\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_a \u001b[38;5;241m=\u001b[39m a\n\u001b[0;32m   2375\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_b \u001b[38;5;241m=\u001b[39m b\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import logging\n",
    "import glob\n",
    "from torch_geometric.data import Data, Batch\n",
    "from collections import defaultdict\n",
    "\n",
    "# Import the model definition and plotting utilities\n",
    "from Train_exp import ExperimentalGNN, CONFIG\n",
    "from diagnostic_plots import create_diagnostic_plots  # Import the plotting function\n",
    "\n",
    "def setup_logging():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        handlers=[logging.StreamHandler()]\n",
    "    )\n",
    "\n",
    "def parse_binary_file(filepath):\n",
    "    \"\"\"Parse a file containing binary strings and their counts.\"\"\"\n",
    "    states_dict = {}\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                binary_str, count = line.strip().split()\n",
    "                states_dict[binary_str] = int(count)\n",
    "    return states_dict\n",
    "\n",
    "def calculate_probabilities(states_dict):\n",
    "    \"\"\"Calculate normalized probabilities from state counts.\"\"\"\n",
    "    total_count = sum(states_dict.values())\n",
    "    return {state: count/total_count for state, count in states_dict.items()}\n",
    "\n",
    "def binary_to_rydberg_probabilities(binary_states, N):\n",
    "    \"\"\"Convert binary states to per-site Rydberg probabilities.\"\"\"\n",
    "    p_rydberg = np.zeros(N)\n",
    "    total_prob = 0\n",
    "    \n",
    "    for state, prob in binary_states.items():\n",
    "        state_int = int(state, 2)\n",
    "        total_prob += prob\n",
    "        for i in range(N):\n",
    "            if (state_int & (1 << i)) != 0:\n",
    "                p_rydberg[i] += prob\n",
    "                \n",
    "    return p_rydberg\n",
    "\n",
    "def reshape_for_subsystem(psi, A_indices, N):\n",
    "    \"\"\"Reshape wavefunction for bipartition.\"\"\"\n",
    "    A_indices = sorted(A_indices)\n",
    "    B_indices = sorted(i for i in range(N) if i not in A_indices)\n",
    "    N_A = len(A_indices)\n",
    "    N_B = N - N_A\n",
    "    \n",
    "    psi_reshaped = np.zeros((2**N_A, 2**N_B), dtype=np.complex128)\n",
    "    \n",
    "    for state_str, amplitude in psi.items():\n",
    "        state_int = int(state_str, 2)\n",
    "        i_A = 0\n",
    "        i_B = 0\n",
    "        \n",
    "        for i in range(N):\n",
    "            bit = (state_int >> i) & 1\n",
    "            if i in A_indices:\n",
    "                i_A |= bit << A_indices.index(i)\n",
    "            else:\n",
    "                i_B |= bit << B_indices.index(i)\n",
    "                \n",
    "        psi_reshaped[i_A, i_B] = amplitude\n",
    "        \n",
    "    return psi_reshaped\n",
    "\n",
    "def calculate_entropy(binary_states, N):\n",
    "    \"\"\"Calculate von Neumann entropy for a random bipartition.\"\"\"\n",
    "    psi = {state: np.sqrt(prob) for state, prob in binary_states.items()}\n",
    "    \n",
    "    A_size = np.random.randint(1, N)\n",
    "    A_indices = np.random.choice(N, A_size, replace=False)\n",
    "    \n",
    "    subsystem_mask = np.zeros(N, dtype=int)\n",
    "    subsystem_mask[A_indices] = 1\n",
    "    \n",
    "    psi_matrix = reshape_for_subsystem(psi, A_indices, N)\n",
    "    U, s, Vh = np.linalg.svd(psi_matrix, full_matrices=False)\n",
    "    s_squared = s**2\n",
    "    s_squared_normalized = s_squared / np.sum(s_squared)\n",
    "    entropy = -np.sum(s_squared_normalized * np.log(s_squared_normalized + 1e-12))\n",
    "    \n",
    "    return entropy, subsystem_mask\n",
    "\n",
    "def create_graph_data(binary_states, N, x_spacing=6.0, y_spacing=6.0):\n",
    "    \"\"\"Create a graph representation for the GNN.\"\"\"\n",
    "    Nx = N // 2  # Assuming 2 rows\n",
    "    Ny = 2\n",
    "    \n",
    "    p_rydberg = binary_to_rydberg_probabilities(binary_states, N)\n",
    "    entropy, subsystem_mask = calculate_entropy(binary_states, N)\n",
    "    \n",
    "    positions = np.array([\n",
    "        (col * x_spacing, row * y_spacing)\n",
    "        for row in range(Nx) for col in range(Ny)\n",
    "    ], dtype=np.float32)\n",
    "    \n",
    "    boundary_dist = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        mask_i = subsystem_mask[i]\n",
    "        min_dist = N\n",
    "        for j in range(N):\n",
    "            if subsystem_mask[j] != mask_i:\n",
    "                dist = abs(i - j)\n",
    "                min_dist = min(min_dist, dist)\n",
    "        boundary_dist[i] = min_dist\n",
    "    \n",
    "    node_features = torch.tensor(np.column_stack([\n",
    "        positions,\n",
    "        p_rydberg.reshape(-1, 1),\n",
    "        subsystem_mask.reshape(-1, 1),\n",
    "        boundary_dist.reshape(-1, 1)\n",
    "    ]), dtype=torch.float)\n",
    "    \n",
    "    R_cut = 25.0\n",
    "    edges = []\n",
    "    edge_attrs = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(i+1, N):\n",
    "            dx = positions[i, 0] - positions[j, 0]\n",
    "            dy = positions[i, 1] - positions[j, 1]\n",
    "            dist = np.sqrt(dx*dx + dy*dy)\n",
    "            \n",
    "            if dist <= R_cut:\n",
    "                edges.append([i, j])\n",
    "                edges.append([j, i])\n",
    "                \n",
    "                angle = np.arctan2(dy, dx)\n",
    "                correlation = p_rydberg[i] * p_rydberg[j]\n",
    "                \n",
    "                edge_attrs.extend([[angle, correlation, dist],\n",
    "                                 [-angle, correlation, dist]])\n",
    "    \n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t()\n",
    "    edge_attr = torch.tensor(edge_attrs, dtype=torch.float)\n",
    "    \n",
    "    data = Data(\n",
    "        x=node_features,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        y=torch.tensor([entropy], dtype=torch.float),\n",
    "        system_size=torch.tensor([[N]], dtype=torch.float),\n",
    "        total_rydberg=torch.tensor([p_rydberg.sum()], dtype=torch.float),\n",
    "        rydberg_density=torch.tensor([p_rydberg.sum()/N], dtype=torch.float),\n",
    "        config_entropy=torch.tensor([[entropy]], dtype=torch.float),\n",
    "        nA=torch.tensor([[subsystem_mask.sum()]], dtype=torch.float),\n",
    "        nB=torch.tensor([[N - subsystem_mask.sum()]], dtype=torch.float)\n",
    "    )\n",
    "    \n",
    "    return data\n",
    "\n",
    "def process_directory(directory_path, model_path):\n",
    "    \"\"\"Process all txt files in a directory and make predictions with diagnostics.\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load the model\n",
    "    sample_data = create_graph_data({'0'*18: 1}, 18)\n",
    "    model = ExperimentalGNN(\n",
    "        num_node_features=sample_data.x.size(1),\n",
    "        edge_attr_dim=sample_data.edge_attr.size(1),\n",
    "        hidden_channels=CONFIG['hidden_channels'],\n",
    "        num_layers=CONFIG['num_layers'],\n",
    "        dropout_p=CONFIG['dropout_p']\n",
    "    ).to(device)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    targets = []\n",
    "    system_sizes = []\n",
    "    rydberg_densities = []\n",
    "    total_densities = []\n",
    "    \n",
    "    for filepath in glob.glob(os.path.join(directory_path, \"*.txt\")):\n",
    "        try:\n",
    "            states_dict = parse_binary_file(filepath)\n",
    "            N = len(next(iter(states_dict.keys())))\n",
    "            \n",
    "            probabilities = calculate_probabilities(states_dict)\n",
    "            data = create_graph_data(probabilities, N)\n",
    "            data = data.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                pred = model(data)\n",
    "                log_s_over_n = pred[0, 0]\n",
    "                s_over_n = pred[0, 1]\n",
    "                abs_pred = torch.exp(log_s_over_n * data.system_size.squeeze(-1))\n",
    "            \n",
    "            predictions.append(float(abs_pred.cpu()))\n",
    "            targets.append(float(data.y.cpu()))\n",
    "            system_sizes.append(int(data.system_size.cpu()))\n",
    "            rydberg_densities.append(float(data.rydberg_density.cpu()))\n",
    "            total_densities.append(1.0)  # Placeholder for total density\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {filepath}: {str(e)}\")\n",
    "    \n",
    "    diagnostics = {\n",
    "        'predictions': np.array(predictions),\n",
    "        'targets': np.array(targets),\n",
    "        'sizes': np.array(system_sizes),\n",
    "        'rydberg_density': np.array(rydberg_densities),\n",
    "        'total_density': np.array(total_densities)\n",
    "    }\n",
    "    \n",
    "    return diagnostics\n",
    "\n",
    "def main():\n",
    "    setup_logging()\n",
    "    \n",
    "    data_dir = r'C:\\Users\\amssa\\Downloads\\rydberg_test13_NR_2.0_2_6_NS_T_2_\\1'\n",
    "    model_path = CONFIG['best_model_path']\n",
    "    \n",
    "    diagnostics = process_directory(data_dir, model_path)\n",
    "    create_diagnostic_plots(diagnostics, save_plots=True, save_dir=\"diagnostic_plots\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269f6c88-615e-47e9-ac62-c3f5ef85c210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
