{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b7f5227-e101-425d-ae50-cdfe93812b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amssa\\AppData\\Local\\Temp\\ipykernel_21372\\823939866.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, self.slices = torch.load(self.processed_paths[0])\n",
      "C:\\Users\\amssa\\anaconda3\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-22 03:44:22 [INFO] Loading model from 'best_experimental_gnn_98.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amssa\\anaconda3\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "C:\\Users\\amssa\\AppData\\Local\\Temp\\ipykernel_21372\\823939866.py:320: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(CONFIG['best_model_path'], map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-22 03:44:41 [INFO] \n",
      "[Eval] Evaluation metrics:\n",
      "2025-01-22 03:44:41 [INFO]   Size= 2 => MSE=1.3343e-04, MAE=6.8993e-03, MAPE=7.82%\n",
      "2025-01-22 03:44:41 [INFO]   Size= 4 => MSE=1.4363e-04, MAE=5.8343e-03, MAPE=1.94%\n",
      "2025-01-22 03:44:41 [INFO]   Size= 6 => MSE=9.9158e-05, MAE=3.6931e-03, MAPE=3.56%\n",
      "2025-01-22 03:46:29 [INFO] Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-01-22 03:46:29 [INFO] Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "\n",
      "Summary Statistics:\n",
      "Overall RÂ² Score: 0.9981\n",
      "Mean Absolute Error: 5.4805e-03\n",
      "Root Mean Squared Error: 1.1201e-02\n",
      "Mean Absolute Percentage Error: 4.45%\n",
      "\n",
      "Size-specific MAPE:\n",
      "Size 2: 7.82%\n",
      "Size 4: 1.94%\n",
      "Size 6: 3.56%\n",
      "2025-01-22 03:46:33 [INFO] Diagnostics completed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import InMemoryDataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from torch_geometric.nn import (\n",
    "    GINEConv,\n",
    "    GlobalAttention,\n",
    "    Set2Set,\n",
    "    TransformerConv\n",
    ")\n",
    "from torch_geometric.nn.norm import BatchNorm\n",
    "\n",
    "try:\n",
    "    from diagnostic_plots import create_diagnostic_plots\n",
    "except ImportError:\n",
    "    def create_diagnostic_plots(*args, **kwargs):\n",
    "        logging.info(\"diagnostic_plots.py not found; skipping plots.\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Configuration\n",
    "# -------------------------------------------------------------------\n",
    "CONFIG = {\n",
    "    'processed_dir': './processed_experimental_test',\n",
    "    'processed_file_name': 'data.pt',\n",
    "    'batch_size': 1024,\n",
    "    'best_model_path': 'best_experimental_gnn_98.pth',\n",
    "    'hidden_channels': 512,\n",
    "    'num_layers': 10,\n",
    "    'dropout_p': 0.4\n",
    "}\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Logging & Utilities\n",
    "# -------------------------------------------------------------------\n",
    "def setup_logging():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "        handlers=[logging.StreamHandler(sys.stdout)]\n",
    "    )\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Dataset\n",
    "# -------------------------------------------------------------------\n",
    "class SpinSystemDataset(InMemoryDataset):\n",
    "    \"\"\"Loads the experimental dataset.\"\"\"\n",
    "    def __init__(self, root='.', transform=None, pre_transform=None):\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [CONFIG['processed_file_name']]\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        pass\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Experimental GNN Model\n",
    "# -------------------------------------------------------------------\n",
    "class ExperimentalGNN(nn.Module):\n",
    "    \"\"\"GNN using only experimentally accessible quantities.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_node_features,\n",
    "        edge_attr_dim,\n",
    "        hidden_channels=512,\n",
    "        num_layers=10,\n",
    "        dropout_p=0.4\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        # Node embedding\n",
    "        self.init_transform = nn.Sequential(\n",
    "            nn.Linear(num_node_features, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "        # Message Passing\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            if i % 2 == 0:\n",
    "                mp_mlp = nn.Sequential(\n",
    "                    nn.Linear(hidden_channels, hidden_channels),\n",
    "                    nn.LayerNorm(hidden_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Dropout(dropout_p),\n",
    "                    nn.Linear(hidden_channels, hidden_channels)\n",
    "                )\n",
    "                conv = GINEConv(mp_mlp, edge_dim=edge_attr_dim)\n",
    "            else:\n",
    "                conv = TransformerConv(\n",
    "                    hidden_channels, hidden_channels // 4,\n",
    "                    heads=4,\n",
    "                    edge_dim=edge_attr_dim,\n",
    "                    dropout=dropout_p,\n",
    "                    beta=True\n",
    "                )\n",
    "            self.convs.append(conv)\n",
    "            self.norms.append(BatchNorm(hidden_channels))\n",
    "\n",
    "        # Readouts\n",
    "        self.set2set_readout = Set2Set(hidden_channels, processing_steps=4)\n",
    "        self.gate_nn = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels // 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_channels // 2, 1)\n",
    "        )\n",
    "        self.global_attention = GlobalAttention(gate_nn=self.gate_nn)\n",
    "\n",
    "        # Global transform (experimental features)\n",
    "        self.global_transform = nn.Sequential(\n",
    "            nn.Linear(7, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_channels, hidden_channels)\n",
    "        )\n",
    "\n",
    "        # Size encoding\n",
    "        self.size_encoder = nn.Sequential(\n",
    "            nn.Linear(1, hidden_channels // 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_channels // 2, hidden_channels // 2)\n",
    "        )\n",
    "\n",
    "        # Final MLP\n",
    "        combined_in_dim = (2 * hidden_channels) + hidden_channels + hidden_channels + (hidden_channels // 2)\n",
    "        self.final_mlp = nn.Sequential(\n",
    "            nn.Linear(combined_in_dim, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_channels, hidden_channels // 2),\n",
    "            nn.LayerNorm(hidden_channels // 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_channels // 2, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        # Node embedding\n",
    "        h = self.init_transform(x)\n",
    "        for i in range(self.num_layers):\n",
    "            h_new = self.convs[i](h, edge_index, edge_attr)\n",
    "            h_new = self.norms[i](h_new)\n",
    "            h = h + h_new\n",
    "\n",
    "        # Graph-level readouts\n",
    "        s2s = self.set2set_readout(h, batch)\n",
    "        ga = self.global_attention(h, batch)\n",
    "\n",
    "        # Experimental global features\n",
    "        system_size = data.system_size.squeeze(-1)\n",
    "        total_ryd = data.total_rydberg\n",
    "        dens_ryd = data.rydberg_density\n",
    "        config_ent = data.config_entropy.squeeze(-1)\n",
    "        rel_ent = config_ent / torch.log(system_size + 1e-6)\n",
    "        nA = data.nA.squeeze(-1)\n",
    "        nB = data.nB.squeeze(-1)\n",
    "\n",
    "        global_feats = torch.stack([\n",
    "            (total_ryd / system_size),\n",
    "            dens_ryd / system_size,\n",
    "            system_size,\n",
    "            config_ent / system_size,\n",
    "            rel_ent,\n",
    "            nA / system_size,\n",
    "            nB / system_size,\n",
    "        ], dim=1)\n",
    "\n",
    "        gf_out = self.global_transform(global_feats)\n",
    "        size_encoded = self.size_encoder(system_size.unsqueeze(-1))\n",
    "        combined = torch.cat([s2s, ga, gf_out, size_encoded], dim=-1)\n",
    "        out = self.final_mlp(combined)\n",
    "        return out\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Evaluation Function\n",
    "# -------------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, name='Eval'):\n",
    "    \"\"\"Evaluates model and returns metrics compatible with diagnostic_plots.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    all_preds_abs = []\n",
    "    all_preds_over_n = []\n",
    "    all_targets = []\n",
    "    all_sizes = []\n",
    "    all_rydberg = []\n",
    "    all_density = []\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        preds = model(data)\n",
    "\n",
    "        # Get predictions\n",
    "        log_s_over_n = preds[:, 0]\n",
    "        s_over_n = preds[:, 1]\n",
    "        abs_pred = torch.exp(log_s_over_n * data.system_size.squeeze(-1))\n",
    "\n",
    "        # Store results\n",
    "        all_preds_abs.append(abs_pred.cpu())\n",
    "        all_preds_over_n.append(s_over_n.cpu())\n",
    "        all_targets.append(data.y.squeeze().cpu())\n",
    "        all_sizes.append(data.system_size.squeeze().cpu())\n",
    "        all_rydberg.append(data.rydberg_density.cpu())\n",
    "        all_density.append(data.total_rydberg.cpu())\n",
    "\n",
    "    # Concatenate everything\n",
    "    predictions = torch.cat(all_preds_abs).numpy()\n",
    "    predictions_over_n = torch.cat(all_preds_over_n).numpy()\n",
    "    targets = torch.cat(all_targets).numpy()\n",
    "    sizes = torch.cat(all_sizes).numpy()\n",
    "    rydberg = torch.cat(all_rydberg).numpy()\n",
    "    density = torch.cat(all_density).numpy()\n",
    "\n",
    "    # Compute size-specific metrics\n",
    "    unique_sizes = np.unique(sizes)\n",
    "    size_metrics = {}\n",
    "    for sz in unique_sizes:\n",
    "        mask = (sizes == sz)\n",
    "        mse_ = mean_squared_error(targets[mask], predictions[mask])\n",
    "        mae_ = mean_absolute_error(targets[mask], predictions[mask])\n",
    "        mape_ = np.mean(np.abs(predictions[mask] - targets[mask]) / (targets[mask] + 1e-10)) * 100\n",
    "        size_metrics[int(sz)] = dict(mse=mse_, mae=mae_, mape=mape_)\n",
    "\n",
    "    logging.info(f\"\\n[{name}] Evaluation metrics:\")\n",
    "    for sz, met in sorted(size_metrics.items()):\n",
    "        logging.info(\n",
    "            f\"  Size={sz:2d} => MSE={met['mse']:.4e}, \"\n",
    "            f\"MAE={met['mae']:.4e}, MAPE={met['mape']:.2f}%\"\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'predictions_over_n': predictions_over_n,\n",
    "        'targets': targets,\n",
    "        'sizes': sizes,\n",
    "        'rydberg_density': rydberg,\n",
    "        'total_density': density,\n",
    "        'size_metrics': size_metrics\n",
    "    }\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Main Diagnostic Function\n",
    "# -------------------------------------------------------------------\n",
    "def run_diagnostics(N=5, save_plots=True, save_dir=\"plots\"):\n",
    "    \"\"\"Runs model evaluation and creates diagnostic plots.\"\"\"\n",
    "    setup_logging()\n",
    "    set_seed(42)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = SpinSystemDataset(root=CONFIG['processed_dir'])\n",
    "    if len(dataset) == 0:\n",
    "        logging.error(\"Dataset is empty. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Take N samples\n",
    "    if N < len(dataset):\n",
    "        dataset = dataset[:N]\n",
    "    loader = DataLoader(dataset, batch_size=CONFIG['batch_size'])\n",
    "\n",
    "    # Initialize model\n",
    "    sample_data = next(iter(loader))\n",
    "    model = ExperimentalGNN(\n",
    "        num_node_features=sample_data.x.size(1),\n",
    "        edge_attr_dim=sample_data.edge_attr.size(1),\n",
    "        hidden_channels=CONFIG['hidden_channels'],\n",
    "        num_layers=CONFIG['num_layers'],\n",
    "        dropout_p=CONFIG['dropout_p']\n",
    "    ).to(device)\n",
    "\n",
    "    # Load weights\n",
    "    if not os.path.exists(CONFIG['best_model_path']):\n",
    "        logging.error(f\"Model weights not found at '{CONFIG['best_model_path']}'\")\n",
    "        return\n",
    "    logging.info(f\"Loading model from '{CONFIG['best_model_path']}'\")\n",
    "    model.load_state_dict(torch.load(CONFIG['best_model_path'], map_location=device))\n",
    "\n",
    "    # Run evaluation\n",
    "    diagnostics = evaluate(model, loader, device)\n",
    "\n",
    "    # Create plots\n",
    "    if save_plots:\n",
    "        create_diagnostic_plots(diagnostics, save_plots=True, save_dir=save_dir)\n",
    "    \n",
    "    logging.info(\"Diagnostics completed.\")\n",
    "    return diagnostics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _ = run_diagnostics(N=1500000, save_plots=True, save_dir=\"diagnostic_plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5786497c-d614-4e68-bc33-431074cee820",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
