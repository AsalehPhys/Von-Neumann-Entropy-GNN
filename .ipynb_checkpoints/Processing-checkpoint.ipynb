{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26b4a69e-8e36-4a22-bb4b-4ea6b71332ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 17:43:37,897 [INFO] First few rows of the dataset:\n",
      "2024-12-06 17:43:37,901 [INFO]    Nx      Delta      Omega  x_spacing  y_spacing      Energy  \\\n",
      "0   5  40.030606   9.988305   7.118764   6.387401 -173.629339   \n",
      "1   5  49.178313  23.871382   7.439762   6.721230 -254.139080   \n",
      "2   5   4.187528  47.495391   7.862528   7.233589 -196.238397   \n",
      "3   5   1.782346  12.313797   4.964102   6.733054  -36.205582   \n",
      "4   5  48.509647  38.981508   7.757996   7.579309 -356.018109   \n",
      "\n",
      "                                      Top_50_Indices  \\\n",
      "0  [409, 614, 393, 582, 281, 401, 550, 610, 408, ...   \n",
      "1  [409, 614, 403, 793, 611, 806, 401, 281, 610, ...   \n",
      "2  [0, 2, 1, 256, 512, 32, 16, 257, 514, 258, 513...   \n",
      "3  [0, 2, 512, 1, 256, 32, 16, 64, 128, 4, 8, 514...   \n",
      "4  [871, 923, 819, 870, 615, 411, 921, 614, 409, ...   \n",
      "\n",
      "                                Top_50_Probabilities  Von_Neumann_Entropy  N_A  \n",
      "0  [0.4158781310204766, 0.4158781307441062, 0.017...             0.694156    5  \n",
      "1  [0.15122562634177888, 0.15122562634177109, 0.0...             0.427202    9  \n",
      "2  [0.012412538419101712, 0.009717820716736438, 0...             0.046399    9  \n",
      "3  [0.10435764548960566, 0.0462267536953749, 0.04...             0.143553    3  \n",
      "4  [0.008372047496963363, 0.00837204749696328, 0....             0.084076    2  \n",
      "2024-12-06 17:43:37,921 [INFO] \n",
      "Dataset Information:\n",
      "2024-12-06 17:43:38,005 [INFO] None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 829318 entries, 0 to 829317\n",
      "Data columns (total 10 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   Nx                    829318 non-null  int64  \n",
      " 1   Delta                 829318 non-null  float64\n",
      " 2   Omega                 829318 non-null  float64\n",
      " 3   x_spacing             829318 non-null  float64\n",
      " 4   y_spacing             829318 non-null  float64\n",
      " 5   Energy                829318 non-null  float64\n",
      " 6   Top_50_Indices        829318 non-null  object \n",
      " 7   Top_50_Probabilities  829318 non-null  object \n",
      " 8   Von_Neumann_Entropy   829318 non-null  float64\n",
      " 9   N_A                   829318 non-null  int64  \n",
      "dtypes: float64(6), int64(2), object(2)\n",
      "memory usage: 63.3+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n",
      "C:\\Users\\amssa\\AppData\\Local\\Temp\\ipykernel_30780\\1898971794.py:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, self.slices = torch.load(self.processed_paths[0])\n",
      "2024-12-06 18:16:19,166 [INFO] \n",
      "Total graphs in dataset: 829318\n",
      "2024-12-06 18:16:19,167 [INFO] \n",
      "Sample Data Object:\n",
      "2024-12-06 18:16:19,170 [INFO] Data(x=[8, 4], edge_index=[2, 28], edge_attr=[28, 2], y=[1], Omega=[1, 1], Delta=[1, 1], Energy=[1, 1])\n",
      "2024-12-06 18:16:19,214 [INFO] \n",
      "Training graphs: 663454\n",
      "2024-12-06 18:16:19,215 [INFO] Validation graphs: 82932\n",
      "2024-12-06 18:16:19,216 [INFO] Test graphs: 82932\n",
      "2024-12-06 18:18:18,769 [INFO] Scalers saved to 'scalers.pkl'.\n",
      "C:\\Users\\amssa\\anaconda3\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "2024-12-06 18:22:54,571 [INFO] Data processing and loading completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, Data, DataLoader\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import logging\n",
    "import random\n",
    "import warnings\n",
    "import joblib  \n",
    "\n",
    "\n",
    "CONFIG = {\n",
    "    'data_path': r'C:\\Users\\amssa\\Documents\\Codes\\1-6\\data1-6.parquet',  # Original data file\n",
    "    'processed_dir': './processed',\n",
    "    'processed_file': './processed/data.pt',\n",
    "    'batch_size': 1024,\n",
    "    'random_seed': 42,\n",
    "    'distance_threshold': 25,  \n",
    "    'scalers_path': 'scalers.pkl',  \n",
    "}\n",
    "\n",
    "\n",
    "def setup_logging():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        handlers=[\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "setup_logging()\n",
    "set_seed(CONFIG['random_seed'])\n",
    "\n",
    "\n",
    "class SpinSystemDataset(InMemoryDataset):\n",
    "    def __init__(self, dataframe, root='.', transform=None, pre_transform=None):\n",
    "        self.df = dataframe\n",
    "        super(SpinSystemDataset, self).__init__(root, transform, pre_transform)\n",
    "        if os.path.exists(self.processed_paths[0]):\n",
    "            self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        else:\n",
    "            self.process()\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "\n",
    "        data_list = []\n",
    "        for idx, row in self.df.iterrows():\n",
    "            # Extract parameters\n",
    "            Nx = row['Nx']\n",
    "            Ny = 2  # As defined in your data generation\n",
    "            N = Nx * Ny  # Total number of spins\n",
    "\n",
    "            # Node Features: Position (x, y), p_rydberg\n",
    "            x_spacing = row['x_spacing']\n",
    "            y_spacing = row['y_spacing']\n",
    "            positions = np.array([\n",
    "                (col * x_spacing, row_idx * y_spacing)\n",
    "                for row_idx in range(Nx) for col in range(Ny)\n",
    "            ])\n",
    "            positions = torch.tensor(positions, dtype=torch.float)\n",
    "\n",
    "            # Normalize positions\n",
    "            pos_min = positions.min(dim=0).values\n",
    "            pos_max = positions.max(dim=0).values\n",
    "            normalized_positions = (positions - pos_min) / (pos_max - pos_min + 1e-8)\n",
    "\n",
    "            # Extracting 'Top_50_Indices' and 'Top_50_Probabilities'\n",
    "            state_indices = row['Top_50_Indices'] \n",
    "            state_probs = row['Top_50_Probabilities']  \n",
    "\n",
    "            num_particles = N  \n",
    "\n",
    "            p_rydberg = [0.0] * num_particles\n",
    "\n",
    "            for state, prob in zip(state_indices, state_probs):\n",
    "                state = int(state)\n",
    "                for i in range(num_particles):\n",
    "                    if state & (1 << i):\n",
    "                        p_rydberg[i] += prob\n",
    "\n",
    "            p_rydberg = torch.tensor(p_rydberg, dtype=torch.float).unsqueeze(1)  # [N, 1]\n",
    "\n",
    "            try:\n",
    "                N_A_idx = int(row['N_A'])  \n",
    "            except ValueError:\n",
    "                logging.error(f\"Graph {idx}: N_A value '{row['N_A']}' is not an integer.\")\n",
    "                raise\n",
    "\n",
    "            N_A_feature = torch.zeros((N, 1), dtype=torch.float)\n",
    "            if 0 <= N_A_idx < N:\n",
    "                N_A_feature[N_A_idx] = 1.0\n",
    "            else:\n",
    "                logging.warning(f'Graph {idx}: N_A index {N_A_idx} out of range for {N} nodes.')\n",
    "\n",
    "            # Concatenate node features: [x, y, p_rydberg, N_A]\n",
    "            node_features = torch.cat([normalized_positions, p_rydberg, N_A_feature], dim=1)  # [N, 4]\n",
    "\n",
    "            # Edge Index and Edge Attributes using Distance Threshold\n",
    "            distance_threshold = CONFIG['distance_threshold']\n",
    "            nbrs = NearestNeighbors(radius=distance_threshold, algorithm='ball_tree').fit(positions.numpy())\n",
    "            indices = nbrs.radius_neighbors(positions.numpy(), return_distance=False)\n",
    "\n",
    "            edge_index = []\n",
    "            for i in range(N):\n",
    "                for j in indices[i]:\n",
    "                    if i < j:\n",
    "                        edge_index.append([i, j])\n",
    "            edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "            logging.debug(f'Graph {idx}: Created {edge_index.size(1)} edges using distance threshold.')\n",
    "\n",
    "            # Compute edge attributes (1/r^6)\n",
    "            if edge_index.size(1) > 0:\n",
    "                pos_i = positions[edge_index[0]]\n",
    "                pos_j = positions[edge_index[1]]\n",
    "                distances = torch.norm(pos_i - pos_j, dim=1, keepdim=True)\n",
    "                epsilon = 1e-8\n",
    "                inv_r6 = 1.0 / (distances.pow(6) + epsilon)  # [E, 1]\n",
    "            else:\n",
    "                inv_r6 = torch.empty((0, 1), dtype=torch.float)\n",
    "\n",
    "            # Compute joint probabilities for edges\n",
    "            edge_tuples = [(edge_index[0, k].item(), edge_index[1, k].item()) for k in range(edge_index.size(1))]\n",
    "            edge_joint_probs = {edge: 0.0 for edge in edge_tuples}\n",
    "\n",
    "            for state, prob in zip(state_indices, state_probs):\n",
    "                state = int(state)\n",
    "                rydberg_particles = [i for i in range(N) if state & (1 << i)]\n",
    "                rydberg_set = set(rydberg_particles)\n",
    "                for i in rydberg_particles:\n",
    "                    for j in rydberg_particles:\n",
    "                        if i < j and (i, j) in edge_joint_probs:\n",
    "                            edge_joint_probs[(i, j)] += prob\n",
    "\n",
    "            # Compute correlation function for edges\n",
    "            edge_correlation = []\n",
    "            for k in range(edge_index.size(1)):\n",
    "                i = edge_index[0, k].item()\n",
    "                j = edge_index[1, k].item()\n",
    "                joint_prob = edge_joint_probs.get((i, j), 0.0)\n",
    "                p_ryd_i = p_rydberg[i].item()\n",
    "                p_ryd_j = p_rydberg[j].item()\n",
    "                correlation = joint_prob - p_ryd_i * p_ryd_j\n",
    "                edge_correlation.append([correlation])\n",
    "\n",
    "            edge_correlation = torch.tensor(edge_correlation, dtype=torch.float)  # [E, 1]\n",
    "\n",
    "            if edge_index.size(1) > 0:\n",
    "                edge_attr = torch.cat([inv_r6, edge_correlation], dim=1)  # [E, 2]\n",
    "            else:\n",
    "                edge_attr = torch.empty((0, 2), dtype=torch.float)\n",
    "\n",
    "            # Target: Von Neumann Entropy\n",
    "            entropy = torch.tensor([row['Von_Neumann_Entropy']], dtype=torch.float)\n",
    "            entropy = torch.log(entropy + 1e-9)  \n",
    "\n",
    "            # Graph-level features: Omega, Delta, Energy\n",
    "            Omega = torch.tensor([[row['Omega']]], dtype=torch.float)       # [1, 1]\n",
    "            Delta = torch.tensor([[row['Delta']]], dtype=torch.float)       # [1, 1]\n",
    "            Energy = torch.tensor([[row['Energy']]], dtype=torch.float)     # [1, 1]\n",
    "\n",
    "            # Create Data object\n",
    "            data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr, y=entropy)\n",
    "            data.Omega = Omega\n",
    "            data.Delta = Delta\n",
    "            data.Energy = Energy\n",
    "\n",
    "            data_list.append(data)\n",
    "\n",
    "        \n",
    "        if self.pre_transform:\n",
    "            data_list = [self.pre_transform(d) for d in data_list]\n",
    "\n",
    "        \n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "\n",
    "def load_data(config):\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch\")\n",
    "\n",
    "    \n",
    "    if not os.path.exists(config['data_path']):\n",
    "        logging.error(f\"Data file not found at {config['data_path']}\")\n",
    "        raise FileNotFoundError(f\"Data file not found at {config['data_path']}\")\n",
    "\n",
    "    df = pd.read_parquet(config['data_path'])\n",
    "\n",
    "    \n",
    "    logging.info(\"First few rows of the dataset:\")\n",
    "    logging.info(df.head())\n",
    "    logging.info(\"\\nDataset Information:\")\n",
    "    logging.info(df.info())\n",
    "\n",
    "    \n",
    "    df_shuffled = df.sample(frac=1, random_state=config['random_seed']).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    dataset = SpinSystemDataset(df_shuffled, root=config['processed_dir'])\n",
    "\n",
    "    logging.info(f'\\nTotal graphs in dataset: {len(dataset)}')\n",
    "    logging.info(f'\\nSample Data Object:')\n",
    "    logging.info(dataset[0])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def split_dataset(dataset, config):\n",
    "    \n",
    "    total = len(dataset)\n",
    "    train_end = int(0.8 * total)\n",
    "    val_end = int(0.9 * total)\n",
    "\n",
    "    \n",
    "    train_dataset = dataset[:train_end]\n",
    "    val_dataset = dataset[train_end:val_end]\n",
    "    test_dataset = dataset[val_end:]\n",
    "\n",
    "    logging.info(f'\\nTraining graphs: {len(train_dataset)}')\n",
    "    logging.info(f'Validation graphs: {len(val_dataset)}')\n",
    "    logging.info(f'Test graphs: {len(test_dataset)}')\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def normalize_features(train_dataset, val_dataset, test_dataset, scalers_path):\n",
    "    # Initialize scalers\n",
    "    scalers = {\n",
    "        'Omega': StandardScaler(),\n",
    "        'Delta': StandardScaler(),\n",
    "        'Energy': StandardScaler()\n",
    "    }\n",
    "\n",
    "    # Fit scalers on training data\n",
    "    features = ['Omega', 'Delta', 'Energy']\n",
    "    for feature in features:\n",
    "        try:\n",
    "            train_values = torch.cat([getattr(data, feature) for data in train_dataset], dim=0).numpy()\n",
    "            scalers[feature].fit(train_values)\n",
    "        except AttributeError as e:\n",
    "            logging.error(f\"AttributeError while accessing feature '{feature}': {e}\")\n",
    "            raise\n",
    "\n",
    "    \n",
    "    joblib.dump(scalers, scalers_path)\n",
    "    logging.info(f\"Scalers saved to '{scalers_path}'.\")\n",
    "\n",
    "    \n",
    "    def normalize(dataset, scalers):\n",
    "        for data in dataset:\n",
    "            for feature in features:\n",
    "                scaled = scalers[feature].transform(getattr(data, feature).numpy())\n",
    "                setattr(data, feature, torch.tensor(scaled, dtype=torch.float))\n",
    "        return dataset\n",
    "\n",
    "    # Apply normalization\n",
    "    train_dataset = normalize(train_dataset, scalers)\n",
    "    val_dataset = normalize(val_dataset, scalers)\n",
    "    test_dataset = normalize(test_dataset, scalers)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def create_dataloaders(train_dataset, val_dataset, test_dataset, config):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load and prepare dataset\n",
    "    dataset = load_data(CONFIG)\n",
    "    train_dataset, val_dataset, test_dataset = split_dataset(dataset, CONFIG)\n",
    "    train_dataset, val_dataset, test_dataset = normalize_features(train_dataset, val_dataset, test_dataset, CONFIG['scalers_path'])\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(train_dataset, val_dataset, test_dataset, CONFIG)\n",
    "\n",
    "    logging.info(\"Data processing and loading completed successfully.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632ea357-205f-4ccb-b156-5677551bad0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
